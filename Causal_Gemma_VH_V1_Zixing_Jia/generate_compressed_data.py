"""
ä»åŸå§‹å‘é‡æ•°æ®ä¸­ç­›é€‰å‹ç¼©åçš„è®­ç»ƒæ•°æ®

é‡è¦è¯´æ˜ï¼š
  - æœ¬è„šæœ¬ä¸é‡æ–°èšç±»ï¼Œç›´æ¥ä»åŸå§‹æ•°æ®ä¸­ç­›é€‰
  - æ ¹æ® clustering_summary_compressed.json ç¡®å®šéœ€è¦ä¿ç•™å“ªäº› (premise, cluster_id) ç»„åˆ
  - ä» causal_A_vectors.npy, causal_B_vectors.npy, causal_metadata.json ä¸­ç­›é€‰å¯¹åº”çš„æ ·æœ¬

è¾“å…¥æ–‡ä»¶ï¼ˆç”± causal_data_processing.py ç”Ÿæˆï¼‰ï¼š
  - causal_A_vectors.npy: åŸå§‹ A å‘é‡
  - causal_B_vectors.npy: åŸå§‹ B å‘é‡
  - causal_metadata.json: åŸå§‹å…ƒæ•°æ®

è¾“å…¥æ–‡ä»¶ï¼ˆç”± compress_clusters.py ç”Ÿæˆï¼‰ï¼š
  - clustering_summary_compressed.json: å‹ç¼©åçš„èšç±»æ‘˜è¦

è¾“å‡ºæ–‡ä»¶ï¼š
  - causal_A_vectors_compressed.npy: å‹ç¼©åçš„ A å‘é‡
  - causal_B_vectors_compressed.npy: å‹ç¼©åçš„ B å‘é‡
  - causal_metadata_compressed.json: å‹ç¼©åçš„å…ƒæ•°æ®

ä½¿ç”¨æ–¹æ³•ï¼š
1. å…ˆè¿è¡Œ causal_data_processing.py ç”ŸæˆåŸå§‹æ•°æ®
2. è¿è¡Œ compress_clusters.py å‹ç¼©èšç±»æ‘˜è¦
3. è¿è¡Œæœ¬è„šæœ¬ç­›é€‰å‘é‡æ•°æ®
"""

import os
import json
import numpy as np
from collections import defaultdict

# ================== é…ç½®å‚æ•° ==================
BASE_DIR = "/home/amax/Zixing_Jia/Vector-HASH-tinkering-main/Gemma_anwer_causal/Causal_Data"

# è¾“å…¥æ–‡ä»¶
ORIGINAL_A_VECTORS = os.path.join(BASE_DIR, "causal_A_vectors.npy")
ORIGINAL_B_VECTORS = os.path.join(BASE_DIR, "causal_B_vectors.npy")
ORIGINAL_METADATA = os.path.join(BASE_DIR, "causal_metadata.json")
COMPRESSED_SUMMARY = os.path.join(BASE_DIR, "clustering_summary_compressed.json")

# è¾“å‡ºæ–‡ä»¶
OUTPUT_A_VECTORS = os.path.join(BASE_DIR, "causal_A_vectors_compressed.npy")
OUTPUT_B_VECTORS = os.path.join(BASE_DIR, "causal_B_vectors_compressed.npy")
OUTPUT_METADATA = os.path.join(BASE_DIR, "causal_metadata_compressed.json")

# ================== Step 1: åŠ è½½åŸå§‹æ•°æ® ==================
print("=" * 60)
print("Step 1: åŠ è½½åŸå§‹å‘é‡æ•°æ®å’Œå…ƒæ•°æ®")
print("=" * 60)

print(f"ğŸ“ åŠ è½½ A å‘é‡: {ORIGINAL_A_VECTORS}")
A_vectors = np.load(ORIGINAL_A_VECTORS)
print(f"   Shape: {A_vectors.shape}")

print(f"ğŸ“ åŠ è½½ B å‘é‡: {ORIGINAL_B_VECTORS}")
B_vectors = np.load(ORIGINAL_B_VECTORS)
print(f"   Shape: {B_vectors.shape}")

print(f"ğŸ“ åŠ è½½å…ƒæ•°æ®: {ORIGINAL_METADATA}")
with open(ORIGINAL_METADATA, 'r', encoding='utf-8') as f:
    metadata = json.load(f)
print(f"   æ ·æœ¬æ•°: {len(metadata)}")

# éªŒè¯æ•°æ®ä¸€è‡´æ€§
assert A_vectors.shape[0] == len(metadata), "A å‘é‡æ•°é‡ä¸å…ƒæ•°æ®ä¸åŒ¹é…ï¼"
assert B_vectors.shape[0] == len(metadata), "B å‘é‡æ•°é‡ä¸å…ƒæ•°æ®ä¸åŒ¹é…ï¼"
print("âœ… æ•°æ®ä¸€è‡´æ€§éªŒè¯é€šè¿‡")

# ================== Step 2: åŠ è½½å‹ç¼©åçš„èšç±»æ‘˜è¦ ==================
print("\n" + "=" * 60)
print("Step 2: åŠ è½½å‹ç¼©åçš„èšç±»æ‘˜è¦")
print("=" * 60)

print(f"ğŸ“ åŠ è½½å‹ç¼©åçš„èšç±»æ‘˜è¦: {COMPRESSED_SUMMARY}")
with open(COMPRESSED_SUMMARY, 'r', encoding='utf-8') as f:
    compressed_summary = json.load(f)
print(f"   Premise æ•°: {len(compressed_summary)}")

# æ„å»ºæœ‰æ•ˆçš„ (premise, cluster_id) é›†åˆ
valid_pairs = set()
for premise, data in compressed_summary.items():
    for cluster_id in data['clusters'].keys():
        valid_pairs.add((premise, cluster_id))

total_clusters = sum(d['n_clusters'] for d in compressed_summary.values())
print(f"   æ€» Cluster æ•°: {total_clusters}")
print(f"   æœ‰æ•ˆ (premise, cluster_id) å¯¹æ•°: {len(valid_pairs)}")

# ================== Step 3: ç­›é€‰æ•°æ® ==================
print("\n" + "=" * 60)
print("Step 3: ç­›é€‰å‹ç¼©åçš„æ•°æ®")
print("=" * 60)

# æ‰¾å‡ºéœ€è¦ä¿ç•™çš„æ ·æœ¬ç´¢å¼•
keep_indices = []
for i, m in enumerate(metadata):
    premise = m['premise']
    cluster_id = str(m['cluster_id'])  # æ³¨æ„ï¼šmetadata ä¸­æ˜¯ intï¼Œsummary ä¸­æ˜¯ str
    
    if (premise, cluster_id) in valid_pairs:
        keep_indices.append(i)

print(f"åŸå§‹æ ·æœ¬æ•°: {len(metadata)}")
print(f"ä¿ç•™æ ·æœ¬æ•°: {len(keep_indices)}")
print(f"ç§»é™¤æ ·æœ¬æ•°: {len(metadata) - len(keep_indices)}")

# ç­›é€‰å‘é‡å’Œå…ƒæ•°æ®
compressed_A = A_vectors[keep_indices]
compressed_B = B_vectors[keep_indices]
compressed_metadata = [metadata[i] for i in keep_indices]

print(f"\nå‹ç¼©å A å‘é‡ Shape: {compressed_A.shape}")
print(f"å‹ç¼©å B å‘é‡ Shape: {compressed_B.shape}")
print(f"å‹ç¼©åå…ƒæ•°æ®æ•°é‡: {len(compressed_metadata)}")

# ================== Step 4: éªŒè¯æ•°æ®ä¸€è‡´æ€§ ==================
print("\n" + "=" * 60)
print("Step 4: éªŒè¯å‹ç¼©åçš„æ•°æ®")
print("=" * 60)

# ç»Ÿè®¡ ASKFOR åˆ†å¸ƒ
askfor_counts = defaultdict(int)
for m in compressed_metadata:
    askfor_counts[m['ASKFOR']] += 1
print(f"ASKFOR åˆ†å¸ƒ: {dict(askfor_counts)}")

# ç»Ÿè®¡å”¯ä¸€çš„ INDEX æ•°é‡
unique_indices = set(m['INDEX'] for m in compressed_metadata)
print(f"å”¯ä¸€ INDEX æ•°é‡: {len(unique_indices)}")

# ç»Ÿè®¡å”¯ä¸€çš„ premise æ•°é‡
unique_premises = set(m['premise'] for m in compressed_metadata)
print(f"å”¯ä¸€ Premise æ•°é‡: {len(unique_premises)}")

# éªŒè¯ä¸å‹ç¼©æ‘˜è¦çš„ä¸€è‡´æ€§
# æ£€æŸ¥æ¯ä¸ª (premise, cluster_id) çš„æ ·æœ¬æ•°æ˜¯å¦ä¸ compressed_summary ä¸­çš„ count ä¸€è‡´
pair_counts = defaultdict(int)
for m in compressed_metadata:
    key = (m['premise'], str(m['cluster_id']))
    pair_counts[key] += 1

mismatches = []
for premise, data in compressed_summary.items():
    for cluster_id, cluster_info in data['clusters'].items():
        expected_count = cluster_info['count']
        actual_count = pair_counts.get((premise, cluster_id), 0)
        if expected_count != actual_count:
            mismatches.append({
                'premise': premise[:50] + '...',
                'cluster_id': cluster_id,
                'expected': expected_count,
                'actual': actual_count
            })

if mismatches:
    print(f"\nâš ï¸ å‘ç° {len(mismatches)} ä¸ªä¸ä¸€è‡´:")
    for m in mismatches[:5]:
        print(f"   - {m}")
else:
    print("âœ… æ‰€æœ‰ (premise, cluster_id) çš„æ ·æœ¬æ•°ä¸å‹ç¼©æ‘˜è¦ä¸€è‡´")

# ================== Step 5: ä¿å­˜å‹ç¼©åçš„æ•°æ® ==================
print("\n" + "=" * 60)
print("Step 5: ä¿å­˜å‹ç¼©åçš„æ•°æ®")
print("=" * 60)

# ä¿å­˜å‘é‡
np.save(OUTPUT_A_VECTORS, compressed_A)
print(f"âœ… ä¿å­˜ A å‘é‡: {OUTPUT_A_VECTORS}")

np.save(OUTPUT_B_VECTORS, compressed_B)
print(f"âœ… ä¿å­˜ B å‘é‡: {OUTPUT_B_VECTORS}")

# ä¿å­˜å…ƒæ•°æ®
with open(OUTPUT_METADATA, 'w', encoding='utf-8') as f:
    json.dump(compressed_metadata, f, ensure_ascii=False, indent=2)
print(f"âœ… ä¿å­˜å…ƒæ•°æ®: {OUTPUT_METADATA}")

# ================== Step 6: æœ€ç»ˆéªŒè¯ ==================
print("\n" + "=" * 60)
print("Step 6: æœ€ç»ˆéªŒè¯")
print("=" * 60)

# é‡æ–°åŠ è½½å¹¶éªŒè¯
A_loaded = np.load(OUTPUT_A_VECTORS)
B_loaded = np.load(OUTPUT_B_VECTORS)
with open(OUTPUT_METADATA, 'r', encoding='utf-8') as f:
    meta_loaded = json.load(f)

print(f"A å‘é‡ Shape: {A_loaded.shape}")
print(f"B å‘é‡ Shape: {B_loaded.shape}")
print(f"å…ƒæ•°æ®æ•°é‡: {len(meta_loaded)}")

assert A_loaded.shape[0] == len(meta_loaded), "A å‘é‡æ•°é‡ä¸å…ƒæ•°æ®ä¸åŒ¹é…ï¼"
assert B_loaded.shape[0] == len(meta_loaded), "B å‘é‡æ•°é‡ä¸å…ƒæ•°æ®ä¸åŒ¹é…ï¼"

print("\nâœ… å‹ç¼©æ•°æ®ç”Ÿæˆå®Œæˆï¼")

# è¾“å‡ºæ‘˜è¦
print("\n" + "=" * 60)
print("å‹ç¼©æ‘˜è¦")
print("=" * 60)
print(f"{'æŒ‡æ ‡':<25} {'å‹ç¼©å‰':>12} {'å‹ç¼©å':>12}")
print("-" * 49)
print(f"{'æ ·æœ¬æ•°':<25} {len(metadata):>12} {len(meta_loaded):>12}")
print(f"{'å”¯ä¸€ Premise æ•°':<25} {len(set(m['premise'] for m in metadata)):>12} {len(unique_premises):>12}")
print(f"{'å‘é‡ç»´åº¦':<25} {A_vectors.shape[1]:>12} {A_loaded.shape[1]:>12}")
