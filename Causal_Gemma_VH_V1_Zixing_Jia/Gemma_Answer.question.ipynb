{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6e7881",
   "metadata": {},
   "source": [
    "# Gemma å› æœæ¨ç†é€‰æ‹©é¢˜æµ‹è¯•\n",
    "\n",
    "æœ¬notebookç”¨äºæµ‹è¯•Gemmaæ¨¡å‹åœ¨å› æœæ¨ç†é€‰æ‹©é¢˜ä¸Šçš„è¡¨ç°ï¼š\n",
    "1. ä»å·²å›ç­”çš„æ•°æ®ä¸­è·å–é—®é¢˜index\n",
    "2. æ ¹æ®indexä»åŸå§‹æ•°æ®é›†ä¸­è·å–å®Œæ•´é¢˜ç›®ï¼ˆpremise, hypothesis1, hypothesis2ï¼‰\n",
    "3. è®©Gemmaç›´æ¥é€‰æ‹©ç­”æ¡ˆ\n",
    "4. è®¡ç®—æ€»æ­£ç¡®ç‡åŠæŒ‰ask-forç±»å‹åˆ†ç±»çš„æ­£ç¡®ç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ac0063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amax/miniconda3/envs/gemma/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é…ç½®åŠ è½½å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: å¯¼å…¥åº“å’Œé…ç½®\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# =================é…ç½®åŒºåŸŸ=================\n",
    "# æ¨¡å‹è·¯å¾„é…ç½®\n",
    "CACHE_DIR = \"/home/amax/Gemma_and_NVembed/Gemma\"\n",
    "MODEL_NAME = \"google/gemma-2-9b-it\"\n",
    "# æ•°æ®è·¯å¾„é…ç½®\n",
    "ANSWERED_FILE = \"/home/amax/Zixing_Jia/Vector-HASH-tinkering-main/Gemma_anwer_causal/T_1.25_Gemma_Answer/gemma_causal_answers_100.jsonl\"\n",
    "TRAIN_FILE = \"/home/amax/Zixing_Jia/Vector-HASH-tinkering-main/Ecare/train.jsonl\"\n",
    "OUTPUT_FILE = \"/home/amax/Zixing_Jia/Vector-HASH-tinkering-main/Gemma_anwer_causal/gemma_choice_results.jsonl\"\n",
    "\n",
    "print(\"é…ç½®åŠ è½½å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3badd619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åŠ è½½æ¨¡å‹: google/gemma-2-9b-it ...\n",
      "ğŸ“ æœ¬åœ°æ¨¡å‹è·¯å¾„: /home/amax/Gemma_and_NVembed/Gemma/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819\n",
      "\n",
      "â³ æ­£åœ¨åŠ è½½ tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenizer åŠ è½½æˆåŠŸï¼\n",
      "\n",
      "â³ æ­£åœ¨åŠ è½½æ¨¡å‹ï¼ˆçº¦éœ€ 1-2 åˆ†é’Ÿï¼‰...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼\n",
      "ğŸ’» æ¨¡å‹è®¾å¤‡: cuda:0\n",
      "ğŸ“Š æ¨¡å‹å‚æ•°é‡: 9.24B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: åŠ è½½æ¨¡å‹\n",
    "def setup_model():\n",
    "    \"\"\"åŠ è½½æœ¬åœ° Gemma æ¨¡å‹\"\"\"\n",
    "    print(f\"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_NAME} ...\")\n",
    "    \n",
    "    # æŸ¥æ‰¾æœ¬åœ°æ¨¡å‹è·¯å¾„\n",
    "    model_base = os.path.join(CACHE_DIR, \"models--google--gemma-2-9b-it\", \"snapshots\")\n",
    "    \n",
    "    if not os.path.exists(model_base):\n",
    "        raise FileNotFoundError(f\"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_base}\")\n",
    "    \n",
    "    snapshots = os.listdir(model_base)\n",
    "    if not snapshots:\n",
    "        raise FileNotFoundError(\"æ¨¡å‹å°šæœªä¸‹è½½ï¼Œè¯·å…ˆä¸‹è½½æ¨¡å‹\")\n",
    "    \n",
    "    local_model_path = os.path.join(model_base, snapshots[0])\n",
    "    print(f\"ğŸ“ æœ¬åœ°æ¨¡å‹è·¯å¾„: {local_model_path}\")\n",
    "    \n",
    "    # åŠ è½½ tokenizer\n",
    "    print(\"\\nâ³ æ­£åœ¨åŠ è½½ tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        local_model_path,\n",
    "        local_files_only=True\n",
    "    )\n",
    "    print(\"âœ… Tokenizer åŠ è½½æˆåŠŸï¼\")\n",
    "    \n",
    "    # åŠ è½½æ¨¡å‹\n",
    "    print(\"\\nâ³ æ­£åœ¨åŠ è½½æ¨¡å‹ï¼ˆçº¦éœ€ 1-2 åˆ†é’Ÿï¼‰...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        local_model_path,\n",
    "        local_files_only=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼\")\n",
    "    print(f\"ğŸ’» æ¨¡å‹è®¾å¤‡: {model.device}\")\n",
    "    print(f\"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {model.num_parameters() / 1e9:.2f}B\")\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "tokenizer, model = setup_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "852da276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ åŠ è½½å·²å›ç­”çš„é—®é¢˜indices...\n",
      "ğŸ“Š å·²å›ç­”çš„unique indexæ•°: 795\n",
      "\n",
      "ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®é›†...\n",
      "ğŸ“Š è®­ç»ƒé›†æ€»æ ·æœ¬æ•°: 14928\n",
      "\n",
      "ğŸ“‚ å‡†å¤‡é—®é¢˜æ•°æ®...\n",
      "ğŸ“Š å¾…æµ‹è¯•é—®é¢˜æ•°: 795\n",
      "\n",
      "ğŸ“ˆ é—®é¢˜ç±»å‹åˆ†å¸ƒ:\n",
      "   - Effectç±»å‹: 359\n",
      "   - Causeç±»å‹: 436\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: åŠ è½½æ•°æ®å¹¶æ„å»ºé—®é¢˜é›†\n",
    "def load_answered_indices(answered_file):\n",
    "    \"\"\"ä»å·²å›ç­”æ–‡ä»¶ä¸­æå–uniqueçš„index\"\"\"\n",
    "    indices = set()\n",
    "    with open(answered_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                record = json.loads(line)\n",
    "                indices.add(record['index'])\n",
    "    return indices\n",
    "\n",
    "def load_train_data(train_file):\n",
    "    \"\"\"åŠ è½½è®­ç»ƒæ•°æ®ï¼Œè¿”å›ä»¥indexä¸ºkeyçš„å­—å…¸\"\"\"\n",
    "    data_dict = {}\n",
    "    with open(train_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                record = json.loads(line)\n",
    "                data_dict[record['index']] = record\n",
    "    return data_dict\n",
    "\n",
    "def prepare_questions(answered_indices, train_data):\n",
    "    \"\"\"æ ¹æ®å·²å›ç­”çš„indiceså‡†å¤‡é—®é¢˜æ•°æ®\"\"\"\n",
    "    questions = []\n",
    "    for idx in answered_indices:\n",
    "        if idx in train_data:\n",
    "            questions.append(train_data[idx])\n",
    "        else:\n",
    "            print(f\"âš ï¸ Index {idx} åœ¨è®­ç»ƒé›†ä¸­æœªæ‰¾åˆ°\")\n",
    "    return questions\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "print(\"ğŸ“‚ åŠ è½½å·²å›ç­”çš„é—®é¢˜indices...\")\n",
    "answered_indices = load_answered_indices(ANSWERED_FILE)\n",
    "print(f\"ğŸ“Š å·²å›ç­”çš„unique indexæ•°: {len(answered_indices)}\")\n",
    "\n",
    "print(\"\\nğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®é›†...\")\n",
    "train_data = load_train_data(TRAIN_FILE)\n",
    "print(f\"ğŸ“Š è®­ç»ƒé›†æ€»æ ·æœ¬æ•°: {len(train_data)}\")\n",
    "\n",
    "print(\"\\nğŸ“‚ å‡†å¤‡é—®é¢˜æ•°æ®...\")\n",
    "questions = prepare_questions(answered_indices, train_data)\n",
    "print(f\"ğŸ“Š å¾…æµ‹è¯•é—®é¢˜æ•°: {len(questions)}\")\n",
    "\n",
    "# ç»Ÿè®¡ask-forç±»å‹åˆ†å¸ƒ\n",
    "effect_count = sum(1 for q in questions if q['ask-for'] == 'effect')\n",
    "cause_count = sum(1 for q in questions if q['ask-for'] == 'cause')\n",
    "print(f\"\\nğŸ“ˆ é—®é¢˜ç±»å‹åˆ†å¸ƒ:\")\n",
    "print(f\"   - Effectç±»å‹: {effect_count}\")\n",
    "print(f\"   - Causeç±»å‹: {cause_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd504b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ å¼€å§‹é€‰æ‹©é¢˜æµ‹è¯•...\n",
      "ğŸ“ è¾“å‡ºæ–‡ä»¶: /home/amax/Zixing_Jia/Vector-HASH-tinkering-main/Gemma_anwer_causal/gemma_choice_results.jsonl\n",
      "ğŸ“Š æ€»é—®é¢˜æ•°: 795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å›ç­”é€‰æ‹©é¢˜:   0%|          | 0/795 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å›ç­”é€‰æ‹©é¢˜: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 795/795 [01:17<00:00, 10.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… æµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: /home/amax/Zixing_Jia/Vector-HASH-tinkering-main/Gemma_anwer_causal/gemma_choice_results.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: æ„é€ promptå¹¶è®©Gemmaå›ç­”é€‰æ‹©é¢˜\n",
    "def construct_choice_prompt(question):\n",
    "    \"\"\"\n",
    "    æ„é€ é€‰æ‹©é¢˜prompt\n",
    "    \n",
    "    Args:\n",
    "        question: åŒ…å« premise, ask-for, hypothesis1, hypothesis2 çš„å­—å…¸\n",
    "    \n",
    "    Returns:\n",
    "        promptå­—ç¬¦ä¸²\n",
    "    \"\"\"\n",
    "    premise = question['premise']\n",
    "    ask_for = question['ask-for']\n",
    "    h1 = question['hypothesis1']\n",
    "    h2 = question['hypothesis2']\n",
    "    \n",
    "    if ask_for == \"effect\":\n",
    "        prompt = f\"\"\"You are a causal reasoning expert. Given a premise, select the most likely effect from the two options.\n",
    "\n",
    "Premise: {premise}\n",
    "\n",
    "Options:\n",
    "A. {h1}\n",
    "B. {h2}\n",
    "\n",
    "Please select the correct answer (A or B only). Just output the letter, nothing else.\n",
    "\n",
    "Answer:\"\"\"\n",
    "    else:  # cause\n",
    "        prompt = f\"\"\"You are a causal reasoning expert. Given a result, select the most likely cause from the two options.\n",
    "\n",
    "Result: {premise}\n",
    "\n",
    "Options:\n",
    "A. {h1}\n",
    "B. {h2}\n",
    "\n",
    "Please select the correct answer (A or B only). Just output the letter, nothing else.\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def get_gemma_choice(tokenizer, model, prompt):\n",
    "    \"\"\"\n",
    "    è®©Gemmaå›ç­”é€‰æ‹©é¢˜\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: åˆ†è¯å™¨\n",
    "        model: è¯­è¨€æ¨¡å‹\n",
    "        prompt: è¾“å…¥prompt\n",
    "    \n",
    "    Returns:\n",
    "        'A', 'B' æˆ– 'unknown'\n",
    "    \"\"\"\n",
    "    # ä½¿ç”¨ chat template\n",
    "    chat = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    prompt_formatted = tokenizer.apply_chat_template(\n",
    "        chat, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(prompt_formatted, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,  # åªéœ€è¦ç”Ÿæˆå¾ˆçŸ­çš„å›ç­”\n",
    "            do_sample=False,    # ä½¿ç”¨greedy decodingç¡®ä¿ä¸€è‡´æ€§\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # åªæå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†\n",
    "    generated_text = tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    ).strip().upper()\n",
    "    \n",
    "    # è§£æç­”æ¡ˆ\n",
    "    if 'A' in generated_text[:5]:\n",
    "        return 'A', generated_text\n",
    "    elif 'B' in generated_text[:5]:\n",
    "        return 'B', generated_text\n",
    "    else:\n",
    "        return 'unknown', generated_text\n",
    "\n",
    "\n",
    "def run_choice_test(tokenizer, model, questions, output_file):\n",
    "    \"\"\"\n",
    "    è¿è¡Œé€‰æ‹©é¢˜æµ‹è¯•\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: åˆ†è¯å™¨\n",
    "        model: è¯­è¨€æ¨¡å‹\n",
    "        questions: é—®é¢˜åˆ—è¡¨\n",
    "        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "    \n",
    "    Returns:\n",
    "        results: ç»“æœåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nğŸš€ å¼€å§‹é€‰æ‹©é¢˜æµ‹è¯•...\")\n",
    "    print(f\"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}\")\n",
    "    print(f\"ğŸ“Š æ€»é—®é¢˜æ•°: {len(questions)}\")\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for question in tqdm(questions, desc=\"å›ç­”é€‰æ‹©é¢˜\"):\n",
    "            # æ„é€ prompt\n",
    "            prompt = construct_choice_prompt(question)\n",
    "            \n",
    "            # è·å–Gemmaçš„é€‰æ‹©\n",
    "            choice, raw_answer = get_gemma_choice(tokenizer, model, prompt)\n",
    "            \n",
    "            # åˆ¤æ–­æ­£ç¡®ç­”æ¡ˆï¼ˆlabel=0è¡¨ç¤ºhypothesis1æ­£ç¡®ï¼Œå³Aæ­£ç¡®ï¼‰\n",
    "            correct_answer = 'A' if question['label'] == 0 else 'B'\n",
    "            is_correct = (choice == correct_answer)\n",
    "            \n",
    "            # æ„å»ºç»“æœè®°å½•\n",
    "            record = {\n",
    "                \"index\": question['index'],\n",
    "                \"premise\": question['premise'],\n",
    "                \"ask-for\": question['ask-for'],\n",
    "                \"hypothesis1\": question['hypothesis1'],\n",
    "                \"hypothesis2\": question['hypothesis2'],\n",
    "                \"label\": question['label'],\n",
    "                \"correct_answer\": correct_answer,\n",
    "                \"gemma_choice\": choice,\n",
    "                \"gemma_raw_answer\": raw_answer,\n",
    "                \"is_correct\": is_correct\n",
    "            }\n",
    "            \n",
    "            results.append(record)\n",
    "            \n",
    "            # å†™å…¥æ–‡ä»¶\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    print(f\"\\nâœ… æµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}\")\n",
    "    return results\n",
    "\n",
    "# è¿è¡Œæµ‹è¯•\n",
    "results = run_choice_test(tokenizer, model, questions, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29ce1b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“Š Gemma å› æœæ¨ç†é€‰æ‹©é¢˜æµ‹è¯•ç»“æœ\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ æ€»ä½“æ­£ç¡®ç‡:\n",
      "   æ€»é—®é¢˜æ•°: 795\n",
      "   æ­£ç¡®å›ç­”: 667\n",
      "   æ— æ•ˆå›ç­”: 1\n",
      "   æ€»æ­£ç¡®ç‡: 83.90%\n",
      "   æœ‰æ•ˆæ­£ç¡®ç‡ (æ’é™¤unknown): 84.01%\n",
      "\n",
      "ğŸ“ˆ Effectç±»å‹æ­£ç¡®ç‡:\n",
      "   æ€»é—®é¢˜æ•°: 359\n",
      "   æ­£ç¡®å›ç­”: 299\n",
      "   æ— æ•ˆå›ç­”: 0\n",
      "   æ­£ç¡®ç‡: 83.29%\n",
      "   æœ‰æ•ˆæ­£ç¡®ç‡: 83.29%\n",
      "\n",
      "ğŸ“ˆ Causeç±»å‹æ­£ç¡®ç‡:\n",
      "   æ€»é—®é¢˜æ•°: 436\n",
      "   æ­£ç¡®å›ç­”: 368\n",
      "   æ— æ•ˆå›ç­”: 1\n",
      "   æ­£ç¡®ç‡: 84.40%\n",
      "   æœ‰æ•ˆæ­£ç¡®ç‡: 84.60%\n",
      "\n",
      "============================================================\n",
      "\n",
      "ğŸ“ ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: /home/amax/Zixing_Jia/Vector-HASH-tinkering-main/Gemma_anwer_causal/gemma_choice_results_stats.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: è®¡ç®—æ­£ç¡®ç‡\n",
    "def calculate_accuracy(results):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ€»æ­£ç¡®ç‡å’ŒæŒ‰ask-foråˆ†ç±»çš„æ­£ç¡®ç‡\n",
    "    \n",
    "    Args:\n",
    "        results: æµ‹è¯•ç»“æœåˆ—è¡¨\n",
    "    \n",
    "    Returns:\n",
    "        ç»Ÿè®¡å­—å…¸\n",
    "    \"\"\"\n",
    "    # æ€»ä½“ç»Ÿè®¡\n",
    "    total = len(results)\n",
    "    correct = sum(1 for r in results if r['is_correct'])\n",
    "    unknown = sum(1 for r in results if r['gemma_choice'] == 'unknown')\n",
    "    \n",
    "    # æŒ‰ask-foråˆ†ç±»ç»Ÿè®¡\n",
    "    effect_results = [r for r in results if r['ask-for'] == 'effect']\n",
    "    cause_results = [r for r in results if r['ask-for'] == 'cause']\n",
    "    \n",
    "    effect_total = len(effect_results)\n",
    "    effect_correct = sum(1 for r in effect_results if r['is_correct'])\n",
    "    effect_unknown = sum(1 for r in effect_results if r['gemma_choice'] == 'unknown')\n",
    "    \n",
    "    cause_total = len(cause_results)\n",
    "    cause_correct = sum(1 for r in cause_results if r['is_correct'])\n",
    "    cause_unknown = sum(1 for r in cause_results if r['gemma_choice'] == 'unknown')\n",
    "    \n",
    "    stats = {\n",
    "        'total': {\n",
    "            'total': total,\n",
    "            'correct': correct,\n",
    "            'unknown': unknown,\n",
    "            'accuracy': correct / total * 100 if total > 0 else 0,\n",
    "            'valid_accuracy': correct / (total - unknown) * 100 if (total - unknown) > 0 else 0\n",
    "        },\n",
    "        'effect': {\n",
    "            'total': effect_total,\n",
    "            'correct': effect_correct,\n",
    "            'unknown': effect_unknown,\n",
    "            'accuracy': effect_correct / effect_total * 100 if effect_total > 0 else 0,\n",
    "            'valid_accuracy': effect_correct / (effect_total - effect_unknown) * 100 if (effect_total - effect_unknown) > 0 else 0\n",
    "        },\n",
    "        'cause': {\n",
    "            'total': cause_total,\n",
    "            'correct': cause_correct,\n",
    "            'unknown': cause_unknown,\n",
    "            'accuracy': cause_correct / cause_total * 100 if cause_total > 0 else 0,\n",
    "            'valid_accuracy': cause_correct / (cause_total - cause_unknown) * 100 if (cause_total - cause_unknown) > 0 else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "def print_statistics(stats):\n",
    "    \"\"\"æ‰“å°ç»Ÿè®¡ç»“æœ\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“Š Gemma å› æœæ¨ç†é€‰æ‹©é¢˜æµ‹è¯•ç»“æœ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ æ€»ä½“æ­£ç¡®ç‡:\")\n",
    "    print(f\"   æ€»é—®é¢˜æ•°: {stats['total']['total']}\")\n",
    "    print(f\"   æ­£ç¡®å›ç­”: {stats['total']['correct']}\")\n",
    "    print(f\"   æ— æ•ˆå›ç­”: {stats['total']['unknown']}\")\n",
    "    print(f\"   æ€»æ­£ç¡®ç‡: {stats['total']['accuracy']:.2f}%\")\n",
    "    print(f\"   æœ‰æ•ˆæ­£ç¡®ç‡ (æ’é™¤unknown): {stats['total']['valid_accuracy']:.2f}%\")\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ Effectç±»å‹æ­£ç¡®ç‡:\")\n",
    "    print(f\"   æ€»é—®é¢˜æ•°: {stats['effect']['total']}\")\n",
    "    print(f\"   æ­£ç¡®å›ç­”: {stats['effect']['correct']}\")\n",
    "    print(f\"   æ— æ•ˆå›ç­”: {stats['effect']['unknown']}\")\n",
    "    print(f\"   æ­£ç¡®ç‡: {stats['effect']['accuracy']:.2f}%\")\n",
    "    print(f\"   æœ‰æ•ˆæ­£ç¡®ç‡: {stats['effect']['valid_accuracy']:.2f}%\")\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ Causeç±»å‹æ­£ç¡®ç‡:\")\n",
    "    print(f\"   æ€»é—®é¢˜æ•°: {stats['cause']['total']}\")\n",
    "    print(f\"   æ­£ç¡®å›ç­”: {stats['cause']['correct']}\")\n",
    "    print(f\"   æ— æ•ˆå›ç­”: {stats['cause']['unknown']}\")\n",
    "    print(f\"   æ­£ç¡®ç‡: {stats['cause']['accuracy']:.2f}%\")\n",
    "    print(f\"   æœ‰æ•ˆæ­£ç¡®ç‡: {stats['cause']['valid_accuracy']:.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "# è®¡ç®—å¹¶æ‰“å°ç»Ÿè®¡ç»“æœ\n",
    "stats = calculate_accuracy(results)\n",
    "print_statistics(stats)\n",
    "\n",
    "# ä¿å­˜ç»Ÿè®¡ç»“æœ\n",
    "stats_file = OUTPUT_FILE.replace('.jsonl', '_stats.json')\n",
    "with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(stats, f, indent=2, ensure_ascii=False)\n",
    "print(f\"\\nğŸ“ ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: {stats_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8153e35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‹ æ­£ç¡®å›ç­”ç¤ºä¾‹:\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1] Index: train-12680\n",
      "    Premise: Tom didn't hear what the boss has said.\n",
      "    Ask-for: effect\n",
      "    A: Tom disobeyed the new principle.\n",
      "    B: Tom couldn't be able to know the taste of whatever he eats.\n",
      "    æ­£ç¡®ç­”æ¡ˆ: A | Gemmaé€‰æ‹©: A âœ…\n",
      "\n",
      "[2] Index: train-7575\n",
      "    Premise: He saw fins.\n",
      "    Ask-for: cause\n",
      "    A: Little Jack stared at the inner part of the guppy.\n",
      "    B: Little Jack stared at the anal part of the guppy.\n",
      "    æ­£ç¡®ç­”æ¡ˆ: B | Gemmaé€‰æ‹©: B âœ…\n",
      "\n",
      "[3] Index: train-4374\n",
      "    Premise: The child is affected by herpes virus.\n",
      "    Ask-for: effect\n",
      "    A: The child must receive immediate treatment.\n",
      "    B: Children infected with the virus will experience gastrointestinal discomfort.\n",
      "    æ­£ç¡®ç­”æ¡ˆ: A | Gemmaé€‰æ‹©: A âœ…\n",
      "\n",
      "[4] Index: train-1824\n",
      "    Premise: Mendelevium is a unique element.\n",
      "    Ask-for: cause\n",
      "    A: Tom has kept carbides under a low temperature.\n",
      "    B: Lack of  naturally occurring isotopes means uniqueness.\n",
      "    æ­£ç¡®ç­”æ¡ˆ: B | Gemmaé€‰æ‹©: B âœ…\n",
      "\n",
      "[5] Index: train-3943\n",
      "    Premise: Tom's underlying values are satisfied.\n",
      "    Ask-for: effect\n",
      "    A: Tom is so happy.\n",
      "    B: Tom's boss said Tom is a very quality person.\n",
      "    æ­£ç¡®ç­”æ¡ˆ: A | Gemmaé€‰æ‹©: A âœ…\n",
      "\n",
      "\n",
      "ğŸ“‹ é”™è¯¯å›ç­”ç¤ºä¾‹:\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1] Index: train-11048\n",
      "    Premise: He goes to see the macows in the zoo.\n",
      "    Ask-for: cause\n",
      "    A: Tom likes to photograph animals.\n",
      "    B: Timothy wants to see the largest parrots.\n",
      "    æ­£ç¡®ç­”æ¡ˆ: B | Gemmaé€‰æ‹©: A âŒ\n",
      "    GemmaåŸå§‹å›ç­”: A\n",
      "\n",
      "[2] Index: train-1056\n",
      "    Premise: Jack and Tom are relatives.\n",
      "    Ask-for: effect\n",
      "    A: They live under the same roof.\n",
      "    B: They grew up.\n",
      "    æ­£ç¡®ç­”æ¡ˆ: A | Gemmaé€‰æ‹©: B âŒ\n",
      "    GemmaåŸå§‹å›ç­”: B\n",
      "\n",
      "[3] Index: train-4457\n",
      "    Premise: He has seriously obeyed the ethics.\n",
      "    Ask-for: cause\n",
      "    A: He does things blindly on principle.\n",
      "    B: The boy has done a lot of cruel things to his mother.\n",
      "    æ­£ç¡®ç­”æ¡ˆ: B | Gemmaé€‰æ‹©: A âŒ\n",
      "    GemmaåŸå§‹å›ç­”: A\n",
      "\n",
      "[4] Index: train-8405\n",
      "    Premise: He is searching for food.\n",
      "    Ask-for: cause\n",
      "    A: Tom, a prisoner in jail, is hungry.\n",
      "    B: He is a diner.\n",
      "    æ­£ç¡®ç­”æ¡ˆ: B | Gemmaé€‰æ‹©: A âŒ\n",
      "    GemmaåŸå§‹å›ç­”: A\n",
      "\n",
      "[5] Index: train-11438\n",
      "    Premise: Tom is observing some pollen grains by using a microscope.\n",
      "    Ask-for: effect\n",
      "    A: He saw many differences.\n",
      "    B: He notices the pollen grains germinate some tubes from the surfaces.\n",
      "    æ­£ç¡®ç­”æ¡ˆ: B | Gemmaé€‰æ‹©: A âŒ\n",
      "    GemmaåŸå§‹å›ç­”: A\n",
      "\n",
      "\n",
      "ğŸ“‹ æ— æ•ˆå›ç­”ç¤ºä¾‹:\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1] Index: train-1548\n",
      "    GemmaåŸå§‹å›ç­”: NEITHER A NOR B ARE LIKELY CAUSES FOR BEING BULLIED\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: æŸ¥çœ‹ç¤ºä¾‹ç»“æœ\n",
    "def show_sample_results(results, n=5):\n",
    "    \"\"\"æ˜¾ç¤ºéƒ¨åˆ†ç»“æœç¤ºä¾‹\"\"\"\n",
    "    print(\"\\nğŸ“‹ æ­£ç¡®å›ç­”ç¤ºä¾‹:\")\n",
    "    print(\"-\" * 60)\n",
    "    correct_examples = [r for r in results if r['is_correct']][:n]\n",
    "    for i, r in enumerate(correct_examples, 1):\n",
    "        print(f\"\\n[{i}] Index: {r['index']}\")\n",
    "        print(f\"    Premise: {r['premise']}\")\n",
    "        print(f\"    Ask-for: {r['ask-for']}\")\n",
    "        print(f\"    A: {r['hypothesis1']}\")\n",
    "        print(f\"    B: {r['hypothesis2']}\")\n",
    "        print(f\"    æ­£ç¡®ç­”æ¡ˆ: {r['correct_answer']} | Gemmaé€‰æ‹©: {r['gemma_choice']} âœ…\")\n",
    "    \n",
    "    print(\"\\n\\nğŸ“‹ é”™è¯¯å›ç­”ç¤ºä¾‹:\")\n",
    "    print(\"-\" * 60)\n",
    "    wrong_examples = [r for r in results if not r['is_correct'] and r['gemma_choice'] != 'unknown'][:n]\n",
    "    for i, r in enumerate(wrong_examples, 1):\n",
    "        print(f\"\\n[{i}] Index: {r['index']}\")\n",
    "        print(f\"    Premise: {r['premise']}\")\n",
    "        print(f\"    Ask-for: {r['ask-for']}\")\n",
    "        print(f\"    A: {r['hypothesis1']}\")\n",
    "        print(f\"    B: {r['hypothesis2']}\")\n",
    "        print(f\"    æ­£ç¡®ç­”æ¡ˆ: {r['correct_answer']} | Gemmaé€‰æ‹©: {r['gemma_choice']} âŒ\")\n",
    "        print(f\"    GemmaåŸå§‹å›ç­”: {r['gemma_raw_answer']}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºæ— æ•ˆå›ç­”\n",
    "    unknown_examples = [r for r in results if r['gemma_choice'] == 'unknown'][:3]\n",
    "    if unknown_examples:\n",
    "        print(\"\\n\\nğŸ“‹ æ— æ•ˆå›ç­”ç¤ºä¾‹:\")\n",
    "        print(\"-\" * 60)\n",
    "        for i, r in enumerate(unknown_examples, 1):\n",
    "            print(f\"\\n[{i}] Index: {r['index']}\")\n",
    "            print(f\"    GemmaåŸå§‹å›ç­”: {r['gemma_raw_answer']}\")\n",
    "\n",
    "# æ˜¾ç¤ºç¤ºä¾‹ç»“æœ\n",
    "show_sample_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12ab96f",
   "metadata": {},
   "source": [
    "# Part 2: é”™è¯¯é¢˜ç›®çš„èšç±»åˆ†æä¸å†æ¬¡éªŒè¯ (ç‹¬ç«‹è¿è¡Œ)\n",
    "\n",
    "**æ­¤éƒ¨åˆ†å¯ç‹¬ç«‹è¿è¡Œï¼Œæ— éœ€å…ˆæ‰§è¡Œ Part 1**\n",
    "\n",
    "å¯¹äº Gemma ç›´æ¥é€‰æ‹©é”™è¯¯çš„é¢˜ç›®ï¼š\n",
    "1. ä»æ–‡ä»¶è¯»å– Gemma é€‰æ‹©ç»“æœ (gemma_choice_results.jsonl)\n",
    "2. åŠ è½½ NV-Embed æ¨¡å‹ï¼Œå¯¹å…¶ 100 æ¬¡å›ç­”è¿›è¡Œå‘é‡åŒ–\n",
    "3. ä½¿ç”¨ HDBSCAN èšç±»ï¼Œæ‰¾åˆ°æœ€å¤§ cluster\n",
    "4. è®¡ç®— core point å’Œ topk ä»£è¡¨å‘é‡å¯¹åº”çš„åŸå§‹å›ç­”\n",
    "5. åŠ è½½ Gemma æ¨¡å‹ï¼Œåˆ¤æ–­è¿™äº›ä»£è¡¨æ€§å›ç­”ä¸å“ªä¸ªé€‰é¡¹ (H1/H2) æ›´æ¥è¿‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08babc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amax/miniconda3/envs/nvembed/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é…ç½®åŠ è½½å®Œæˆï¼\n",
      "============================================================\n",
      "åŠ è½½ NV-Embed æ¨¡å‹...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NV-Embed æ¨¡å‹åŠ è½½æˆåŠŸï¼è®¾å¤‡: cuda\n",
      "âœ… encode_texts_nvembed å‡½æ•°å®šä¹‰å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Part2 ç‹¬ç«‹è¿è¡Œ - å¯¼å…¥åº“ã€é…ç½®å’ŒåŠ è½½ NV-Embed æ¨¡å‹\n",
    "# ========== å¯¼å…¥åº“ ==========\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import hdbscan\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# ========== é…ç½®åŒºåŸŸ ==========\n",
    "# NV-Embed æ¨¡å‹è·¯å¾„\n",
    "NVEMBED_PATH = \"/home/amax/Gemma_and_NVembed/NV/snapshots/main\"\n",
    "\n",
    "# æ•°æ®è·¯å¾„é…ç½®\n",
    "GEMMA_CHOICE_RESULTS_FILE = \"/home/amax/Zixing_Jia/Vector-HASH-tinkering-main/Gemma_anwer_causal/gemma_choice_results.jsonl\"\n",
    "ANSWERED_FILE = \"/home/amax/Zixing_Jia/Vector-HASH-tinkering-main/Gemma_anwer_causal/T_1.25_Gemma_Answer/gemma_causal_answers_100.jsonl\"\n",
    "OUTPUT_FILE = \"/home/amax/Zixing_Jia/Vector-HASH-tinkering-main/Gemma_anwer_causal/gemma_choice_results.jsonl\"\n",
    "\n",
    "print(\"é…ç½®åŠ è½½å®Œæˆï¼\")\n",
    "\n",
    "# åŠ è½½ NV-Embed æ¨¡å‹\n",
    "print(\"=\" * 60)\n",
    "print(\"åŠ è½½ NV-Embed æ¨¡å‹...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "nvembed_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    NVEMBED_PATH,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "nvembed_model = AutoModel.from_pretrained(\n",
    "    NVEMBED_PATH,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "nvembed_model = nvembed_model.to(device)\n",
    "nvembed_model.eval()\n",
    "\n",
    "print(f\"âœ… NV-Embed æ¨¡å‹åŠ è½½æˆåŠŸï¼è®¾å¤‡: {device}\")\n",
    "\n",
    "\n",
    "def encode_texts_nvembed(texts, max_length=4096, batch_size=32):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ NV-Embed æ‰¹é‡ç¼–ç æ–‡æœ¬ä¸ºå‘é‡\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    eos_token = nvembed_tokenizer.eos_token if nvembed_tokenizer.eos_token else \"</s>\"\n",
    "    texts_with_eos = [text + eos_token for text in texts]\n",
    "    \n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts_with_eos), batch_size):\n",
    "        batch_texts = texts_with_eos[i:i+batch_size]\n",
    "        \n",
    "        batch_dict = nvembed_tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        batch_dict = {k: v.to(device) for k, v in batch_dict.items()}\n",
    "        \n",
    "        attention_mask = batch_dict[\"attention_mask\"]\n",
    "        seq_lengths = attention_mask.sum(dim=1)\n",
    "        pool_mask = torch.zeros_like(attention_mask)\n",
    "        for j, length in enumerate(seq_lengths):\n",
    "            pool_mask[j, length - 1] = 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = nvembed_model(\n",
    "                input_ids=batch_dict[\"input_ids\"],\n",
    "                attention_mask=attention_mask,\n",
    "                pool_mask=pool_mask\n",
    "            )\n",
    "            \n",
    "            if isinstance(outputs, dict):\n",
    "                if \"sentence_embeddings\" in outputs:\n",
    "                    embeddings = outputs[\"sentence_embeddings\"]\n",
    "                elif \"last_hidden_state\" in outputs:\n",
    "                    embeddings = outputs[\"last_hidden_state\"][:, -1, :]\n",
    "                else:\n",
    "                    raise ValueError(f\"æ— æ³•ä»è¾“å‡ºä¸­æå–åµŒå…¥å‘é‡: {outputs.keys()}\")\n",
    "            elif hasattr(outputs, \"sentence_embeddings\"):\n",
    "                embeddings = outputs.sentence_embeddings\n",
    "            elif hasattr(outputs, \"last_hidden_state\"):\n",
    "                embeddings = outputs.last_hidden_state[:, -1, :]\n",
    "            else:\n",
    "                embeddings = outputs\n",
    "        \n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        embeddings_cpu = embeddings.float().cpu()\n",
    "        all_embeddings.append(np.array(embeddings_cpu.tolist()))\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "print(\"âœ… encode_texts_nvembed å‡½æ•°å®šä¹‰å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d9a056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ä»æ–‡ä»¶è¯»å– Gemma é€‰æ‹©ç»“æœ...\n",
      "============================================================\n",
      "ğŸ“Š ä»æ–‡ä»¶è¯»å–äº† 800 æ¡é€‰æ‹©ç»“æœ\n",
      "ğŸ“Š åŸå§‹æ­£ç¡®ç‡: 672/800 = 84.00%\n",
      "\n",
      "============================================================\n",
      "æ‰¾å‡º Gemma å›ç­”é”™è¯¯çš„é¢˜ç›®...\n",
      "============================================================\n",
      "ğŸ“Š é”™è¯¯é¢˜ç›®æ•°é‡: 127\n",
      "\n",
      "ğŸ“‚ åŠ è½½ Gemma çš„ 100 æ¬¡å›ç­”...\n",
      "âœ… åŠ è½½å®Œæˆï¼\n",
      "ğŸ“Š æœ‰ 100 æ¬¡å›ç­”çš„é”™è¯¯é¢˜ç›®æ•°: 127\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: ä»æ–‡ä»¶è¯»å– Gemma é€‰æ‹©ç»“æœï¼Œæ‰¾å‡ºé”™è¯¯é¢˜ç›®å¹¶åŠ è½½å…¶100æ¬¡å›ç­”\n",
    "print(\"=\" * 60)\n",
    "print(\"ä»æ–‡ä»¶è¯»å– Gemma é€‰æ‹©ç»“æœ...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========== ä»æ–‡ä»¶è¯»å– Gemma é€‰æ‹©ç»“æœ ==========\n",
    "results = []\n",
    "with open(GEMMA_CHOICE_RESULTS_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            results.append(json.loads(line))\n",
    "\n",
    "print(f\"ğŸ“Š ä»æ–‡ä»¶è¯»å–äº† {len(results)} æ¡é€‰æ‹©ç»“æœ\")\n",
    "\n",
    "# ç»Ÿè®¡æ­£ç¡®ç‡\n",
    "total_correct = sum(1 for r in results if r['is_correct'])\n",
    "print(f\"ğŸ“Š åŸå§‹æ­£ç¡®ç‡: {total_correct}/{len(results)} = {total_correct/len(results)*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"æ‰¾å‡º Gemma å›ç­”é”™è¯¯çš„é¢˜ç›®...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# è·å–é”™è¯¯é¢˜ç›®çš„ index\n",
    "wrong_results = [r for r in results if not r['is_correct'] and r['gemma_choice'] != 'unknown']\n",
    "wrong_indices = [r['index'] for r in wrong_results]\n",
    "\n",
    "print(f\"ğŸ“Š é”™è¯¯é¢˜ç›®æ•°é‡: {len(wrong_indices)}\")\n",
    "\n",
    "# åŠ è½½ Gemma çš„ 100 æ¬¡å›ç­”\n",
    "print(\"\\nğŸ“‚ åŠ è½½ Gemma çš„ 100 æ¬¡å›ç­”...\")\n",
    "gemma_100_answers = defaultdict(list)\n",
    "\n",
    "with open(ANSWERED_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            record = json.loads(line)\n",
    "            idx = record['index']\n",
    "            if idx in wrong_indices:\n",
    "                gemma_100_answers[idx].append({\n",
    "                    'answer': record['Gemma_answer'],\n",
    "                    'round': record['answer_round'],\n",
    "                    'ask_for': record['ask-for'],\n",
    "                    'premise': record['premise']\n",
    "                })\n",
    "\n",
    "print(f\"âœ… åŠ è½½å®Œæˆï¼\")\n",
    "print(f\"ğŸ“Š æœ‰ 100 æ¬¡å›ç­”çš„é”™è¯¯é¢˜ç›®æ•°: {len([k for k, v in gemma_100_answers.items() if len(v) >= 100])}\")\n",
    "\n",
    "# åˆ›å»º index -> wrong_result çš„æ˜ å°„\n",
    "wrong_results_dict = {r['index']: r for r in wrong_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c2f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "å¯¹é”™è¯¯é¢˜ç›®è¿›è¡Œ HDBSCAN èšç±»åˆ†æ...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "åˆ†æé”™è¯¯é¢˜ç›®:   0%|          | 0/127 [00:00<?, ?it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:   1%|          | 1/127 [00:01<02:06,  1.00s/it]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:   2%|â–         | 2/127 [00:01<01:12,  1.73it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:   2%|â–         | 3/127 [00:01<00:55,  2.22it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:   3%|â–         | 4/127 [00:01<00:46,  2.67it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:   4%|â–         | 5/127 [00:02<00:43,  2.78it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:   5%|â–         | 6/127 [00:02<00:41,  2.90it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:   6%|â–Œ         | 7/127 [00:02<00:40,  3.00it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:   6%|â–‹         | 8/127 [00:03<00:38,  3.12it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:   7%|â–‹         | 9/127 [00:03<00:37,  3.11it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:   8%|â–Š         | 10/127 [00:03<00:37,  3.12it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:   9%|â–Š         | 11/127 [00:04<00:36,  3.19it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:   9%|â–‰         | 12/127 [00:04<00:36,  3.18it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  10%|â–ˆ         | 13/127 [00:04<00:36,  3.10it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  11%|â–ˆ         | 14/127 [00:05<00:36,  3.13it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  12%|â–ˆâ–        | 15/127 [00:05<00:35,  3.15it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  13%|â–ˆâ–        | 16/127 [00:05<00:35,  3.16it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  13%|â–ˆâ–        | 17/127 [00:05<00:34,  3.16it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  14%|â–ˆâ–        | 18/127 [00:06<00:34,  3.12it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  15%|â–ˆâ–        | 19/127 [00:06<00:35,  3.08it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  16%|â–ˆâ–Œ        | 20/127 [00:06<00:34,  3.06it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  17%|â–ˆâ–‹        | 21/127 [00:07<00:35,  3.00it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  17%|â–ˆâ–‹        | 22/127 [00:07<00:34,  3.03it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  18%|â–ˆâ–Š        | 23/127 [00:07<00:33,  3.14it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  19%|â–ˆâ–‰        | 24/127 [00:08<00:32,  3.20it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  20%|â–ˆâ–‰        | 25/127 [00:08<00:33,  3.07it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  20%|â–ˆâ–ˆ        | 26/127 [00:08<00:32,  3.12it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  21%|â–ˆâ–ˆâ–       | 27/127 [00:09<00:31,  3.17it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  22%|â–ˆâ–ˆâ–       | 28/127 [00:09<00:30,  3.20it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  23%|â–ˆâ–ˆâ–       | 29/127 [00:09<00:31,  3.11it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  24%|â–ˆâ–ˆâ–       | 30/127 [00:10<00:30,  3.21it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  24%|â–ˆâ–ˆâ–       | 31/127 [00:10<00:30,  3.14it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  25%|â–ˆâ–ˆâ–Œ       | 32/127 [00:10<00:29,  3.24it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  26%|â–ˆâ–ˆâ–Œ       | 33/127 [00:11<00:28,  3.26it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  27%|â–ˆâ–ˆâ–‹       | 34/127 [00:11<00:28,  3.24it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  28%|â–ˆâ–ˆâ–Š       | 35/127 [00:11<00:29,  3.10it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  28%|â–ˆâ–ˆâ–Š       | 36/127 [00:12<00:29,  3.09it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  29%|â–ˆâ–ˆâ–‰       | 37/127 [00:12<00:29,  3.09it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  30%|â–ˆâ–ˆâ–‰       | 38/127 [00:12<00:28,  3.17it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  31%|â–ˆâ–ˆâ–ˆ       | 39/127 [00:12<00:26,  3.26it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  31%|â–ˆâ–ˆâ–ˆâ–      | 40/127 [00:13<00:27,  3.20it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  32%|â–ˆâ–ˆâ–ˆâ–      | 41/127 [00:13<00:27,  3.18it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  33%|â–ˆâ–ˆâ–ˆâ–      | 42/127 [00:13<00:26,  3.21it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  34%|â–ˆâ–ˆâ–ˆâ–      | 43/127 [00:14<00:26,  3.14it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  35%|â–ˆâ–ˆâ–ˆâ–      | 44/127 [00:14<00:26,  3.10it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 45/127 [00:14<00:26,  3.07it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 46/127 [00:15<00:25,  3.13it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 47/127 [00:15<00:25,  3.14it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 48/127 [00:15<00:25,  3.08it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 49/127 [00:16<00:24,  3.14it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 50/127 [00:16<00:23,  3.21it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 51/127 [00:16<00:23,  3.22it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 52/127 [00:17<00:23,  3.22it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 53/127 [00:17<00:22,  3.23it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 54/127 [00:17<00:22,  3.22it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 55/127 [00:17<00:22,  3.26it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 56/127 [00:18<00:21,  3.27it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 57/127 [00:18<00:21,  3.22it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 58/127 [00:18<00:21,  3.15it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 59/127 [00:19<00:21,  3.15it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 60/127 [00:19<00:20,  3.20it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 61/127 [00:19<00:21,  3.14it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 62/127 [00:20<00:21,  3.09it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 63/127 [00:20<00:20,  3.17it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 64/127 [00:20<00:20,  3.12it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 65/127 [00:21<00:19,  3.11it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 66/127 [00:21<00:19,  3.08it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 67/127 [00:21<00:19,  3.12it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 68/127 [00:22<00:18,  3.16it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 69/127 [00:22<00:18,  3.16it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 70/127 [00:22<00:18,  3.12it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 71/127 [00:23<00:18,  3.08it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 72/127 [00:23<00:17,  3.15it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 73/127 [00:23<00:17,  3.14it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 74/127 [00:24<00:16,  3.13it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 75/127 [00:24<00:16,  3.16it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 76/127 [00:24<00:15,  3.20it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 77/127 [00:24<00:15,  3.15it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 78/127 [00:25<00:15,  3.20it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 79/127 [00:25<00:15,  3.10it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 80/127 [00:25<00:14,  3.26it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 81/127 [00:26<00:14,  3.23it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 82/127 [00:26<00:13,  3.32it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 83/127 [00:26<00:13,  3.21it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 84/127 [00:27<00:13,  3.17it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 85/127 [00:27<00:13,  3.12it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 86/127 [00:27<00:12,  3.18it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 87/127 [00:28<00:12,  3.12it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 88/127 [00:28<00:12,  3.08it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 89/127 [00:28<00:12,  3.07it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 90/127 [00:29<00:11,  3.15it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 91/127 [00:29<00:11,  3.22it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 92/127 [00:29<00:11,  3.17it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 93/127 [00:30<00:10,  3.20it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 94/127 [00:30<00:10,  3.11it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 95/127 [00:30<00:10,  3.09it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 96/127 [00:31<00:10,  3.06it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 97/127 [00:31<00:09,  3.07it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 98/127 [00:31<00:09,  3.10it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 99/127 [00:31<00:09,  3.08it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 100/127 [00:32<00:08,  3.15it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 101/127 [00:32<00:08,  3.14it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 102/127 [00:32<00:08,  3.07it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 103/127 [00:33<00:07,  3.07it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 104/127 [00:33<00:07,  3.04it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 105/127 [00:33<00:07,  3.10it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 106/127 [00:34<00:06,  3.02it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 107/127 [00:34<00:06,  3.02it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 108/127 [00:34<00:06,  2.98it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 109/127 [00:35<00:05,  3.01it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 110/127 [00:35<00:05,  3.11it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 111/127 [00:35<00:05,  3.09it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 112/127 [00:36<00:04,  3.14it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 113/127 [00:36<00:04,  3.11it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 114/127 [00:36<00:04,  3.16it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 115/127 [00:37<00:03,  3.18it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 116/127 [00:37<00:03,  3.13it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 117/127 [00:37<00:03,  3.24it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 118/127 [00:38<00:02,  3.16it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 119/127 [00:38<00:02,  3.17it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 120/127 [00:38<00:02,  3.15it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 121/127 [00:39<00:01,  3.06it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 122/127 [00:39<00:01,  3.04it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 123/127 [00:39<00:01,  3.07it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 124/127 [00:40<00:00,  3.06it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 125/127 [00:40<00:00,  3.23it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 126/127 [00:40<00:00,  3.09it/s]/home/amax/miniconda3/envs/nvembed/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "åˆ†æé”™è¯¯é¢˜ç›®: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:40<00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… åˆ†æå®Œæˆï¼å…±å¤„ç† 127 ä¸ªé”™è¯¯é¢˜ç›®\n",
      "ğŸ“ èšç±»åˆ†æç»“æœå·²ä¿å­˜åˆ°: /home/amax/Zixing_Jia/Vector-HASH-tinkering-main/Gemma_anwer_causal/wrong_analysis_results.jsonl\n",
      "   ï¼ˆåˆ‡æ¢ç¯å¢ƒé‡å¯å†…æ ¸åï¼Œå¯ä» Cell 10 ç»§ç»­è¿è¡Œï¼‰\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: å¯¹é”™è¯¯é¢˜ç›®è¿›è¡Œ HDBSCAN èšç±»ï¼Œæ‰¾åˆ° max cluster çš„ core_point å’Œ topk\n",
    "print(\"=\" * 60)\n",
    "print(\"å¯¹é”™è¯¯é¢˜ç›®è¿›è¡Œ HDBSCAN èšç±»åˆ†æ...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# HDBSCAN é…ç½®\n",
    "HDBSCAN_MIN_CLUSTER_SIZE = 5\n",
    "HDBSCAN_MIN_SAMPLES = 3\n",
    "\n",
    "def analyze_wrong_question(idx, answers_list, wrong_info):\n",
    "    \"\"\"\n",
    "    åˆ†æå•ä¸ªé”™è¯¯é¢˜ç›®ï¼šèšç±»å¹¶æ‰¾åˆ° max cluster çš„ä»£è¡¨å‘é‡\n",
    "    \n",
    "    Returns:\n",
    "        dict: åŒ…å« core_point_text, topk_text ç­‰ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    # è·å–æ‰€æœ‰å›ç­”æ–‡æœ¬\n",
    "    answer_texts = [a['answer'] for a in answers_list[:100]]\n",
    "    \n",
    "    # å‘é‡åŒ–æ‰€æœ‰å›ç­”\n",
    "    answer_embeddings = encode_texts_nvembed(answer_texts)\n",
    "    \n",
    "    # HDBSCAN èšç±»\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=HDBSCAN_MIN_CLUSTER_SIZE,\n",
    "        min_samples=HDBSCAN_MIN_SAMPLES,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom'\n",
    "    )\n",
    "    cluster_labels = clusterer.fit_predict(answer_embeddings)\n",
    "    \n",
    "    # æ‰¾åˆ°å„ä¸ª cluster çš„å¤§å°\n",
    "    unique_clusters = set(cluster_labels)\n",
    "    cluster_sizes = {}\n",
    "    for cid in unique_clusters:\n",
    "        if cid >= 0:  # æ’é™¤å™ªå£°ç‚¹ (-1)\n",
    "            cluster_sizes[cid] = np.sum(cluster_labels == cid)\n",
    "    \n",
    "    if not cluster_sizes:\n",
    "        # æ²¡æœ‰æœ‰æ•ˆèšç±»ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®\n",
    "        max_cluster_id = -1\n",
    "        cluster_mask = np.ones(len(answer_texts), dtype=bool)\n",
    "    else:\n",
    "        # æ‰¾åˆ°æœ€å¤§çš„ cluster\n",
    "        max_cluster_id = max(cluster_sizes, key=cluster_sizes.get)\n",
    "        cluster_mask = cluster_labels == max_cluster_id\n",
    "    \n",
    "    cluster_indices = np.where(cluster_mask)[0]\n",
    "    cluster_embeddings = answer_embeddings[cluster_mask]\n",
    "    cluster_texts = [answer_texts[i] for i in cluster_indices]\n",
    "    \n",
    "    # è®¡ç®— centroid\n",
    "    centroid = np.mean(cluster_embeddings, axis=0)\n",
    "    centroid = centroid / (np.linalg.norm(centroid) + 1e-12)\n",
    "    \n",
    "    # è®¡ç®— core_point (åˆ°å…¶ä»–ç‚¹å¹³å‡è·ç¦»æœ€å°çš„ç‚¹)\n",
    "    if len(cluster_embeddings) > 1:\n",
    "        pairwise_distances = cdist(cluster_embeddings, cluster_embeddings, metric='cosine')\n",
    "        avg_distances = np.mean(pairwise_distances, axis=1)\n",
    "        core_idx_in_cluster = np.argmin(avg_distances)\n",
    "        core_point_vec = cluster_embeddings[core_idx_in_cluster]\n",
    "        core_point_text = cluster_texts[core_idx_in_cluster]\n",
    "    else:\n",
    "        core_idx_in_cluster = 0\n",
    "        core_point_vec = cluster_embeddings[0]\n",
    "        core_point_text = cluster_texts[0]\n",
    "    \n",
    "    # è®¡ç®— topk (è·ç¦» centroid æœ€è¿‘çš„ k ä¸ªç‚¹çš„å¹³å‡)\n",
    "    distances_to_centroid = cdist([centroid], cluster_embeddings, metric='cosine')[0]\n",
    "    k = min(3, len(cluster_embeddings))\n",
    "    topk_indices_in_cluster = np.argsort(distances_to_centroid)[:k]\n",
    "    topk_vec = np.mean(cluster_embeddings[topk_indices_in_cluster], axis=0)\n",
    "    topk_vec = topk_vec / (np.linalg.norm(topk_vec) + 1e-12)\n",
    "    topk_texts = [cluster_texts[i] for i in topk_indices_in_cluster]\n",
    "    \n",
    "    # æ‰¾åˆ°è·ç¦» topk_vec æœ€è¿‘çš„åŸå§‹æ–‡æœ¬\n",
    "    topk_distances = cdist([topk_vec], cluster_embeddings, metric='cosine')[0]\n",
    "    closest_to_topk_idx = np.argmin(topk_distances)\n",
    "    topk_representative_text = cluster_texts[closest_to_topk_idx]\n",
    "    \n",
    "    return {\n",
    "        'index': idx,\n",
    "        'premise': wrong_info['premise'],\n",
    "        'ask_for': wrong_info['ask-for'],\n",
    "        'h1': wrong_info['hypothesis1'],\n",
    "        'h2': wrong_info['hypothesis2'],\n",
    "        'correct_answer': wrong_info['correct_answer'],\n",
    "        'gemma_wrong_choice': wrong_info['gemma_choice'],\n",
    "        'max_cluster_id': int(max_cluster_id),\n",
    "        'max_cluster_size': int(np.sum(cluster_mask)),\n",
    "        'n_clusters': len(cluster_sizes),\n",
    "        'core_point_text': core_point_text,\n",
    "        'topk_texts': topk_texts,\n",
    "        'topk_representative_text': topk_representative_text,\n",
    "        'core_point_vec': core_point_vec,\n",
    "        'topk_vec': topk_vec,\n",
    "        'centroid_vec': centroid\n",
    "    }\n",
    "\n",
    "\n",
    "# å¤„ç†æ‰€æœ‰é”™è¯¯é¢˜ç›®\n",
    "wrong_analysis_results = []\n",
    "\n",
    "for idx in tqdm(wrong_indices, desc=\"åˆ†æé”™è¯¯é¢˜ç›®\"):\n",
    "    if idx not in gemma_100_answers or len(gemma_100_answers[idx]) < 100:\n",
    "        print(f\"âš ï¸ è·³è¿‡ {idx}: å›ç­”æ•°ä¸è¶³ 100\")\n",
    "        continue\n",
    "    \n",
    "    answers_list = gemma_100_answers[idx]\n",
    "    wrong_info = wrong_results_dict[idx]\n",
    "    \n",
    "    result = analyze_wrong_question(idx, answers_list, wrong_info)\n",
    "    wrong_analysis_results.append(result)\n",
    "\n",
    "print(f\"\\nâœ… åˆ†æå®Œæˆï¼å…±å¤„ç† {len(wrong_analysis_results)} ä¸ªé”™è¯¯é¢˜ç›®\")\n",
    "\n",
    "# ========== ä¿å­˜èšç±»åˆ†æç»“æœåˆ°æ–‡ä»¶ï¼ˆç”¨äºå†…æ ¸é‡å¯åæ¢å¤ï¼‰ ==========\n",
    "WRONG_ANALYSIS_FILE = \"/home/amax/Zixing_Jia/Vector-HASH-tinkering-main/Gemma_anwer_causal/wrong_analysis_results.jsonl\"\n",
    "\n",
    "with open(WRONG_ANALYSIS_FILE, 'w', encoding='utf-8') as f:\n",
    "    for r in wrong_analysis_results:\n",
    "        # ä¿å­˜æ—¶æ’é™¤å‘é‡æ•°æ®ï¼ˆæ— æ³•JSONåºåˆ—åŒ–ä¸”åç»­æ­¥éª¤ä¸éœ€è¦ï¼‰\n",
    "        r_save = {k: v for k, v in r.items() if not k.endswith('_vec')}\n",
    "        f.write(json.dumps(r_save, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"ğŸ“ èšç±»åˆ†æç»“æœå·²ä¿å­˜åˆ°: {WRONG_ANALYSIS_FILE}\")\n",
    "print(\"   ï¼ˆåˆ‡æ¢ç¯å¢ƒé‡å¯å†…æ ¸åï¼Œå¯ä» Cell 10 ç»§ç»­è¿è¡Œï¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a56108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amax/miniconda3/envs/gemma/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ä»æ–‡ä»¶æ¢å¤èšç±»åˆ†æç»“æœ...\n",
      "âœ… å·²æ¢å¤ 127 ä¸ªé”™è¯¯é¢˜ç›®çš„åˆ†æç»“æœ\n",
      "\n",
      "============================================================\n",
      "åŠ è½½ Gemma æ¨¡å‹...\n",
      "============================================================\n",
      "ğŸ“ æœ¬åœ°æ¨¡å‹è·¯å¾„: /home/amax/Gemma_and_NVembed/Gemma/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819\n",
      "\n",
      "â³ æ­£åœ¨åŠ è½½ Gemma tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenizer åŠ è½½æˆåŠŸï¼\n",
      "\n",
      "â³ æ­£åœ¨åŠ è½½ Gemma æ¨¡å‹ï¼ˆçº¦éœ€ 1-2 åˆ†é’Ÿï¼‰...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gemma æ¨¡å‹åŠ è½½æˆåŠŸï¼\n",
      "ğŸ’» æ¨¡å‹è®¾å¤‡: cuda:0\n",
      "ğŸ“Š æ¨¡å‹å‚æ•°é‡: 9.24B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: åŠ è½½ Gemma æ¨¡å‹ (ç”¨äºåˆ¤æ–­ç›¸ä¼¼åº¦)\n",
    "# ========== å¯¼å…¥åº“ (å†…æ ¸é‡å¯åéœ€è¦) ==========\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Gemma æ¨¡å‹è·¯å¾„é…ç½®\n",
    "GEMMA_CACHE_DIR = \"/home/amax/Gemma_and_NVembed/Gemma\"\n",
    "GEMMA_MODEL_NAME = \"google/gemma-2-9b-it\"\n",
    "\n",
    "# ========== ä»æ–‡ä»¶æ¢å¤èšç±»åˆ†æç»“æœï¼ˆå†…æ ¸é‡å¯åä½¿ç”¨ï¼‰ ==========\n",
    "WRONG_ANALYSIS_FILE = \"/home/amax/Zixing_Jia/Vector-HASH-tinkering-main/Gemma_anwer_causal/wrong_analysis_results.jsonl\"\n",
    "\n",
    "if 'wrong_analysis_results' not in dir():\n",
    "    print(\"ğŸ“‚ ä»æ–‡ä»¶æ¢å¤èšç±»åˆ†æç»“æœ...\")\n",
    "    wrong_analysis_results = []\n",
    "    with open(WRONG_ANALYSIS_FILE, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                wrong_analysis_results.append(json.loads(line))\n",
    "    print(f\"âœ… å·²æ¢å¤ {len(wrong_analysis_results)} ä¸ªé”™è¯¯é¢˜ç›®çš„åˆ†æç»“æœ\")\n",
    "else:\n",
    "    print(\"âœ… èšç±»åˆ†æç»“æœå·²åœ¨å†…å­˜ä¸­\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"åŠ è½½ Gemma æ¨¡å‹...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# æŸ¥æ‰¾æœ¬åœ°æ¨¡å‹è·¯å¾„\n",
    "model_base = os.path.join(GEMMA_CACHE_DIR, \"models--google--gemma-2-9b-it\", \"snapshots\")\n",
    "snapshots = os.listdir(model_base)\n",
    "local_model_path = os.path.join(model_base, snapshots[0])\n",
    "print(f\"ğŸ“ æœ¬åœ°æ¨¡å‹è·¯å¾„: {local_model_path}\")\n",
    "\n",
    "# åŠ è½½ tokenizer\n",
    "print(\"\\nâ³ æ­£åœ¨åŠ è½½ Gemma tokenizer...\")\n",
    "gemma_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    local_model_path,\n",
    "    local_files_only=True\n",
    ")\n",
    "print(\"âœ… Tokenizer åŠ è½½æˆåŠŸï¼\")\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "print(\"\\nâ³ æ­£åœ¨åŠ è½½ Gemma æ¨¡å‹ï¼ˆçº¦éœ€ 1-2 åˆ†é’Ÿï¼‰...\")\n",
    "gemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    local_files_only=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… Gemma æ¨¡å‹åŠ è½½æˆåŠŸï¼\")\n",
    "print(f\"ğŸ’» æ¨¡å‹è®¾å¤‡: {gemma_model.device}\")\n",
    "print(f\"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {gemma_model.num_parameters() / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb1e3cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "è®© Gemma åˆ¤æ–­ä»£è¡¨æ€§å›ç­”ä¸å“ªä¸ªé€‰é¡¹æ›´æ¥è¿‘...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma é‡æ–°åˆ¤æ–­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:31<00:00,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… é‡æ–°åˆ¤æ–­å®Œæˆï¼å…±å¤„ç† 127 ä¸ªé¢˜ç›®\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Cell 11: è®© Gemma åˆ¤æ–­ core_point/topk å›ç­”ä¸å“ªä¸ªé€‰é¡¹æ›´æ¥è¿‘\n",
    "print(\"=\" * 60)\n",
    "print(\"è®© Gemma åˆ¤æ–­ä»£è¡¨æ€§å›ç­”ä¸å“ªä¸ªé€‰é¡¹æ›´æ¥è¿‘...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def construct_similarity_prompt(representative_text, h1, h2, ask_for, premise):\n",
    "    \"\"\"\n",
    "    æ„é€ è®© Gemma åˆ¤æ–­ç›¸ä¼¼åº¦çš„ prompt\n",
    "    \n",
    "    Args:\n",
    "        representative_text: core_point æˆ– topk å¯¹åº”çš„ä»£è¡¨æ€§å›ç­”\n",
    "        h1: hypothesis1\n",
    "        h2: hypothesis2\n",
    "        ask_for: 'cause' æˆ– 'effect'\n",
    "        premise: åŸå§‹å‰æ/èƒŒæ™¯\n",
    "    \n",
    "    Returns:\n",
    "        prompt å­—ç¬¦ä¸²\n",
    "    \"\"\"\n",
    "    if ask_for == 'cause':\n",
    "        question_type = \"What is the most likely cause of this situation?\"\n",
    "    else:\n",
    "        question_type = \"What is the most likely effect of this situation?\"\n",
    "    \n",
    "    prompt = f\"\"\"You are a semantic similarity expert. Given a premise and a question, I will show you one generated answer and two candidate options. Please determine which option is more semantically similar to the generated answer.\n",
    "\n",
    "Premise: {premise}\n",
    "Question: {question_type}\n",
    "\n",
    "Generated Answer: {representative_text}\n",
    "\n",
    "Option A: {h1}\n",
    "Option B: {h2}\n",
    "\n",
    "Which option (A or B) is more semantically similar to the generated answer? Just output A or B, nothing else.\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def get_gemma_similarity_choice(representative_text, h1, h2, ask_for, premise):\n",
    "    \"\"\"\n",
    "    è®© Gemma åˆ¤æ–­ä»£è¡¨æ€§å›ç­”ä¸å“ªä¸ªé€‰é¡¹æ›´æ¥è¿‘\n",
    "    \n",
    "    Returns:\n",
    "        'A', 'B' æˆ– 'unknown'\n",
    "    \"\"\"\n",
    "    prompt = construct_similarity_prompt(representative_text, h1, h2, ask_for, premise)\n",
    "    \n",
    "    chat = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    prompt_formatted = gemma_tokenizer.apply_chat_template(\n",
    "        chat, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = gemma_tokenizer(prompt_formatted, return_tensors=\"pt\").to(gemma_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = gemma_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            pad_token_id=gemma_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = gemma_tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    ).strip().upper()\n",
    "    \n",
    "    if 'A' in generated_text[:5]:\n",
    "        return 'A', generated_text\n",
    "    elif 'B' in generated_text[:5]:\n",
    "        return 'B', generated_text\n",
    "    else:\n",
    "        return 'unknown', generated_text\n",
    "\n",
    "\n",
    "# å¯¹æ¯ä¸ªé”™è¯¯é¢˜ç›®ï¼Œç”¨ core_point å’Œ topk è®© Gemma é‡æ–°åˆ¤æ–­\n",
    "verification_results = []\n",
    "\n",
    "for analysis in tqdm(wrong_analysis_results, desc=\"Gemma é‡æ–°åˆ¤æ–­\"):\n",
    "    idx = analysis['index']\n",
    "    h1 = analysis['h1']\n",
    "    h2 = analysis['h2']\n",
    "    ask_for = analysis['ask_for']\n",
    "    correct_answer = analysis['correct_answer']\n",
    "    premise = analysis['premise']\n",
    "    \n",
    "    # ä½¿ç”¨ core_point_text è®© Gemma åˆ¤æ–­\n",
    "    core_choice, core_raw = get_gemma_similarity_choice(\n",
    "        analysis['core_point_text'], h1, h2, ask_for, premise\n",
    "    )\n",
    "    core_correct = (core_choice == correct_answer)\n",
    "    \n",
    "    # ä½¿ç”¨ topk_representative_text è®© Gemma åˆ¤æ–­\n",
    "    topk_choice, topk_raw = get_gemma_similarity_choice(\n",
    "        analysis['topk_representative_text'], h1, h2, ask_for, premise\n",
    "    )\n",
    "    topk_correct = (topk_choice == correct_answer)\n",
    "    \n",
    "    verification_results.append({\n",
    "        'index': idx,\n",
    "        'premise': analysis['premise'],\n",
    "        'ask_for': ask_for,\n",
    "        'h1': h1,\n",
    "        'h2': h2,\n",
    "        'correct_answer': correct_answer,\n",
    "        'original_wrong_choice': analysis['gemma_wrong_choice'],\n",
    "        'max_cluster_size': analysis['max_cluster_size'],\n",
    "        'n_clusters': analysis['n_clusters'],\n",
    "        # Core point ç»“æœ\n",
    "        'core_point_text': analysis['core_point_text'],\n",
    "        'core_point_choice': core_choice,\n",
    "        'core_point_raw_answer': core_raw,\n",
    "        'core_point_correct': core_correct,\n",
    "        # TopK ç»“æœ\n",
    "        'topk_representative_text': analysis['topk_representative_text'],\n",
    "        'topk_choice': topk_choice,\n",
    "        'topk_raw_answer': topk_raw,\n",
    "        'topk_correct': topk_correct,\n",
    "    })\n",
    "\n",
    "print(f\"\\nâœ… é‡æ–°åˆ¤æ–­å®Œæˆï¼å…±å¤„ç† {len(verification_results)} ä¸ªé¢˜ç›®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "326ed9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“Š é”™è¯¯é¢˜ç›®é‡æ–°åˆ¤æ–­ç»“æœç»Ÿè®¡\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ æ€»ä½“ç»Ÿè®¡ (åŸå§‹é”™è¯¯é¢˜ç›®æ•°: 127):\n",
      "   Core Point æ–¹æ³•:\n",
      "     - ä¿®æ­£æ­£ç¡®: 46/127 = 36.22%\n",
      "     - æ— æ•ˆå›ç­”: 0\n",
      "   TopK æ–¹æ³•:\n",
      "     - ä¿®æ­£æ­£ç¡®: 47/127 = 37.01%\n",
      "     - æ— æ•ˆå›ç­”: 0\n",
      "\n",
      "ğŸ“ˆ æŒ‰ Ask-for åˆ†ç±»ç»Ÿè®¡:\n",
      "\n",
      "   CAUSE ç±» (67 ä¸ª):\n",
      "     - Core Point ä¿®æ­£æ­£ç¡®: 26/67 = 38.81%\n",
      "     - TopK ä¿®æ­£æ­£ç¡®: 25/67 = 37.31%\n",
      "\n",
      "   EFFECT ç±» (60 ä¸ª):\n",
      "     - Core Point ä¿®æ­£æ­£ç¡®: 20/60 = 33.33%\n",
      "     - TopK ä¿®æ­£æ­£ç¡®: 22/60 = 36.67%\n",
      "\n",
      "ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: /home/amax/Zixing_Jia/Vector-HASH-tinkering-main/Gemma_anwer_causal/gemma_choice_results_verification_results.jsonl\n",
      "ğŸ“ ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: /home/amax/Zixing_Jia/Vector-HASH-tinkering-main/Gemma_anwer_causal/gemma_choice_results_verification_stats.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: ç»Ÿè®¡å’Œåˆ†æç»“æœ\n",
    "# é…ç½®æ–‡ä»¶è·¯å¾„\n",
    "OUTPUT_FILE = \"/home/amax/Zixing_Jia/Vector-HASH-tinkering-main/Gemma_anwer_causal/gemma_choice_results.jsonl\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š é”™è¯¯é¢˜ç›®é‡æ–°åˆ¤æ–­ç»“æœç»Ÿè®¡\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# æ€»ä½“ç»Ÿè®¡\n",
    "total_wrong = len(verification_results)\n",
    "core_correct_count = sum(1 for r in verification_results if r['core_point_correct'])\n",
    "topk_correct_count = sum(1 for r in verification_results if r['topk_correct'])\n",
    "core_unknown_count = sum(1 for r in verification_results if r['core_point_choice'] == 'unknown')\n",
    "topk_unknown_count = sum(1 for r in verification_results if r['topk_choice'] == 'unknown')\n",
    "\n",
    "print(f\"\\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡ (åŸå§‹é”™è¯¯é¢˜ç›®æ•°: {total_wrong}):\")\n",
    "print(f\"   Core Point æ–¹æ³•:\")\n",
    "print(f\"     - ä¿®æ­£æ­£ç¡®: {core_correct_count}/{total_wrong} = {core_correct_count/total_wrong*100:.2f}%\")\n",
    "print(f\"     - æ— æ•ˆå›ç­”: {core_unknown_count}\")\n",
    "print(f\"   TopK æ–¹æ³•:\")\n",
    "print(f\"     - ä¿®æ­£æ­£ç¡®: {topk_correct_count}/{total_wrong} = {topk_correct_count/total_wrong*100:.2f}%\")\n",
    "print(f\"     - æ— æ•ˆå›ç­”: {topk_unknown_count}\")\n",
    "\n",
    "# æŒ‰ ask-for åˆ†ç±»ç»Ÿè®¡\n",
    "cause_results = [r for r in verification_results if r['ask_for'] == 'cause']\n",
    "effect_results = [r for r in verification_results if r['ask_for'] == 'effect']\n",
    "\n",
    "print(f\"\\nğŸ“ˆ æŒ‰ Ask-for åˆ†ç±»ç»Ÿè®¡:\")\n",
    "print(f\"\\n   CAUSE ç±» ({len(cause_results)} ä¸ª):\")\n",
    "if cause_results:\n",
    "    cause_core_correct = sum(1 for r in cause_results if r['core_point_correct'])\n",
    "    cause_topk_correct = sum(1 for r in cause_results if r['topk_correct'])\n",
    "    print(f\"     - Core Point ä¿®æ­£æ­£ç¡®: {cause_core_correct}/{len(cause_results)} = {cause_core_correct/len(cause_results)*100:.2f}%\")\n",
    "    print(f\"     - TopK ä¿®æ­£æ­£ç¡®: {cause_topk_correct}/{len(cause_results)} = {cause_topk_correct/len(cause_results)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n   EFFECT ç±» ({len(effect_results)} ä¸ª):\")\n",
    "if effect_results:\n",
    "    effect_core_correct = sum(1 for r in effect_results if r['core_point_correct'])\n",
    "    effect_topk_correct = sum(1 for r in effect_results if r['topk_correct'])\n",
    "    print(f\"     - Core Point ä¿®æ­£æ­£ç¡®: {effect_core_correct}/{len(effect_results)} = {effect_core_correct/len(effect_results)*100:.2f}%\")\n",
    "    print(f\"     - TopK ä¿®æ­£æ­£ç¡®: {effect_topk_correct}/{len(effect_results)} = {effect_topk_correct/len(effect_results)*100:.2f}%\")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "verification_output_file = OUTPUT_FILE.replace('.jsonl', '_verification_results.jsonl')\n",
    "with open(verification_output_file, 'w', encoding='utf-8') as f:\n",
    "    for r in verification_results:\n",
    "        # ç§»é™¤å‘é‡æ•°æ®ï¼ˆå¤ªå¤§äº†ï¼‰\n",
    "        r_save = {k: v for k, v in r.items() if not k.endswith('_vec')}\n",
    "        f.write(json.dumps(r_save, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"\\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {verification_output_file}\")\n",
    "\n",
    "# ä¿å­˜ç»Ÿè®¡ç»“æœ\n",
    "verification_stats = {\n",
    "    'total_wrong': total_wrong,\n",
    "    'core_point': {\n",
    "        'correct': core_correct_count,\n",
    "        'accuracy': core_correct_count / total_wrong * 100 if total_wrong > 0 else 0,\n",
    "        'unknown': core_unknown_count\n",
    "    },\n",
    "    'topk': {\n",
    "        'correct': topk_correct_count,\n",
    "        'accuracy': topk_correct_count / total_wrong * 100 if total_wrong > 0 else 0,\n",
    "        'unknown': topk_unknown_count\n",
    "    },\n",
    "    'by_ask_for': {\n",
    "        'cause': {\n",
    "            'total': len(cause_results),\n",
    "            'core_correct': sum(1 for r in cause_results if r['core_point_correct']) if cause_results else 0,\n",
    "            'topk_correct': sum(1 for r in cause_results if r['topk_correct']) if cause_results else 0,\n",
    "        },\n",
    "        'effect': {\n",
    "            'total': len(effect_results),\n",
    "            'core_correct': sum(1 for r in effect_results if r['core_point_correct']) if effect_results else 0,\n",
    "            'topk_correct': sum(1 for r in effect_results if r['topk_correct']) if effect_results else 0,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "verification_stats_file = OUTPUT_FILE.replace('.jsonl', '_verification_stats.json')\n",
    "\n",
    "with open(verification_stats_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(verification_stats, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"ğŸ“ ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: {verification_stats_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10962b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“‹ ç¤ºä¾‹ç»“æœå±•ç¤º\n",
      "============================================================\n",
      "\n",
      "âœ… Core Point ä¿®æ­£æˆåŠŸçš„ç¤ºä¾‹ (å…± 46 ä¸ª):\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1] Index: train-1056\n",
      "    Premise: Jack and Tom are relatives.\n",
      "    Ask-for: effect\n",
      "    A (H1): They live under the same roof.\n",
      "    B (H2): They grew up.\n",
      "    æ­£ç¡®ç­”æ¡ˆ: A\n",
      "    åŸå§‹é”™è¯¯é€‰æ‹©: B\n",
      "    Core Point å›ç­”: They might share a common ancestor.\n",
      "    Core Point é€‰æ‹©: A âœ…\n",
      "\n",
      "[2] Index: train-3554\n",
      "    Premise: The azaleas have grown in different ways.\n",
      "    Ask-for: cause\n",
      "    A (H1): Scientists have done a research about the azaleas with root rot.\n",
      "    B (H2): Farmers weeded the land. .\n",
      "    æ­£ç¡®ç­”æ¡ˆ: A\n",
      "    åŸå§‹é”™è¯¯é€‰æ‹©: B\n",
      "    Core Point å›ç­”: The azaleas received varying amounts of sunlight or water.\n",
      "    Core Point é€‰æ‹©: A âœ…\n",
      "\n",
      "[3] Index: train-3594\n",
      "    Premise: His conclusions are not based on new scientific evidence.\n",
      "    Ask-for: effect\n",
      "    A (H1): Others don't believe his conlusions.\n",
      "    B (H2): His predictions were not accurate.\n",
      "    æ­£ç¡®ç­”æ¡ˆ: A\n",
      "    åŸå§‹é”™è¯¯é€‰æ‹©: B\n",
      "    Core Point å›ç­”: His conclusions will likely be met with skepticism or rejection by the scientific community.\n",
      "    Core Point é€‰æ‹©: A âœ…\n",
      "\n",
      "\n",
      "âœ… TopK ä¿®æ­£æˆåŠŸçš„ç¤ºä¾‹ (å…± 47 ä¸ª):\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1] Index: train-1056\n",
      "    Premise: Jack and Tom are relatives.\n",
      "    A (H1): They live under the same roof.\n",
      "    B (H2): They grew up.\n",
      "    æ­£ç¡®ç­”æ¡ˆ: A\n",
      "    TopK ä»£è¡¨å›ç­”: They might share a common ancestor.\n",
      "    TopK é€‰æ‹©: A âœ…\n",
      "\n",
      "[2] Index: train-3554\n",
      "    Premise: The azaleas have grown in different ways.\n",
      "    A (H1): Scientists have done a research about the azaleas with root rot.\n",
      "    B (H2): Farmers weeded the land. .\n",
      "    æ­£ç¡®ç­”æ¡ˆ: A\n",
      "    TopK ä»£è¡¨å›ç­”: The azaleas received varying amounts of sunlight or water.\n",
      "    TopK é€‰æ‹©: A âœ…\n",
      "\n",
      "[3] Index: train-3594\n",
      "    Premise: His conclusions are not based on new scientific evidence.\n",
      "    A (H1): Others don't believe his conlusions.\n",
      "    B (H2): His predictions were not accurate.\n",
      "    æ­£ç¡®ç­”æ¡ˆ: A\n",
      "    TopK ä»£è¡¨å›ç­”: His conclusions will likely be met with skepticism and rejection by the scientific community.\n",
      "    TopK é€‰æ‹©: A âœ…\n",
      "\n",
      "\n",
      "âŒ ä»ç„¶é”™è¯¯çš„ç¤ºä¾‹ (å…± 79 ä¸ª):\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1] Index: train-11048\n",
      "    Premise: He goes to see the macows in the zoo.\n",
      "    Ask-for: cause\n",
      "    A (H1): Tom likes to photograph animals.\n",
      "    B (H2): Timothy wants to see the largest parrots.\n",
      "    æ­£ç¡®ç­”æ¡ˆ: B\n",
      "    Core Point å›ç­”: He is interested in seeing cows.\n",
      "    Core Point é€‰æ‹©: A âŒ\n",
      "    TopK å›ç­”: He is interested in seeing cows.\n",
      "    TopK é€‰æ‹©: A âŒ\n",
      "\n",
      "[2] Index: train-4457\n",
      "    Premise: He has seriously obeyed the ethics.\n",
      "    Ask-for: cause\n",
      "    A (H1): He does things blindly on principle.\n",
      "    B (H2): The boy has done a lot of cruel things to his mother.\n",
      "    æ­£ç¡®ç­”æ¡ˆ: B\n",
      "    Core Point å›ç­”: He has a strong moral compass.\n",
      "    Core Point é€‰æ‹©: A âŒ\n",
      "    TopK å›ç­”: He has a strong moral compass.\n",
      "    TopK é€‰æ‹©: A âŒ\n",
      "\n",
      "[3] Index: train-8405\n",
      "    Premise: He is searching for food.\n",
      "    Ask-for: cause\n",
      "    A (H1): Tom, a prisoner in jail, is hungry.\n",
      "    B (H2): He is a diner.\n",
      "    æ­£ç¡®ç­”æ¡ˆ: B\n",
      "    Core Point å›ç­”: He is hungry.\n",
      "    Core Point é€‰æ‹©: A âŒ\n",
      "    TopK å›ç­”: He is hungry.\n",
      "    TopK é€‰æ‹©: A âŒ\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: å±•ç¤ºç¤ºä¾‹ç»“æœ\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“‹ ç¤ºä¾‹ç»“æœå±•ç¤º\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# å±•ç¤ºè¢« Core Point ä¿®æ­£æˆåŠŸçš„ä¾‹å­\n",
    "core_corrected = [r for r in verification_results if r['core_point_correct']]\n",
    "print(f\"\\nâœ… Core Point ä¿®æ­£æˆåŠŸçš„ç¤ºä¾‹ (å…± {len(core_corrected)} ä¸ª):\")\n",
    "print(\"-\" * 60)\n",
    "for i, r in enumerate(core_corrected[:3], 1):\n",
    "    print(f\"\\n[{i}] Index: {r['index']}\")\n",
    "    print(f\"    Premise: {r['premise']}\")\n",
    "    print(f\"    Ask-for: {r['ask_for']}\")\n",
    "    print(f\"    A (H1): {r['h1']}\")\n",
    "    print(f\"    B (H2): {r['h2']}\")\n",
    "    print(f\"    æ­£ç¡®ç­”æ¡ˆ: {r['correct_answer']}\")\n",
    "    print(f\"    åŸå§‹é”™è¯¯é€‰æ‹©: {r['original_wrong_choice']}\")\n",
    "    print(f\"    Core Point å›ç­”: {r['core_point_text']}\")\n",
    "    print(f\"    Core Point é€‰æ‹©: {r['core_point_choice']} âœ…\")\n",
    "\n",
    "# å±•ç¤ºè¢« TopK ä¿®æ­£æˆåŠŸçš„ä¾‹å­\n",
    "topk_corrected = [r for r in verification_results if r['topk_correct']]\n",
    "print(f\"\\n\\nâœ… TopK ä¿®æ­£æˆåŠŸçš„ç¤ºä¾‹ (å…± {len(topk_corrected)} ä¸ª):\")\n",
    "print(\"-\" * 60)\n",
    "for i, r in enumerate(topk_corrected[:3], 1):\n",
    "    print(f\"\\n[{i}] Index: {r['index']}\")\n",
    "    print(f\"    Premise: {r['premise']}\")\n",
    "    print(f\"    A (H1): {r['h1']}\")\n",
    "    print(f\"    B (H2): {r['h2']}\")\n",
    "    print(f\"    æ­£ç¡®ç­”æ¡ˆ: {r['correct_answer']}\")\n",
    "    print(f\"    TopK ä»£è¡¨å›ç­”: {r['topk_representative_text']}\")\n",
    "    print(f\"    TopK é€‰æ‹©: {r['topk_choice']} âœ…\")\n",
    "\n",
    "# å±•ç¤ºä»ç„¶é”™è¯¯çš„ä¾‹å­\n",
    "still_wrong = [r for r in verification_results if not r['core_point_correct'] and not r['topk_correct']]\n",
    "print(f\"\\n\\nâŒ ä»ç„¶é”™è¯¯çš„ç¤ºä¾‹ (å…± {len(still_wrong)} ä¸ª):\")\n",
    "print(\"-\" * 60)\n",
    "for i, r in enumerate(still_wrong[:3], 1):\n",
    "    print(f\"\\n[{i}] Index: {r['index']}\")\n",
    "    print(f\"    Premise: {r['premise']}\")\n",
    "    print(f\"    Ask-for: {r['ask_for']}\")\n",
    "    print(f\"    A (H1): {r['h1']}\")\n",
    "    print(f\"    B (H2): {r['h2']}\")\n",
    "    print(f\"    æ­£ç¡®ç­”æ¡ˆ: {r['correct_answer']}\")\n",
    "    print(f\"    Core Point å›ç­”: {r['core_point_text']}\")\n",
    "    print(f\"    Core Point é€‰æ‹©: {r['core_point_choice']} âŒ\")\n",
    "    print(f\"    TopK å›ç­”: {r['topk_representative_text']}\")\n",
    "    print(f\"    TopK é€‰æ‹©: {r['topk_choice']} âŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32a74be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
