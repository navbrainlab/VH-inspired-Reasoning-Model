{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c577b5",
   "metadata": {},
   "source": [
    "# Causal Reasoning with Vector-HASH and LLM\n",
    "\n",
    "æœ¬ notebook å®ç°åŸºäº Vector-HASH æ¨¡å‹çš„å› æœæ¨ç†è¯„ä¼°ï¼š\n",
    "1. åŠ è½½ NV-Embed-v2 æ¨¡å‹\n",
    "2. æ„å»º INDEX â†’ æœ€å¤§ Cluster æ˜ å°„\n",
    "3. Vector-HASH æ¨ç†è¯„ä¼°ï¼ˆä¸ Gemma ç­”æ¡ˆå¯¹æ¯”ï¼‰\n",
    "4. Vector-HASH é‡å»ºç‡åˆ†æ\n",
    "\n",
    "## å·¥ä½œæµç¨‹è¯´æ˜\n",
    "å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼ŒæŒ‰ä»¥ä¸‹æ­¥éª¤è¿›è¡Œ Vector-HASH æ¨ç†ï¼š\n",
    "\n",
    "$$s_i \\rightarrow p_i \\rightarrow g_i^{noisy} \\rightarrow g_i^{clean} \\Rightarrow g_{i+1}^{clean} \\rightarrow p_{i+1} \\rightarrow s_{i+1}$$\n",
    "\n",
    "- æ­¥éª¤1: $s_i$ é€šè¿‡ $W_{sp}$ æ˜ å°„åˆ° $p_i$\n",
    "- æ­¥éª¤2: $p_i$ é€šè¿‡ $W_{gp}$ æ˜ å°„åˆ° $g_i^{noisy}$\n",
    "- æ­¥éª¤3: å¯¹ $g_i^{noisy}$ è¿›è¡Œæ¨¡å—åŒ–æœ€è¿‘é‚»æ¸…æ´—å¾—åˆ° $g_i^{clean}$\n",
    "- æ­¥éª¤4: æ ¹æ® askfor ç¡®å®šæ–¹å‘ï¼Œé€šè¿‡ $W_{gg}$ è¿›è¡Œè·¯å¾„ç§¯åˆ†å¾—åˆ° $g_{i+1}^{clean}$\n",
    "- æ­¥éª¤5: $g_{i+1}^{clean}$ é€šè¿‡ $W_{pg}$ æ˜ å°„åˆ° $p_{i+1}$\n",
    "- æ­¥éª¤6: $p_{i+1}$ é€šè¿‡ $W_{ps}$ æ˜ å°„å› sensory ç©ºé—´å¾—åˆ° $\\hat{s}_{i+1}$\n",
    "\n",
    "**æ³¨æ„**ï¼šæ¨ç†æµç¨‹ä¸­ä¸è¿›è¡Œå½’ä¸€åŒ–ï¼Œ$g_i^{clean}$ ä¸ç†è®ºä¸Šåº”è¯¥å¯¹åº”çš„å‘é‡ç›´æ¥ä½œå¯¹æ¯”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4662597d",
   "metadata": {},
   "source": [
    "## Section 1: ç¯å¢ƒé…ç½®å’Œ NVembed åŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c68df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 1: å¯¼å…¥ä¾èµ–åº“å’ŒåŠ è½½ NV-Embed-v2 æ¨¡å‹ =====\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æ·»åŠ  src ç›®å½•åˆ° path\n",
    "sys.path.insert(0, '/home/amax/Zixing_Jia/Vector-HASH-tinkering-main/src')\n",
    "\n",
    "# ===== é…ç½®è·¯å¾„ =====\n",
    "BASE_DIR = \"/home/amax/Zixing_Jia/Vector-HASH-tinkering-main\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"Gemma_anwer_causal/Causal_Data\")\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"Gemma_anwer_causal/Causal_Results\")\n",
    "JSONL_PATH = os.path.join(BASE_DIR, \"Ecare/train.jsonl\")\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Base dir: {BASE_DIR}\")\n",
    "print(f\"Data dir: {DATA_DIR}\")\n",
    "print(f\"Results dir: {RESULTS_DIR}\")\n",
    "\n",
    "# ===== åŠ è½½ NV-Embed æ¨¡å‹ =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Loading NV-Embed-v2 Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_path = \"/home/amax/Gemma_and_NVembed/NV/snapshots/main\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, local_files_only=True)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True, local_files_only=True, torch_dtype=torch.float16)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… NV-Embed model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e18b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 2: å®šä¹‰ç¼–ç å‡½æ•°ï¼ˆæ”¯æŒ batch_size é…ç½®ï¼‰=====\n",
    "\"\"\"æ”¯æŒå¯é…ç½® batch_size çš„æ–‡æœ¬ç¼–ç å‡½æ•°\"\"\"\n",
    "\n",
    "def encode_texts(texts, max_length=4096, batch_size=32, instruction=\"\"):\n",
    "    \"\"\"\n",
    "    æ‰¹é‡ç¼–ç æ–‡æœ¬ä¸ºå‘é‡\n",
    "    \n",
    "    Args:\n",
    "        texts: å•ä¸ªå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨\n",
    "        max_length: æœ€å¤§åºåˆ—é•¿åº¦\n",
    "        batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆå¯é…ç½®ï¼‰\n",
    "        instruction: å¯é€‰çš„æŒ‡ä»¤å‰ç¼€\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (n_texts, 4096)\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    eos_token = tokenizer.eos_token if tokenizer.eos_token else \"</s>\"\n",
    "    \n",
    "    # å¦‚æœæœ‰æŒ‡ä»¤ï¼Œæ·»åŠ æŒ‡ä»¤å‰ç¼€\n",
    "    if instruction:\n",
    "        texts = [f\"Instruct: {instruction}\\nQuery: {text}\" for text in texts]\n",
    "    \n",
    "    # æ·»åŠ  EOS token\n",
    "    texts_with_eos = [text + eos_token for text in texts]\n",
    "    \n",
    "    all_embeddings = []\n",
    "    \n",
    "    # æŒ‰ batch_size å¤„ç†\n",
    "    for i in range(0, len(texts_with_eos), batch_size):\n",
    "        batch_texts = texts_with_eos[i:i+batch_size]\n",
    "        \n",
    "        batch_dict = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        batch_dict = {k: v.to(device) for k, v in batch_dict.items()}\n",
    "        \n",
    "        # åˆ›å»º pool_mask\n",
    "        attention_mask = batch_dict[\"attention_mask\"]\n",
    "        seq_lengths = attention_mask.sum(dim=1)\n",
    "        pool_mask = torch.zeros_like(attention_mask)\n",
    "        for j, length in enumerate(seq_lengths):\n",
    "            pool_mask[j, length - 1] = 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=batch_dict[\"input_ids\"],\n",
    "                attention_mask=attention_mask,\n",
    "                pool_mask=pool_mask\n",
    "            )\n",
    "            \n",
    "            if isinstance(outputs, dict):\n",
    "                if \"sentence_embeddings\" in outputs:\n",
    "                    embeddings = outputs[\"sentence_embeddings\"]\n",
    "                elif \"last_hidden_state\" in outputs:\n",
    "                    embeddings = outputs[\"last_hidden_state\"][:, -1, :]\n",
    "                else:\n",
    "                    raise ValueError(f\"Cannot extract embeddings: {outputs.keys()}\")\n",
    "            elif hasattr(outputs, \"sentence_embeddings\"):\n",
    "                embeddings = outputs.sentence_embeddings\n",
    "            elif hasattr(outputs, \"last_hidden_state\"):\n",
    "                embeddings = outputs.last_hidden_state[:, -1, :]\n",
    "            else:\n",
    "                embeddings = outputs\n",
    "        \n",
    "        # å½’ä¸€åŒ–\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        embeddings_cpu = embeddings.float().cpu()\n",
    "        all_embeddings.append(np.array(embeddings_cpu.tolist()))\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\"\"\"\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0.0\n",
    "    return np.dot(a, b) / (norm_a * norm_b)\n",
    "\n",
    "print(\"âœ… Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7ffa9d",
   "metadata": {},
   "source": [
    "## Section 2: åŠ è½½å› æœæ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0616f4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3: åŠ è½½å› æœæ•°æ® =====\n",
    "print(\"=\"*60)\n",
    "print(\"Loading Causal Data...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# åŠ è½½å‹ç¼©åçš„å› æœå‘é‡æ•°æ®\n",
    "causal_A = np.load(os.path.join(DATA_DIR, 'causal_A_vectors_compressed.npy'))\n",
    "causal_B = np.load(os.path.join(DATA_DIR, 'causal_B_vectors_compressed.npy'))\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'causal_metadata_compressed.json'), 'r', encoding='utf-8') as f:\n",
    "    causal_metadata = json.load(f)\n",
    "\n",
    "print(f\"âœ… causal_A shape: {causal_A.shape}\")\n",
    "print(f\"âœ… causal_B shape: {causal_B.shape}\")\n",
    "print(f\"âœ… causal_metadata count: {len(causal_metadata)}\")\n",
    "\n",
    "# ç»Ÿè®¡ askfor åˆ†å¸ƒ\n",
    "askfor_counts = defaultdict(int)\n",
    "for meta in causal_metadata:\n",
    "    askfor_counts[meta['ASKFOR']] += 1\n",
    "print(f\"ğŸ“Š ASKFOR distribution: {dict(askfor_counts)}\")\n",
    "\n",
    "# åŠ è½½ E-care/Ecare æ•°æ®è·å– H1/H2\n",
    "print(\"\\nLoading Ecare data from train.jsonl...\")\n",
    "e_care_data = {}\n",
    "with open(JSONL_PATH, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line.strip())\n",
    "        e_care_data[item['index']] = {\n",
    "            'premise': item['premise'],\n",
    "            'hypothesis1': item['hypothesis1'],\n",
    "            'hypothesis2': item['hypothesis2'],\n",
    "            'label': item['label'],\n",
    "            'ask-for': item['ask-for']\n",
    "        }\n",
    "\n",
    "print(f\"âœ… Loaded {len(e_care_data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc5d849",
   "metadata": {},
   "source": [
    "## Section 3: æ„å»º INDEX â†’ æœ€å¤§ Cluster æ˜ å°„\n",
    "\n",
    "è¿™æ˜¯é‡å»ºç‡åˆ†æçš„å…³é”®æ­¥éª¤ã€‚ä¸ºäº†è·å–æ¯ä¸ª INDEX å¯¹åº”çš„æœ€å¤§ clusterï¼ˆæ ·æœ¬æ•°æœ€å¤šï¼‰çš„ centroid å‘é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d5f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 4: æ„å»º INDEX â†’ æœ€å¤§ Cluster æ˜ å°„ =====\n",
    "\"\"\"\n",
    "ç›®çš„ï¼šæ„å»º INDEX -> æœ€å¤§ cluster å¯¹åº”çš„ sample_idx çš„æ˜ å°„\n",
    "\n",
    "æ•°æ®ç»“æ„å…³ç³»ï¼š\n",
    "- causal_metadata[sample_idx] åŒ…å« {'ASKFOR', 'INDEX', 'cluster_id', 'premise'}\n",
    "- åŒä¸€ä¸ª INDEX å¯èƒ½å¯¹åº”å¤šä¸ª sample_idxï¼ˆä¸åŒ clusterï¼‰\n",
    "- æˆ‘ä»¬éœ€è¦æ‰¾åˆ°æ¯ä¸ª INDEX ä¸­æ ·æœ¬æ•°æœ€å¤šçš„ cluster\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Building INDEX -> Largest Cluster Mapping\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: æŒ‰ INDEX åˆ†ç»„ sample_idx\n",
    "index_to_samples = defaultdict(list)\n",
    "for i, meta in enumerate(causal_metadata):\n",
    "    index_to_samples[meta['INDEX']].append({\n",
    "        'sample_idx': i,\n",
    "        'cluster_id': meta.get('cluster_id', None),\n",
    "        'askfor': meta['ASKFOR']\n",
    "    })\n",
    "\n",
    "print(f\"âœ… Found {len(index_to_samples)} unique INDEX\")\n",
    "\n",
    "# Step 2: åŠ è½½ clustering_summary è·å–æ¯ä¸ª cluster çš„æ ·æœ¬æ•°\n",
    "CLUSTERING_SUMMARY_PATH = os.path.join(DATA_DIR, \"clustering_summary_compressed.json\")\n",
    "with open(CLUSTERING_SUMMARY_PATH, 'r', encoding='utf-8') as f:\n",
    "    clustering_summary = json.load(f)\n",
    "\n",
    "print(f\"âœ… Loaded clustering_summary with {len(clustering_summary)} premises\")\n",
    "\n",
    "# Step 3: ä¸ºæ¯ä¸ª INDEX æ‰¾åˆ°æœ€å¤§ cluster\n",
    "index_to_largest_cluster = {}\n",
    "for premise, summary in clustering_summary.items():\n",
    "    idx = summary['index']\n",
    "    clusters = summary['clusters']\n",
    "    if clusters:\n",
    "        # æ‰¾åˆ° count æœ€å¤§çš„ cluster\n",
    "        largest_cid = max(clusters.keys(), key=lambda cid: clusters[cid]['count'])\n",
    "        largest_count = clusters[largest_cid]['count']\n",
    "        index_to_largest_cluster[idx] = {\n",
    "            'premise': premise[:50] + '...' if len(premise) > 50 else premise,\n",
    "            'largest_cluster_id': int(largest_cid),\n",
    "            'largest_count': largest_count,\n",
    "            'ask_for': summary['ask_for']\n",
    "        }\n",
    "\n",
    "print(f\"âœ… Found largest cluster info for {len(index_to_largest_cluster)} INDEX\")\n",
    "\n",
    "# Step 4: æ„å»º INDEX -> æœ€å¤§ cluster å¯¹åº”çš„ sample_idx çš„æ˜ å°„\n",
    "index_to_largest_sample = {}\n",
    "for idx, samples in index_to_samples.items():\n",
    "    if idx in index_to_largest_cluster:\n",
    "        largest_cid = index_to_largest_cluster[idx]['largest_cluster_id']\n",
    "        # æ‰¾åˆ°è¿™ä¸ª cluster_id å¯¹åº”çš„ç¬¬ä¸€ä¸ª sample_idx\n",
    "        for s in samples:\n",
    "            if s['cluster_id'] == largest_cid:\n",
    "                index_to_largest_sample[idx] = s['sample_idx']\n",
    "                break\n",
    "        # å¦‚æœæ²¡æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª\n",
    "        if idx not in index_to_largest_sample:\n",
    "            index_to_largest_sample[idx] = samples[0]['sample_idx']\n",
    "\n",
    "print(f\"âœ… Built index_to_largest_sample mapping with {len(index_to_largest_sample)} entries\")\n",
    "\n",
    "# æ˜¾ç¤ºå‡ ä¸ªç¤ºä¾‹\n",
    "print(\"\\nğŸ“Œ Sample mappings:\")\n",
    "for i, (idx, sample_idx) in enumerate(list(index_to_largest_sample.items())[:3]):\n",
    "    askfor = causal_metadata[sample_idx]['ASKFOR']\n",
    "    print(f\"  INDEX={idx} -> sample_idx={sample_idx}, askfor={askfor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e10ddbc",
   "metadata": {},
   "source": [
    "## Section 4: ç”Ÿæˆå€™é€‰ç­”æ¡ˆ H1/H2 çš„ Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9eb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5: ç”Ÿæˆ H1/H2 Embeddings =====\n",
    "\"\"\"ä¸ºæ¯ä¸ª E-care æ ·æœ¬ç”Ÿæˆ hypothesis1 å’Œ hypothesis2 çš„å‘é‡åµŒå…¥\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Generating H1/H2 Embeddings...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è·å–éœ€è¦å¤„ç†çš„ INDEX åˆ—è¡¨\n",
    "indices_to_process = list(index_to_largest_sample.keys())\n",
    "print(f\"Processing {len(indices_to_process)} indices...\")\n",
    "\n",
    "# æ”¶é›†æ‰€æœ‰éœ€è¦ç¼–ç çš„æ–‡æœ¬\n",
    "h1_texts = []\n",
    "h2_texts = []\n",
    "valid_indices = []\n",
    "\n",
    "for idx in indices_to_process:\n",
    "    if idx in e_care_data:\n",
    "        h1_texts.append(e_care_data[idx]['hypothesis1'])\n",
    "        h2_texts.append(e_care_data[idx]['hypothesis2'])\n",
    "        valid_indices.append(idx)\n",
    "\n",
    "print(f\"âœ… Found {len(valid_indices)} valid indices with E-care data\")\n",
    "\n",
    "# æ‰¹é‡ç¼–ç  H1ï¼ˆä½¿ç”¨ batch_size=32ï¼‰\n",
    "print(\"\\nEncoding H1 hypotheses...\")\n",
    "h1_embeddings = encode_texts(h1_texts, batch_size=32)\n",
    "print(f\"âœ… H1 embeddings shape: {h1_embeddings.shape}\")\n",
    "\n",
    "# æ‰¹é‡ç¼–ç  H2\n",
    "print(\"Encoding H2 hypotheses...\")\n",
    "h2_embeddings = encode_texts(h2_texts, batch_size=32)\n",
    "print(f\"âœ… H2 embeddings shape: {h2_embeddings.shape}\")\n",
    "\n",
    "# å­˜å‚¨åˆ° e_care_data\n",
    "for i, idx in enumerate(valid_indices):\n",
    "    e_care_data[idx]['h1_vec'] = h1_embeddings[i]\n",
    "    e_care_data[idx]['h2_vec'] = h2_embeddings[i]\n",
    "\n",
    "print(f\"\\nâœ… H1/H2 embeddings generated and stored for {len(valid_indices)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef6fb85",
   "metadata": {},
   "source": [
    "## Section 5: RRM (Vector-HASH) æ¨¡å‹å®šä¹‰\n",
    "\n",
    "å®šä¹‰ RRM æ¨¡å‹ç”¨äºå› æœæ¨ç†ï¼ŒåŒ…å«å¤šç§é‡‡æ ·æ¨¡å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e5a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 6: å®šä¹‰ RRM æ¨¡å‹ =====\n",
    "\"\"\"RRM (Relational Reasoning Model) - Vector-HASH æ ¸å¿ƒ\"\"\"\n",
    "\n",
    "class RRM_Model:\n",
    "    \"\"\"Relevance-Rate Matrix Model for Vector-HASH\"\"\"\n",
    "    def __init__(self, max_effects=7, learning_rate=0.05, decay=0.001, T=1.0):\n",
    "        \"\"\"\n",
    "        Initialize RRM model\n",
    "        \n",
    "        Args:\n",
    "            max_effects: æœ€å¤§ effect æ•°é‡ï¼ˆslotsï¼‰\n",
    "            learning_rate: å­¦ä¹ ç‡\n",
    "            decay: è¡°å‡ç³»æ•°\n",
    "            T: Temperature for Boltzmann sampling\n",
    "        \"\"\"\n",
    "        self.max_effects = max_effects\n",
    "        self.eta = learning_rate\n",
    "        self.lam = decay\n",
    "        self.T = T\n",
    "        self.weights = {}  # å­˜å‚¨æ¯ä¸ª (source_id, slot) çš„æƒé‡\n",
    "        self.counts = {}   # å­˜å‚¨è®¡æ•°\n",
    "    \n",
    "    def get_weights(self, key):\n",
    "        \"\"\"ä¸ºç»™å®šçš„ key è·å–æƒé‡å‘é‡\"\"\"\n",
    "        if key not in self.weights:\n",
    "            self.weights[key] = np.zeros(self.max_effects)\n",
    "            self.counts[key] = np.zeros(self.max_effects)\n",
    "        return self.weights[key]\n",
    "    \n",
    "    def update(self, key, slot_idx, x_c=1.0, y_o=1.0):\n",
    "        \"\"\"æ›´æ–°æƒé‡ (Hebbian learning)\"\"\"\n",
    "        w = self.get_weights(key)\n",
    "        y_vec = np.zeros(self.max_effects)\n",
    "        y_vec[slot_idx] = y_o\n",
    "        delta_w = self.eta * x_c * y_vec - self.lam * w\n",
    "        self.weights[key] += delta_w\n",
    "        self.weights[key] = np.maximum(self.weights[key], 0)\n",
    "        self.counts[key][slot_idx] += 1\n",
    "    \n",
    "    def sample(self, key, mode=\"direct\"):\n",
    "        \"\"\"\n",
    "        ä» RRM ä¸­é‡‡æ ·ä¸€ä¸ª slot\n",
    "        \n",
    "        Args:\n",
    "            key: æºäº‹ä»¶çš„ ID\n",
    "            mode: é‡‡æ ·æ¨¡å¼\n",
    "                - \"direct\": æŒ‰æƒé‡å½’ä¸€åŒ–åçš„æ¦‚ç‡é‡‡æ ·\n",
    "                - \"Boltzmann\": ä½¿ç”¨ Boltzmann åˆ†å¸ƒï¼ˆsoftmaxï¼‰é‡‡æ ·\n",
    "                - \"max_prob\": ç›´æ¥é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„ slotï¼ˆç¡®å®šæ€§é€‰æ‹©ï¼‰\n",
    "        \n",
    "        Returns:\n",
    "            (sampled_slot, probs): é‡‡æ ·åˆ°çš„ slot ç´¢å¼•å’Œæ¦‚ç‡åˆ†å¸ƒ\n",
    "        \"\"\"\n",
    "        w = self.get_weights(key)\n",
    "        \n",
    "        if mode == \"direct\":\n",
    "            total_w = np.sum(w)\n",
    "            if total_w == 0:\n",
    "                probs = np.ones(self.max_effects) / self.max_effects\n",
    "            else:\n",
    "                probs = w / total_w\n",
    "            return np.random.choice(self.max_effects, p=probs), probs\n",
    "        \n",
    "        elif mode == \"Boltzmann\":\n",
    "            logits = w / self.T\n",
    "            exp_logits = np.exp(logits - np.max(logits))\n",
    "            probs = exp_logits / np.sum(exp_logits)\n",
    "            return np.random.choice(self.max_effects, p=probs), probs\n",
    "        \n",
    "        elif mode == \"max_prob\":\n",
    "            # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒï¼ˆç”¨äºè¿”å›ï¼‰\n",
    "            total_w = np.sum(w)\n",
    "            if total_w == 0:\n",
    "                probs = np.ones(self.max_effects) / self.max_effects\n",
    "            else:\n",
    "                probs = w / total_w\n",
    "            # ç›´æ¥é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„ slotï¼ˆç¡®å®šæ€§ï¼‰\n",
    "            max_slot = np.argmax(probs)\n",
    "            return max_slot, probs\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sampling mode: {mode}\")\n",
    "\n",
    "print(\"âœ… RRM_Model class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03b0100",
   "metadata": {},
   "source": [
    "## Section 4.5: åŠ è½½ Vector-HASH æ¨¡å‹å’Œæƒé‡çŸ©é˜µ\n",
    "\n",
    "ä»ç£ç›˜åŠ è½½é¢„è®­ç»ƒçš„æƒé‡çŸ©é˜µ (Wsp, Wps, Wgp, Wpg) å’Œåˆå§‹åŒ–æ¨¡å‹å‚æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a6ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 4.5: åŠ è½½ Vector-HASH æ¨¡å‹å’Œæƒé‡çŸ©é˜µ =====\n",
    "\"\"\"\n",
    "åˆå§‹åŒ– Vector-HASH æ¨¡å‹å‚æ•°å’ŒåŠ è½½é¢„è®­ç»ƒæƒé‡\n",
    "\"\"\"\n",
    "\n",
    "from src.assoc_utils_np_2D import gen_gbook_2d, path_integration_Wgg_2d, module_wise_NN_2d\n",
    "from src import assoc_utils_np as assoc_utils\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Initialize Vector-HASH Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== æ¨¡å‹è¶…å‚æ•° =====\n",
    "lambdas = [3, 4, 5, 7]\n",
    "Np = 4096  # Place cells\n",
    "Ns = 4096  # Sensory cells (NV-Embed dimension)\n",
    "Ng = int(np.sum(np.square(lambdas)))  # Total grid cells\n",
    "Npos = int(np.prod(lambdas))  # Grid positions per side\n",
    "thresh = 2.0  # Nonlinearity threshold\n",
    "max_effects = 7  # RRM max effects\n",
    "\n",
    "print(f\"Model parameters:\")\n",
    "print(f\"  lambdas: {lambdas}\")\n",
    "print(f\"  Ng (total grid cells): {Ng}\")\n",
    "print(f\"  Npos (position dimension): {Npos}\")\n",
    "print(f\"  Np (place cells): {Np}\")\n",
    "print(f\"  Ns (sensory dim): {Ns}\")\n",
    "print(f\"  thresh: {thresh}\")\n",
    "\n",
    "# ===== ç”Ÿæˆ Grid Codebook =====\n",
    "print(f\"\\nGenerating grid codebook...\")\n",
    "gbook = gen_gbook_2d(lambdas, Ng, Npos)  # (Ng, Npos, Npos)\n",
    "gbook_flat = gbook.reshape(Ng, Npos * Npos)\n",
    "print(f\"  gbook shape: {gbook.shape}\")\n",
    "print(f\"  gbook_flat shape: {gbook_flat.shape}\")\n",
    "\n",
    "# ===== Module ä¿¡æ¯ =====\n",
    "module_sizes = np.square(lambdas)  # [9, 16, 25, 49]\n",
    "module_gbooks = [np.eye(int(size)) for size in module_sizes]\n",
    "print(f\"  module_sizes: {module_sizes}\")\n",
    "print(f\"  module_gbooks lengths: {[mg.shape for mg in module_gbooks]}\")\n",
    "\n",
    "# ===== åŠ è½½/åˆå§‹åŒ–æƒé‡çŸ©é˜µ =====\n",
    "print(f\"\\nLoading/initializing weight matrices...\")\n",
    "\n",
    "# å°è¯•ä»ç£ç›˜åŠ è½½\n",
    "Wsp_path = os.path.join(RESULTS_DIR, \"Wsp.npy\")\n",
    "Wps_path = os.path.join(RESULTS_DIR, \"Wps.npy\")\n",
    "Wpg_path = os.path.join(RESULTS_DIR, \"Wpg.npy\")\n",
    "Wgp_path = os.path.join(RESULTS_DIR, \"Wgp.npy\")\n",
    "pbook_path = os.path.join(RESULTS_DIR, \"pbook.npy\")\n",
    "\n",
    "weights_loaded = True\n",
    "\n",
    "if os.path.exists(Wsp_path) and os.path.exists(Wpg_path):\n",
    "    Wsp = np.load(Wsp_path)\n",
    "    Wps = np.load(Wps_path)\n",
    "    Wpg = np.load(Wpg_path)\n",
    "    Wgp = np.load(Wgp_path)\n",
    "    pbook_flat = np.load(pbook_path)  # shape: (nruns, Np, Npos^2)\n",
    "    print(f\"âœ… Loaded weights from {RESULTS_DIR}\")\n",
    "\n",
    "# å¤„ç† pbook: ä» (nruns, Np, Npos^2) reshape æˆ (Np, Npos, Npos)\n",
    "# è¿™æ ·å¯ä»¥ç”¨ pbook[:, x, y] ç›´æ¥ç´¢å¼•\n",
    "if pbook_flat.ndim == 3 and pbook_flat.shape[0] == 1:\n",
    "    # pbook_flat shape: (1, Np, Npos^2) -> (Np, Npos, Npos)\n",
    "    pbook = pbook_flat[0].reshape(Np, Npos, Npos)\n",
    "else:\n",
    "    pbook = pbook_flat.reshape(Np, Npos, Npos)\n",
    "print(f\"âœ… pbook reshaped: {pbook_flat.shape} -> {pbook.shape}\")\n",
    "\n",
    "Wpg = np.squeeze(Wpg)\n",
    "print(f\"\\n   æœ€ç»ˆ shape:\")\n",
    "print(f\"   Wsp: {Wsp.shape}\")\n",
    "print(f\"   Wps: {Wps.shape}\")\n",
    "print(f\"   Wpg: {Wpg.shape}\")\n",
    "print(f\"   Wgp: {Wgp.shape}\")\n",
    "print(f\"   pbook: {pbook.shape}\")\n",
    "# ===== ç”Ÿæˆ Wgg (è·¯å¾„ç§¯åˆ†çŸ©é˜µ) =====\n",
    "print(f\"\\nGenerating path integration matrices...\")\n",
    "Wgg_c2e = path_integration_Wgg_2d(lambdas, Ng, axis=0, direction=1)   # cause->effect: x+1\n",
    "Wgg_e2c = path_integration_Wgg_2d(lambdas, Ng, axis=0, direction=-1)  # effect->cause: x-1\n",
    "print(f\"  Wgg_c2e shape: {Wgg_c2e.shape}\")\n",
    "print(f\"  Wgg_e2c shape: {Wgg_e2c.shape}\")\n",
    "\n",
    "print(\"\\nâœ… Vector-HASH model initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3fe1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 4.6: å®šä¹‰ Vector-HASH è¾…åŠ©å‡½æ•°å’Œæ¨ç†ç®¡é“ =====\n",
    "\"\"\"\n",
    "å®ç° Vector-HASH æ¨ç†çš„åˆ†æ­¥å‡½æ•°å’Œå®Œæ•´æµç¨‹\n",
    "\"\"\"\n",
    "\n",
    "def normalize_vec(v):\n",
    "    \"\"\"å½’ä¸€åŒ–å‘é‡\"\"\"\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm < 1e-12:\n",
    "        return v\n",
    "    return v / norm\n",
    "\n",
    "def sensory_to_place(s, Wps):\n",
    "    \"\"\"Sensory -> Place cell (é€šè¿‡ Wsp)\"\"\"\n",
    "    # s: (Ns,), Wsp: (Ns, Np) -> p: (Np,)\n",
    "    p = Wps @ s  # æ³¨æ„ï¼šs^T Wsp\n",
    "    return p\n",
    "\n",
    "def place_to_sensory(p, Wsp):\n",
    "    \"\"\"Place cell -> Sensory (é€šè¿‡ Wps)\"\"\"\n",
    "    # p: (Np,), Wps: (Ns, Np) -> s: (Ns,)\n",
    "    s = Wsp @ p # p^T Wps^T = p^T @ Wps^T\n",
    "    return s\n",
    "\n",
    "def place_to_grid_noisy(p, Wgp):\n",
    "    \"\"\"Place cell -> Grid cell noisy (é€šè¿‡ Wgp, æ— éçº¿æ€§)\"\"\"\n",
    "    # p: (Np,), Wgp: (Ng, Np) -> g: (Ng,)\n",
    "    g = np.dot(Wgp, p)  # Wgp @ p\n",
    "    return g\n",
    "\n",
    "def grid_to_place_with_nonlin(g, Wpg, thresh=2.0):\n",
    "    \"\"\"\n",
    "    Grid cell -> Place cell (é€šè¿‡ Wpg + nonlinearity)\n",
    "    \n",
    "    ä½¿ç”¨ pbook (place cell book) è¿›è¡Œéçº¿æ€§å˜æ¢\n",
    "    pbook shape: (Np, Npos, Npos) æˆ– (Np, Npos*Npos)\n",
    "    \"\"\"\n",
    "    # é¦–å…ˆé€šè¿‡ Wpg è¿›è¡Œçº¿æ€§å˜æ¢\n",
    "    # g: (Ng,), Wpg: (Np, Ng) -> p_linear: (Np,)\n",
    "    p_linear = np.dot(Wpg, g)\n",
    "    \n",
    "   \n",
    "    # éçº¿æ€§å‡½æ•°ï¼ˆé˜ˆå€¼å‡½æ•°ï¼‰\n",
    "    # p_output = ReLU(p_linear) å¦‚æœè¶…è¿‡é˜ˆå€¼\n",
    "    p_output = np.maximum(0, p_linear - thresh)\n",
    "    \n",
    "    return p_output\n",
    "\n",
    "def get_vertical_shift(slot):\n",
    "    \"\"\"æ ¹æ® slot ç´¢å¼•è®¡ç®— y æ–¹å‘åç§»\"\"\"\n",
    "    # ç®€å•æ˜ å°„ï¼šslot -> delta_y\n",
    "    # slot åœ¨ [0, max_effects-1] ä¹‹é—´\n",
    "    # æ˜ å°„åˆ° y åç§»ï¼šæœ€å¤šä¸Šä¸‹ç§»åŠ¨ k = 3\n",
    "    k = 3\n",
    "    delta_y = slot - k  # [-3, -2, -1, 0, 1, 2, 3]\n",
    "    return delta_y\n",
    "\n",
    "def vector_hash_infer_with_rrm(s_input, askfor, Wps, Wsp, Wgp, Wpg, \n",
    "                                Wgg_c2e, Wgg_e2c, \n",
    "                                module_gbooks, module_sizes,\n",
    "                                rrm_model, source_id, \n",
    "                                sampling_mode=\"direct\",\n",
    "                                statistic=False):\n",
    "    \"\"\"\n",
    "    å®Œæ•´çš„ Vector-HASH æ¨ç†æµç¨‹ï¼ˆå«RRMé‡‡æ ·ï¼‰:\n",
    "    \n",
    "    æ¨ç†æµç¨‹ï¼ˆå‚è€ƒCausal_V2ï¼‰:\n",
    "    1. s_i -> p_i (é€šè¿‡ Wsp)\n",
    "    2. p_i -> g_i^noisy (é€šè¿‡ Wgp)\n",
    "    3. g_i^noisy -> g_i^clean (é€šè¿‡ module_wise_NN_2då¸å¼•å­åŠ¨åŠ›å­¦)\n",
    "    4. ä»RRMé‡‡æ ·ä¸€ä¸ªslotï¼Œå†³å®šyæ–¹å‘åç§»\n",
    "    5. Path Integration: \n",
    "       - xæ–¹å‘: ç”±askforå†³å®š (cause->effect: x+1, effect->cause: x-1)\n",
    "       - yæ–¹å‘: ç”±é‡‡æ ·çš„slotå†³å®š\n",
    "    6. g_{i+1}^clean -> p_{i+1} (é€šè¿‡ Wpg + nonlin) \n",
    "       æ³¨æ„ï¼špath_integrationågå·²ç»æ˜¯cleançš„ï¼Œæ— éœ€å†æ¸…æ´—\n",
    "    7. p_{i+1} -> s_{i+1} (é€šè¿‡ Wps)\n",
    "    \n",
    "    Args:\n",
    "        s_input: è¾“å…¥çš„ sensory å‘é‡ (4096ç»´)\n",
    "        askfor: \"cause\" æˆ– \"effect\"\n",
    "        Wsp: Place to Sensory çŸ©é˜µ\n",
    "        Wps: Sensory to Place çŸ©é˜µ\n",
    "        rrm_model: RRMæ¨¡å‹å®ä¾‹\n",
    "        source_id: æºäº‹ä»¶IDï¼ˆç”¨äºRRMé‡‡æ ·ï¼‰\n",
    "        sampling_mode: \"direct\", \"Boltzmann\" æˆ– \"max_prob\"\n",
    "        statistic: æ˜¯å¦è¿”å›ä¸­é—´å‘é‡ï¼ˆç”¨äºå¤–éƒ¨è¯¯å·®è®¡ç®—ï¼‰\n",
    "    \n",
    "    Returns:\n",
    "        å¦‚æœ statistic=False:\n",
    "            (s_output, sampled_slot): é¢„æµ‹å‘é‡å’Œé‡‡æ ·slot\n",
    "        å¦‚æœ statistic=True:\n",
    "            (s_output, sampled_slot, stats_dict): é¢„æµ‹å‘é‡ã€é‡‡æ ·slotå’Œç»Ÿè®¡å­—å…¸\n",
    "            stats_dict åªåŒ…å«ä¸­é—´æ­¥éª¤çš„å‘é‡ï¼ˆè¯¯å·®åœ¨å¤–éƒ¨è®¡ç®—ï¼‰\n",
    "    \"\"\"\n",
    "    # å½’ä¸€åŒ–è¾“å…¥\n",
    "    s = normalize_vec(s_input)\n",
    "    \n",
    "    # Step 1: Sensory -> Place cell (ä½¿ç”¨ Wspï¼Œä¸æ˜¯ Wpsï¼)\n",
    "    p_i = sensory_to_place(s, Wps)\n",
    "    \n",
    "    # Step 2: Place cell -> Grid cell (noisy)\n",
    "    g_i_noisy = place_to_grid_noisy(p_i, Wgp)\n",
    "    \n",
    "    # Step 3: Grid cell æ¸…æ´— (ä½¿ç”¨ module_wise_NN_2d)\n",
    "    gin = g_i_noisy.reshape(1, -1, 1)  # (1, Ng, 1)\n",
    "    g_i_clean = module_wise_NN_2d(gin, module_gbooks, module_sizes)\n",
    "    g_i_clean = g_i_clean[0, :, 0]  # (Ng,)\n",
    "    Ng = g_i_clean.shape[0]\n",
    "    \n",
    "    # Step 4: ä»RRMé‡‡æ ·ä¸€ä¸ªslot\n",
    "    sampled_slot, probs = rrm_model.sample(source_id, mode=sampling_mode)\n",
    "    \n",
    "    # Step 5: Path Integration\n",
    "    # xæ–¹å‘ç”±askforå†³å®š\n",
    "    if askfor == \"effect\":\n",
    "        Wgg_x = Wgg_c2e  # cause -> effect (x+1)\n",
    "    else:\n",
    "        Wgg_x = Wgg_e2c  # effect -> cause (x-1)\n",
    "    \n",
    "    # å…ˆåšxæ–¹å‘çš„path integration\n",
    "    g_next = Wgg_x @ g_i_clean\n",
    "    \n",
    "    # yæ–¹å‘ç”±slotå†³å®š\n",
    "    delta_y = get_vertical_shift(sampled_slot)\n",
    "    if delta_y != 0:\n",
    "        # yæ–¹å‘çš„path integration\n",
    "        direction_y = 1 if delta_y > 0 else -1\n",
    "        Wgg_y = path_integration_Wgg_2d(lambdas, Ng, axis=1, direction=direction_y)\n",
    "        for _ in range(abs(delta_y)):\n",
    "            g_next = Wgg_y @ g_next\n",
    "    \n",
    "    # æ³¨æ„ï¼špath_integrationåg_nextå·²ç»æ˜¯cleançš„ï¼ˆWggæ˜¯ç²¾ç¡®ç½®æ¢çŸ©é˜µï¼‰ï¼Œæ— éœ€å†æ¸…æ´—\n",
    "    \n",
    "    # Step 6: Grid cell -> Place cell (éœ€è¦nonlin!)\n",
    "    p_next = grid_to_place_with_nonlin(g_next, Wpg, thresh=thresh)\n",
    "    \n",
    "    # Step 7: Place cell -> Sensory \n",
    "    s_output = place_to_sensory(p_next, Wsp)\n",
    "    \n",
    "    # å¦‚æœéœ€è¦ç»Ÿè®¡ä¿¡æ¯ï¼ˆåªè¿”å›ä¸­é—´å‘é‡ï¼Œè¯¯å·®åœ¨å¤–éƒ¨è®¡ç®—ï¼‰\n",
    "    if statistic:\n",
    "        stats_dict = {\n",
    "            'p_i': p_i,\n",
    "            'g_i_noisy': g_i_noisy,\n",
    "            'g_i_clean': g_i_clean,\n",
    "            'g_next': g_next,\n",
    "            'p_next': p_next,\n",
    "            's_output': s_output,\n",
    "            'delta_y': delta_y,\n",
    "        }\n",
    "        return s_output, sampled_slot, stats_dict\n",
    "    \n",
    "    return s_output, sampled_slot\n",
    "\n",
    "print(\"âœ… Vector-HASH inference functions defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daed153",
   "metadata": {},
   "source": [
    "## Section 6: æ¨ç†è¯„ä¼° - Gemma centroid vs V-H Reconstructed\n",
    "\n",
    "**æ ¸å¿ƒå¯¹æ¯”**ï¼š\n",
    "- æ–¹æ³•1ï¼šGemma ç­”æ¡ˆ centroidï¼ˆæœ€å¤§ cluster çš„èšç±»ä¸­å¿ƒå‘é‡ï¼‰\n",
    "- æ–¹æ³•2ï¼šV-H Reconstructed vectorï¼ˆç»è¿‡ Vector-HASH é‡å»ºåçš„å‘é‡ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185427b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 7: æ¨ç†è¯„ä¼° =====\n",
    "\"\"\"\n",
    "å¯¹æ¯”ä¸¤ç§æ–¹æ³•åœ¨å› æœæ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½\n",
    "\n",
    "æ•°æ®ç»“æ„è¯´æ˜ (æ¥è‡ª causal_data_processing.py):\n",
    "  - askfor='cause': è¾“å…¥=Effect (premise), Gemmaæ¨æ–­=Cause â†’ causal_A[sample_idx]\n",
    "  - askfor='effect': è¾“å…¥=Cause (premise), Gemmaæ¨æ–­=Effect â†’ causal_B[sample_idx]\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Inference Accuracy Comparison: Gemma Centroid vs V-H\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ç¡®ä¿æœ‰ H1/H2 embeddings\n",
    "sample_with_embeddings = [idx for idx in index_to_largest_sample.keys() \n",
    "                          if idx in e_care_data and 'h1_vec' in e_care_data[idx]]\n",
    "print(f\"âœ… Found {len(sample_with_embeddings)} samples with H1/H2 embeddings\")\n",
    "\n",
    "# åˆå§‹åŒ– RRM æ¨¡å‹ç”¨äºæ¨ç†\n",
    "rrm_model_infer = RRM_Model(max_effects=max_effects)\n",
    "\n",
    "# å¼€å§‹è¯„ä¼°\n",
    "inference_results = []\n",
    "\n",
    "for idx in tqdm(sample_with_embeddings, desc=\"Inference evaluation\"):\n",
    "    # è·å–æœ€å¤§ cluster å¯¹åº”çš„ sample_idx\n",
    "    sample_idx = index_to_largest_sample[idx]\n",
    "    \n",
    "    # è·å–å…ƒæ•°æ®\n",
    "    meta = causal_metadata[sample_idx]\n",
    "    askfor = meta['ASKFOR']\n",
    "    \n",
    "    # è·å– H1/H2 å‘é‡å’ŒçœŸå®æ ‡ç­¾\n",
    "    h1_vec = e_care_data[idx]['h1_vec']\n",
    "    h2_vec = e_care_data[idx]['h2_vec']\n",
    "    true_label = e_care_data[idx]['label']\n",
    "    \n",
    "    # ===== æ–¹æ³• 1: Gemma ç­”æ¡ˆ centroid =====\n",
    "    # askfor='cause' â†’ Gemmaæ¨æ–­çš„Cause centroid åœ¨ causal_A\n",
    "    # askfor='effect' â†’ Gemmaæ¨æ–­çš„Effect centroid åœ¨ causal_B\n",
    "    if askfor == 'cause':\n",
    "        gemma_centroid = causal_A[sample_idx]\n",
    "        premise_vec = causal_B[sample_idx]\n",
    "    else:  # askfor == 'effect'\n",
    "        gemma_centroid = causal_B[sample_idx]\n",
    "        premise_vec = causal_A[sample_idx]\n",
    "    \n",
    "    # è®¡ç®— Gemma centroid ä¸ H1/H2 çš„ç›¸ä¼¼åº¦\n",
    "    sim_gemma_h1 = cosine_similarity(gemma_centroid, h1_vec)\n",
    "    sim_gemma_h2 = cosine_similarity(gemma_centroid, h2_vec)\n",
    "    pred_gemma = 0 if sim_gemma_h1 >= sim_gemma_h2 else 1\n",
    "    correct_gemma = (pred_gemma == true_label)\n",
    "    \n",
    "    # ===== æ–¹æ³• 2: V-H Reconstructed vector =====\n",
    "    try:\n",
    "        # æ„é€  RRM çš„ source_id\n",
    "        source_id = f\"{askfor}_{idx}\"\n",
    "        \n",
    "        # è°ƒç”¨å®Œæ•´çš„ Vector-HASH æ¨ç†æµç¨‹\n",
    "        s_reconstructed, sampled_slot = vector_hash_infer_with_rrm(\n",
    "            s_input=premise_vec,\n",
    "            askfor=askfor,\n",
    "            Wps=Wps,\n",
    "            Wsp=Wsp,\n",
    "            Wgp=Wgp,\n",
    "            Wpg=Wpg,\n",
    "            Wgg_c2e=Wgg_c2e,\n",
    "            Wgg_e2c=Wgg_e2c,\n",
    "            module_gbooks=module_gbooks,\n",
    "            module_sizes=module_sizes,\n",
    "            rrm_model=rrm_model_infer,\n",
    "            source_id=source_id,\n",
    "            sampling_mode=\"max_prob\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # å¦‚æœæ¨ç†å¤±è´¥ï¼Œä½¿ç”¨ premise_vec ä½œä¸ºé™çº§æ–¹æ¡ˆ\n",
    "        s_reconstructed = premise_vec\n",
    "        sampled_slot = -1\n",
    "        # print(f\"  Warning: inference failed for {idx}: {e}\")\n",
    "    \n",
    "    # è®¡ç®—é‡å»ºå‘é‡ä¸ H1/H2 çš„ç›¸ä¼¼åº¦\n",
    "    sim_recon_h1 = cosine_similarity(s_reconstructed, h1_vec)\n",
    "    sim_recon_h2 = cosine_similarity(s_reconstructed, h2_vec)\n",
    "    pred_recon = 0 if sim_recon_h1 >= sim_recon_h2 else 1\n",
    "    correct_recon = (pred_recon == true_label)\n",
    "    \n",
    "    # è®¡ç®—é‡å»ºå‘é‡ä¸ç›®æ ‡å‘é‡ (Gemma centroid) çš„ç›¸ä¼¼åº¦ï¼ˆç”¨äºé‡å»ºç‡åˆ†æï¼‰\n",
    "    sim_recon_target = cosine_similarity(s_reconstructed, gemma_centroid)\n",
    "    \n",
    "    # è®°å½•ç»“æœ\n",
    "    inference_results.append({\n",
    "        'INDEX': idx,\n",
    "        'sample_idx': sample_idx,\n",
    "        'askfor': askfor,\n",
    "        'true_label': true_label,\n",
    "        'sim_gemma_h1': float(sim_gemma_h1),\n",
    "        'sim_gemma_h2': float(sim_gemma_h2),\n",
    "        'pred_gemma': int(pred_gemma),\n",
    "        'correct_gemma': bool(correct_gemma),\n",
    "        'sim_recon_h1': float(sim_recon_h1),\n",
    "        'sim_recon_h2': float(sim_recon_h2),\n",
    "        'pred_recon': int(pred_recon),\n",
    "        'correct_recon': bool(correct_recon),\n",
    "        'sampled_slot': int(sampled_slot),\n",
    "        'sim_recon_target': float(sim_recon_target)  # V-Hé‡å»ºå‘é‡ä¸Gemma centroidçš„ç›¸ä¼¼åº¦\n",
    "    })\n",
    "\n",
    "print(f\"\\nâœ… Evaluated {len(inference_results)} samples\")\n",
    "\n",
    "# ===== ç»Ÿè®¡ç»“æœ =====\n",
    "correct_gemma_arr = np.array([r['correct_gemma'] for r in inference_results])\n",
    "correct_recon_arr = np.array([r['correct_recon'] for r in inference_results])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Inference Accuracy Comparison\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Method':<50} {'Accuracy':>15} {'Count':>15}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Gemma Centroid (Largest Cluster)':<50} {correct_gemma_arr.mean():>15.2%} {f'{correct_gemma_arr.sum()}/{len(correct_gemma_arr)}':>15}\")\n",
    "print(f\"{'V-H Reconstructed Vector':<50} {correct_recon_arr.mean():>15.2%} {f'{correct_recon_arr.sum()}/{len(correct_recon_arr)}':>15}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Random Baseline':<50} {'50.00%':>15}\")\n",
    "\n",
    "# æŒ‰ askfor åˆ†é‡åˆ†æ\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Performance by askfor type\")\n",
    "print(\"=\"*70)\n",
    "for askfor_type in ['cause', 'effect']:\n",
    "    subset = [r for r in inference_results if r['askfor'] == askfor_type]\n",
    "    if subset:\n",
    "        subset_arr = np.array([r['correct_recon'] for r in subset])\n",
    "        gemma_subset_arr = np.array([r['correct_gemma'] for r in subset])\n",
    "        print(f\"\\n{askfor_type.upper()}:\")\n",
    "        print(f\"  Gemma:  {gemma_subset_arr.mean():.2%} ({gemma_subset_arr.sum()}/{len(gemma_subset_arr)})\")\n",
    "        print(f\"  V-H:    {subset_arr.mean():.2%} ({subset_arr.sum()}/{len(subset_arr)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f298f5",
   "metadata": {},
   "source": [
    "## Section 7: é‡å»ºç‡åˆ†æ\n",
    "\n",
    "æ¯”è¾ƒ $\\hat{s}_{i+1}$ å’Œæœ€å¤§ Cluster å¯¹åº”çš„ Centroid å‘é‡çš„ç›¸ä¼¼åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3351a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 8: é‡å»ºç‡åˆ†æ =====\n",
    "\"\"\"\n",
    "åˆ†æ Vector-HASH é‡å»ºçš„å‡†ç¡®æ€§\n",
    "\n",
    "å¯¹æ¯ä¸ªæ ·æœ¬è®¡ç®—ï¼š\n",
    "1. è¾“å…¥: premise_vec\n",
    "2. é¢„æœŸè¾“å‡º: Gemma centroid (æœ€å¤§ cluster)\n",
    "3. å®é™…è¾“å‡º: V-H reconstructed vector (s_reconstructed)\n",
    "4. ç›¸ä¼¼åº¦åˆ†æ: cos_sim(s_reconstructed, gemma_centroid)\n",
    "5. ç­”æ¡ˆå˜åŒ–åˆ†æ: é‡å»ºä¸å®Œç¾å¯¼è‡´çš„ç­”æ¡ˆæ”¹å˜\n",
    "\n",
    "æ¨ç†æµç¨‹ï¼š\n",
    "s_i â†’ p_i â†’ g_i^{noisy} â†’ g_i^{clean} â†’ g_{i+1}^{clean} â†’ p_{i+1} â†’ s_{i+1}\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Vector-HASH Reconstruction Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "reconstruction_analysis = []\n",
    "\n",
    "# ç”¨äºç»Ÿè®¡ç­”æ¡ˆå˜åŒ–çš„è®¡æ•°å™¨\n",
    "answer_change_stats = {\n",
    "    'originally_correct_now_wrong': 0,  # åŸæœ¬å¯¹ï¼Œä½†å› é‡å»ºä¸å®Œç¾é€‰é”™äº†\n",
    "    'originally_wrong_now_correct': 0,  # åŸæœ¬é”™ï¼Œä½†å› é‡å»ºä¸å®Œç¾åè€Œé€‰å¯¹äº†\n",
    "    'both_correct': 0,                  # ä¸¤è€…éƒ½å¯¹\n",
    "    'both_wrong': 0,                    # ä¸¤è€…éƒ½é”™\n",
    "}\n",
    "\n",
    "answer_change_details = {\n",
    "    'originally_correct_now_wrong': [],\n",
    "    'originally_wrong_now_correct': [],\n",
    "}\n",
    "\n",
    "for r in tqdm(inference_results, desc=\"Reconstruction analysis\"):\n",
    "    idx = r['INDEX']\n",
    "    sample_idx = r['sample_idx']\n",
    "    askfor = r['askfor']\n",
    "    \n",
    "    # ç›´æ¥ä½¿ç”¨å·²è®¡ç®—çš„ s_reconstructed ä¸ target (Gemma centroid) çš„ç›¸ä¼¼åº¦\n",
    "    # sim_recon_target ä»£è¡¨ V-H é‡å»ºå‘é‡ä¸ç›®æ ‡å‘é‡çš„ç›¸ä¼¼åº¦\n",
    "    # å¦‚æœç›¸ä¼¼åº¦ = 1ï¼Œè¯´æ˜é‡å»ºå®Œç¾ï¼›ç›¸ä¼¼åº¦è¶Šä½ï¼Œé‡å»ºè´¨é‡è¶Šå·®\n",
    "    reconstruction_similarity = r['sim_recon_target']\n",
    "    \n",
    "    # å¯¹æ ‡ï¼šGemma centroid è‡ªå·±ä¸é€‰ä¸­çš„å‡è®¾ï¼ˆH1æˆ–H2ï¼‰çš„ç›¸ä¼¼åº¦\n",
    "    # è¿™ä»£è¡¨ Gemma ç­”æ¡ˆçš„\"ç½®ä¿¡åº¦\"\n",
    "    gemma_confidence = (r['sim_gemma_h1'] if r['pred_gemma'] == 0 else r['sim_gemma_h2'])\n",
    "    \n",
    "    # åˆ†æç­”æ¡ˆå˜åŒ–æƒ…å†µ\n",
    "    pred_gemma = r['pred_gemma']\n",
    "    pred_recon = r['pred_recon']\n",
    "    true_label = r['true_label']\n",
    "    correct_gemma = r['correct_gemma']\n",
    "    correct_recon = r['correct_recon']\n",
    "    \n",
    "    # åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦å› é‡å»ºä¸å®Œç¾è€Œæ”¹å˜\n",
    "    answer_changed = (pred_gemma != pred_recon)\n",
    "    \n",
    "    if answer_changed:\n",
    "        if correct_gemma and not correct_recon:\n",
    "            # åŸæœ¬å¯¹ï¼ˆGemma centroidé€‰å¯¹äº†ï¼‰ï¼Œä½†å› é‡å»ºä¸å®Œç¾è€Œé€‰é”™äº†\n",
    "            answer_change_stats['originally_correct_now_wrong'] += 1\n",
    "            answer_change_details['originally_correct_now_wrong'].append({\n",
    "                'INDEX': idx,\n",
    "                'askfor': askfor,\n",
    "                'reconstruction_similarity': reconstruction_similarity,\n",
    "                'gemma_confidence': gemma_confidence,\n",
    "                'pred_gemma': pred_gemma,\n",
    "                'pred_recon': pred_recon,\n",
    "                'true_label': true_label\n",
    "            })\n",
    "        elif not correct_gemma and correct_recon:\n",
    "            # åŸæœ¬é”™ï¼ˆGemma centroidé€‰é”™äº†ï¼‰ï¼Œä½†å› é‡å»ºä¸å®Œç¾åè€Œé€‰å¯¹äº†\n",
    "            answer_change_stats['originally_wrong_now_correct'] += 1\n",
    "            answer_change_details['originally_wrong_now_correct'].append({\n",
    "                'INDEX': idx,\n",
    "                'askfor': askfor,\n",
    "                'reconstruction_similarity': reconstruction_similarity,\n",
    "                'gemma_confidence': gemma_confidence,\n",
    "                'pred_gemma': pred_gemma,\n",
    "                'pred_recon': pred_recon,\n",
    "                'true_label': true_label\n",
    "            })\n",
    "    else:\n",
    "        # ç­”æ¡ˆæ²¡å˜\n",
    "        if correct_gemma:\n",
    "            answer_change_stats['both_correct'] += 1\n",
    "        else:\n",
    "            answer_change_stats['both_wrong'] += 1\n",
    "    \n",
    "    reconstruction_analysis.append({\n",
    "        'INDEX': idx,\n",
    "        'askfor': askfor,\n",
    "        'reconstruction_similarity': float(reconstruction_similarity),\n",
    "        'gemma_confidence': float(gemma_confidence),\n",
    "        'similarity_delta': float(reconstruction_similarity - gemma_confidence),\n",
    "        'vh_correct': r['correct_recon'],\n",
    "        'gemma_correct': r['correct_gemma'],\n",
    "        'answer_changed': answer_changed\n",
    "    })\n",
    "\n",
    "print(f\"\\nâœ… Analyzed reconstruction for {len(reconstruction_analysis)} samples\")\n",
    "\n",
    "# ç»Ÿè®¡\n",
    "recon_sim_arr = np.array([r['reconstruction_similarity'] for r in reconstruction_analysis])\n",
    "gemma_conf_arr = np.array([r['gemma_confidence'] for r in reconstruction_analysis])\n",
    "sim_delta_arr = np.array([r['similarity_delta'] for r in reconstruction_analysis])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Reconstruction Quality Analysis\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nè¯´æ˜:\")\n",
    "print(\"  - é‡å»ºç›¸ä¼¼åº¦: V-Hé‡å»ºå‘é‡(s_reconstructed) ä¸ Gemma centroid çš„ä½™å¼¦ç›¸ä¼¼åº¦\")\n",
    "print(\"  - Gemmaç½®ä¿¡åº¦: Gemma centroid ä¸å…¶é€‰ä¸­å‡è®¾(H1/H2)çš„ä½™å¼¦ç›¸ä¼¼åº¦\")\n",
    "print(\"  - å¦‚æœé‡å»ºç›¸ä¼¼åº¦ = 1.0ï¼Œè¯´æ˜V-Hå®Œç¾é‡å»ºäº†Gemma centroid\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nV-H é‡å»ºç›¸ä¼¼åº¦ (ä¸ Gemma centroid):\")\n",
    "print(f\"  Mean:     {recon_sim_arr.mean():.4f}\")\n",
    "print(f\"  Std dev:  {recon_sim_arr.std():.4f}\")\n",
    "print(f\"  Min:      {recon_sim_arr.min():.4f}\")\n",
    "print(f\"  Max:      {recon_sim_arr.max():.4f}\")\n",
    "print(f\"  Median:   {np.median(recon_sim_arr):.4f}\")\n",
    "\n",
    "print(f\"\\nGemma Centroid ç½®ä¿¡åº¦ (ä¸å…¶é€‰ä¸­çš„å‡è®¾):\")\n",
    "print(f\"  Mean:     {gemma_conf_arr.mean():.4f}\")\n",
    "print(f\"  Std dev:  {gemma_conf_arr.std():.4f}\")\n",
    "\n",
    "print(f\"\\nç›¸ä¼¼åº¦å·®å¼‚ (V-Hé‡å»ºç›¸ä¼¼åº¦ - Gemmaç½®ä¿¡åº¦):\")\n",
    "print(f\"  Mean delta:      {sim_delta_arr.mean():.4f}\")\n",
    "print(f\"  Positive delta:  {(sim_delta_arr > 0).sum()} samples ({(sim_delta_arr > 0).mean():.2%})\")\n",
    "print(f\"  Negative delta:  {(sim_delta_arr < 0).sum()} samples ({(sim_delta_arr < 0).mean():.2%})\")\n",
    "\n",
    "# æŒ‰ç™¾åˆ†ä½æ•°ç»Ÿè®¡\n",
    "print(f\"\\nPercentile Analysis (V-H é‡å»ºç›¸ä¼¼åº¦):\")\n",
    "for p in [25, 50, 75, 90, 95]:\n",
    "    print(f\"  {p}th percentile: {np.percentile(recon_sim_arr, p):.4f}\")\n",
    "\n",
    "# ===== ç­”æ¡ˆå˜åŒ–åˆ†æ =====\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Answer Change Analysis (ç”±äºé‡å»ºä¸å®Œç¾å¯¼è‡´çš„ç­”æ¡ˆå˜åŒ–)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nè¯´æ˜:\")\n",
    "print(\"  - 'åŸæœ¬' æŒ‡ä½¿ç”¨ Gemma centroid çš„é¢„æµ‹ç»“æœ\")\n",
    "print(\"  - 'é‡å»ºå' æŒ‡ä½¿ç”¨ V-H reconstructed vector çš„é¢„æµ‹ç»“æœ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_samples = len(reconstruction_analysis)\n",
    "print(f\"\\næ€»æ ·æœ¬æ•°: {total_samples}\")\n",
    "print(f\"\\nç­”æ¡ˆä¸€è‡´ (pred_gemma == pred_recon):\")\n",
    "print(f\"  ä¸¤è€…éƒ½å¯¹: {answer_change_stats['both_correct']} ({answer_change_stats['both_correct']/total_samples:.2%})\")\n",
    "print(f\"  ä¸¤è€…éƒ½é”™: {answer_change_stats['both_wrong']} ({answer_change_stats['both_wrong']/total_samples:.2%})\")\n",
    "\n",
    "print(f\"\\nç­”æ¡ˆæ”¹å˜ (pred_gemma != pred_recon):\")\n",
    "print(f\"  åŸæœ¬å¯¹ â†’ é‡å»ºåé”™: {answer_change_stats['originally_correct_now_wrong']} ({answer_change_stats['originally_correct_now_wrong']/total_samples:.2%})\")\n",
    "print(f\"  åŸæœ¬é”™ â†’ é‡å»ºåå¯¹: {answer_change_stats['originally_wrong_now_correct']} ({answer_change_stats['originally_wrong_now_correct']/total_samples:.2%})\")\n",
    "\n",
    "total_changed = answer_change_stats['originally_correct_now_wrong'] + answer_change_stats['originally_wrong_now_correct']\n",
    "print(f\"\\næ€»ç­”æ¡ˆæ”¹å˜æ•°: {total_changed} ({total_changed/total_samples:.2%})\")\n",
    "\n",
    "if answer_change_stats['originally_correct_now_wrong'] > 0:\n",
    "    # åˆ†æ\"åŸæœ¬å¯¹ä½†é€‰é”™\"çš„æ ·æœ¬çš„é‡å»ºç›¸ä¼¼åº¦\n",
    "    wrong_samples = answer_change_details['originally_correct_now_wrong']\n",
    "    wrong_sim = [s['reconstruction_similarity'] for s in wrong_samples]\n",
    "    print(f\"\\n'åŸæœ¬å¯¹â†’é‡å»ºåé”™' æ ·æœ¬çš„é‡å»ºç›¸ä¼¼åº¦åˆ†æ:\")\n",
    "    print(f\"  Mean: {np.mean(wrong_sim):.4f}\")\n",
    "    print(f\"  Median: {np.median(wrong_sim):.4f}\")\n",
    "\n",
    "if answer_change_stats['originally_wrong_now_correct'] > 0:\n",
    "    # åˆ†æ\"åŸæœ¬é”™ä½†é€‰å¯¹\"çš„æ ·æœ¬çš„é‡å»ºç›¸ä¼¼åº¦\n",
    "    correct_samples = answer_change_details['originally_wrong_now_correct']\n",
    "    correct_sim = [s['reconstruction_similarity'] for s in correct_samples]\n",
    "    print(f\"\\n'åŸæœ¬é”™â†’é‡å»ºåå¯¹' æ ·æœ¬çš„é‡å»ºç›¸ä¼¼åº¦åˆ†æ:\")\n",
    "    print(f\"  Mean: {np.mean(correct_sim):.4f}\")\n",
    "    print(f\"  Median: {np.median(correct_sim):.4f}\")\n",
    "\n",
    "# è®¡ç®—å‡€å½±å“ï¼ˆè´Ÿé¢å½±å“ - æ­£é¢å½±å“ï¼‰\n",
    "net_negative_impact = answer_change_stats['originally_correct_now_wrong'] - answer_change_stats['originally_wrong_now_correct']\n",
    "print(f\"\\nå‡€è´Ÿé¢å½±å“ (åŸæœ¬å¯¹â†’é”™ - åŸæœ¬é”™â†’å¯¹): {net_negative_impact}\")\n",
    "if net_negative_impact > 0:\n",
    "    print(f\"  â†’ é‡å»ºä¸å®Œç¾å¯¼è‡´å‡€æŸå¤± {net_negative_impact} ä¸ªæ­£ç¡®ç­”æ¡ˆ\")\n",
    "elif net_negative_impact < 0:\n",
    "    print(f\"  â†’ é‡å»ºä¸å®Œç¾åè€Œå¸¦æ¥å‡€å¢åŠ  {-net_negative_impact} ä¸ªæ­£ç¡®ç­”æ¡ˆ\")\n",
    "else:\n",
    "    print(f\"  â†’ é‡å»ºä¸å®Œç¾çš„æ­£è´Ÿå½±å“ç›¸äº’æŠµæ¶ˆ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcdcf53",
   "metadata": {},
   "source": [
    "## Section 7.5: é€æ­¥é‡å»ºè¯¯å·®åˆ†æ\n",
    "\n",
    "åˆ†æ Vector-HASH æ¨ç†æµç¨‹ä¸­æ¯ä¸€æ­¥çš„é‡å»ºè¯¯å·®ï¼š\n",
    "$$s_i \\rightarrow p_i \\rightarrow g_i^{noisy} \\rightarrow g_i^{clean} \\Rightarrow g_{i+1}^{clean} \\rightarrow p_{i+1} \\rightarrow s_{i+1}$$\n",
    "\n",
    "å¯¹äºæ¯ä¸€æ­¥ï¼Œè®¡ç®—å®é™…é‡å»ºå‘é‡ä¸çœŸå®ç›®æ ‡å‘é‡çš„è¯¯å·®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881fc39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 7.5: é€æ­¥é‡å»ºè¯¯å·®åˆ†æï¼ˆä½¿ç”¨ statistic=Trueï¼‰ =====\n",
    "\"\"\"\n",
    "åˆ†ææ¨ç†æµç¨‹æ¯ä¸€æ­¥çš„é‡å»ºè¯¯å·®\n",
    "\n",
    "æ¨ç†æµç¨‹ï¼š\n",
    "1. s_i -> p_i (é€šè¿‡ Wps)\n",
    "2. p_i -> g_i^noisy (é€šè¿‡ Wgp)\n",
    "3. g_i^noisy -> g_i^clean (é€šè¿‡ module_wise_NN_2d)\n",
    "4. ä»RRMé‡‡æ ·slotï¼Œå†³å®šyæ–¹å‘åç§»\n",
    "5. Path Integration: g_i^clean -> g_{i+1}^clean (é€šè¿‡ Wggï¼ŒåŒ…å«xå’Œyæ–¹å‘)\n",
    "6. g_{i+1}^clean -> p_{i+1} (é€šè¿‡ Wpg + nonlin)\n",
    "7. p_{i+1} -> s_{i+1} (é€šè¿‡ Wsp)\n",
    "\n",
    "è¯¯å·®è®¡ç®—ï¼šå®Œå…¨åœ¨å¤–éƒ¨è®¡ç®—ï¼Œä¸çœŸå®ç›®æ ‡å‘é‡ï¼ˆä»pbook/gbookè·å–ï¼‰å¯¹æ¯”\n",
    "æ³¨æ„ï¼šç”±äºRRMé‡‡æ ·å†³å®šyæ–¹å‘åç§»ï¼ŒçœŸå®çš„ç›®æ ‡ä½ç½®éœ€è¦æ ¹æ®é‡‡æ ·ç»“æœç¡®å®š\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Step-by-Step Reconstruction Error Analysis (RRM-aware)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# åŠ è½½ä½ç½®ä¿¡æ¯\n",
    "positions_path = os.path.join(RESULTS_DIR, 'positions.json')\n",
    "if not os.path.exists(positions_path):\n",
    "    print(f\"âš ï¸ {positions_path} not found. Skipping step-by-step error analysis.\")\n",
    "else:\n",
    "    with open(positions_path, 'r', encoding='utf-8') as f:\n",
    "        positions_data = json.load(f)\n",
    "    \n",
    "    premise_locs = positions_data.get('premise_locs', {})\n",
    "    print(f\"âœ… Loaded {len(premise_locs)} premise locations\")\n",
    "    \n",
    "    # ç”¨äºå­˜å‚¨æ¯ä¸€æ­¥çš„è¯¯å·®\n",
    "    step_errors = {\n",
    "        'err_p_i': [],       # p_i vs true_p_i\n",
    "        'err_g_i_noisy': [], # g_i^noisy vs true_g_i\n",
    "        'err_g_i_clean': [], # g_i^clean vs true_g_i  \n",
    "        'err_g_next': [],    # g_{i+1}^clean vs true_g_{i+1} (RRM-aware)\n",
    "        'err_p_next': [],    # p_{i+1} vs true_p_{i+1} (RRM-aware)\n",
    "        'err_s_next': [],    # s_{i+1} vs true_s_{i+1} (gemma_centroid)\n",
    "    }\n",
    "    \n",
    "    step_error_details = []\n",
    "    skipped_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for r in tqdm(inference_results[:100], desc=\"Computing step-by-step errors\"):  # å…ˆå¤„ç†å‰100ä¸ªæ ·æœ¬ä½œä¸ºç¤ºä¾‹\n",
    "        idx = r['INDEX']\n",
    "        sample_idx = r['sample_idx']\n",
    "        askfor = r['askfor']\n",
    "        \n",
    "        # æ„é€ ä½ç½®key\n",
    "        loc_key = f\"{idx}_{askfor}\"\n",
    "        \n",
    "        if loc_key not in premise_locs:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        # è·å–premiseä½ç½®\n",
    "        pos_i = premise_locs[loc_key]\n",
    "        x_i, y_i = pos_i[0], pos_i[1]\n",
    "        \n",
    "        # æ£€æŸ¥å½“å‰ä½ç½®æ˜¯å¦åœ¨åˆæ³•èŒƒå›´å†…\n",
    "        if not (0 <= x_i < Npos and 0 <= y_i < Npos):\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        # è·å–è¾“å…¥å‘é‡å’ŒçœŸå®ç›®æ ‡å‘é‡\n",
    "        if askfor == 'cause':\n",
    "            s_i = causal_B[sample_idx]  # premise (Effect)\n",
    "            gemma_centroid = causal_A[sample_idx]  # target (Cause)\n",
    "        else:\n",
    "            s_i = causal_A[sample_idx]  # premise (Cause)\n",
    "            gemma_centroid = causal_B[sample_idx]  # target (Effect)\n",
    "        \n",
    "        # è·å–å½“å‰ä½ç½®çš„çœŸå®å‘é‡\n",
    "        true_p_i = pbook[:, x_i, y_i]\n",
    "        true_g_i = gbook[:, x_i, y_i]\n",
    "        \n",
    "        try:\n",
    "            # è°ƒç”¨æ¨ç†å‡½æ•°ï¼ˆstatistic=Trueï¼Œè·å–ä¸­é—´å‘é‡ï¼‰\n",
    "            result = vector_hash_infer_with_rrm(\n",
    "                s_input=s_i,\n",
    "                askfor=askfor,\n",
    "                Wps=Wps, Wsp=Wsp, Wgp=Wgp, Wpg=Wpg,\n",
    "                Wgg_c2e=Wgg_c2e, Wgg_e2c=Wgg_e2c,\n",
    "                module_gbooks=module_gbooks,\n",
    "                module_sizes=module_sizes,\n",
    "                rrm_model=rrm_model_infer,\n",
    "                source_id=idx,\n",
    "                sampling_mode=\"max_prob\",\n",
    "                statistic=True\n",
    "            )\n",
    "            \n",
    "            # æ£€æŸ¥è¿”å›å€¼\n",
    "            if len(result) != 3:\n",
    "                print(f\"âš ï¸ Sample {idx}: unexpected return length {len(result)}\")\n",
    "                error_count += 1\n",
    "                continue\n",
    "                \n",
    "            s_output, sampled_slot, stats_dict = result\n",
    "            \n",
    "        except Exception as e:\n",
    "            if error_count < 5:  # åªæ‰“å°å‰5ä¸ªé”™è¯¯\n",
    "                print(f\"âš ï¸ Error on sample {idx} (askfor={askfor}): {e}\")\n",
    "            error_count += 1\n",
    "            continue\n",
    "        \n",
    "        # ä»stats_dictè·å–delta_yå’Œä¸­é—´å‘é‡\n",
    "        delta_y = stats_dict['delta_y']\n",
    "        p_i = stats_dict['p_i']\n",
    "        g_i_noisy = stats_dict['g_i_noisy']\n",
    "        g_i_clean = stats_dict['g_i_clean']\n",
    "        g_next = stats_dict['g_next']\n",
    "        p_next = stats_dict['p_next']\n",
    "        \n",
    "        # è®¡ç®—å®é™…çš„ç›®æ ‡ä½ç½®ï¼ˆè€ƒè™‘RRMé‡‡æ ·ï¼‰\n",
    "        if askfor == 'effect':\n",
    "            x_next = x_i + 1\n",
    "        else:\n",
    "            x_next = x_i - 1\n",
    "        y_next = y_i + delta_y\n",
    "        \n",
    "        # æ£€æŸ¥ç›®æ ‡ä½ç½®æ˜¯å¦åœ¨åˆæ³•èŒƒå›´å†…\n",
    "        if not (0 <= x_next < Npos and 0 <= y_next < Npos):\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        # è·å–çœŸå®çš„ç›®æ ‡å‘é‡ï¼ˆRRM-awareï¼‰\n",
    "        true_p_next = pbook[:, x_next, y_next]\n",
    "        true_g_next = gbook[:, x_next, y_next]\n",
    "        \n",
    "        # ===== åœ¨å¤–éƒ¨è®¡ç®—æ‰€æœ‰è¯¯å·® =====\n",
    "        err_p_i = np.sum(np.abs(p_i - true_p_i)) / (2 * Np)\n",
    "        err_g_i_noisy = np.sum(np.abs(g_i_noisy - true_g_i)) / (2 * Ng)\n",
    "        err_g_i_clean = np.sum(np.abs(g_i_clean - true_g_i)) / (2 * Ng)\n",
    "        err_g_next = np.sum(np.abs(g_next - true_g_next)) / (2 * Ng)\n",
    "        err_p_next = np.sum(np.abs(p_next - true_p_next)) / (2 * Np)\n",
    "        err_s_next = np.sum(np.abs(s_output - gemma_centroid)) / (2 * Ns)\n",
    "        \n",
    "        # è®°å½•è¯¯å·®\n",
    "        step_errors['err_p_i'].append(err_p_i)\n",
    "        step_errors['err_g_i_noisy'].append(err_g_i_noisy)\n",
    "        step_errors['err_g_i_clean'].append(err_g_i_clean)\n",
    "        step_errors['err_g_next'].append(err_g_next)\n",
    "        step_errors['err_p_next'].append(err_p_next)\n",
    "        step_errors['err_s_next'].append(err_s_next)\n",
    "        \n",
    "        step_error_details.append({\n",
    "            'INDEX': idx,\n",
    "            'askfor': askfor,\n",
    "            'pos_i': [int(x_i), int(y_i)],\n",
    "            'pos_next': [int(x_next), int(y_next)],\n",
    "            'delta_y': int(delta_y),\n",
    "            'sampled_slot': int(sampled_slot),\n",
    "            'err_p_i': float(err_p_i),\n",
    "            'err_g_i_noisy': float(err_g_i_noisy),\n",
    "            'err_g_i_clean': float(err_g_i_clean),\n",
    "            'err_g_next': float(err_g_next),\n",
    "            'err_p_next': float(err_p_next),\n",
    "            'err_s_next': float(err_s_next)\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nâœ… Computed step-by-step errors for {len(step_error_details)} samples\")\n",
    "    print(f\"âš ï¸ Skipped {skipped_count} samples (missing position or out of range)\")\n",
    "    print(f\"âŒ Error count: {error_count} samples\")\n",
    "    \n",
    "    # ç»Ÿè®¡ç»“æœ\n",
    "    if len(step_error_details) > 0:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"Step-by-Step Reconstruction Error Statistics (RRM-aware)\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nè¯¯å·®å®šä¹‰: err = sum(|predicted - true|) / (2 * N)\")\n",
    "        print(\"  å…¶ä¸­ N æ˜¯è¯¥å±‚çš„ç»´åº¦ (Np, Ng, æˆ– Ns)\")\n",
    "        print(\"  è¯¯å·®èŒƒå›´: [0, 1], 0è¡¨ç¤ºå®Œç¾é‡å»º, 1è¡¨ç¤ºå®Œå…¨ç›¸å\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        step_names = [\n",
    "            ('err_p_i',       's_i â†’ p_i'),\n",
    "            ('err_g_i_noisy', 'p_i â†’ g_i^noisy'),\n",
    "            ('err_g_i_clean', 'g_i^noisy â†’ g_i^clean (cleanup)'),\n",
    "            ('err_g_next',    'g_i^clean â†’ g_{i+1}^clean (path integration + RRM)'),\n",
    "            ('err_p_next',    'g_{i+1}^clean â†’ p_{i+1}'),\n",
    "            ('err_s_next',    'p_{i+1} â†’ s_{i+1} (vs gemma_centroid)'),\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\n{'Step':<55} {'Mean Err':>12} {'Std':>12} {'Min':>12} {'Max':>12}\")\n",
    "        print(\"-\"*100)\n",
    "        for key, name in step_names:\n",
    "            errs = np.array(step_errors[key])\n",
    "            print(f\"{name:<55} {errs.mean():>12.4f} {errs.std():>12.4f} {errs.min():>12.4f} {errs.max():>12.4f}\")\n",
    "        \n",
    "        # ç´¯ç§¯è¯¯å·®åˆ†æ\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"Cumulative Error Analysis (è¯¯å·®å¦‚ä½•åœ¨æµç¨‹ä¸­ç´¯ç§¯)\")\n",
    "        print(\"=\"*70)\n",
    "        total_err = (np.array(step_errors['err_p_i']) + \n",
    "                     np.array(step_errors['err_g_i_noisy']) +\n",
    "                     np.array(step_errors['err_g_i_clean']) +\n",
    "                     np.array(step_errors['err_g_next']) +\n",
    "                     np.array(step_errors['err_p_next']) +\n",
    "                     np.array(step_errors['err_s_next']))\n",
    "        print(f\"Total accumulated error (mean): {total_err.mean():.4f}\")\n",
    "        print(f\"Total accumulated error (std):  {total_err.std():.4f}\")\n",
    "        \n",
    "        # æ‰¾å‡ºè¯¯å·®æœ€å¤§çš„æ­¥éª¤\n",
    "        mean_errors = {name: np.mean(step_errors[key]) for key, name in step_names}\n",
    "        worst_step = max(mean_errors.items(), key=lambda x: x[1])\n",
    "        best_step = min(mean_errors.items(), key=lambda x: x[1])\n",
    "        print(f\"\\nè¯¯å·®æœ€å¤§çš„æ­¥éª¤: {worst_step[0]} (mean error = {worst_step[1]:.4f})\")\n",
    "        print(f\"è¯¯å·®æœ€å°çš„æ­¥éª¤: {best_step[0]} (mean error = {best_step[1]:.4f})\")\n",
    "        \n",
    "        # RRMé‡‡æ ·ç»Ÿè®¡\n",
    "        delta_y_values = [d['delta_y'] for d in step_error_details]\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"RRM Sampling Statistics (yæ–¹å‘åç§»ç»Ÿè®¡)\")\n",
    "        print(\"=\"*70)\n",
    "        delta_y_counts = {}\n",
    "        for dy in delta_y_values:\n",
    "            delta_y_counts[dy] = delta_y_counts.get(dy, 0) + 1\n",
    "        for dy in sorted(delta_y_counts.keys()):\n",
    "            print(f\"  delta_y = {dy:+2d}: {delta_y_counts[dy]:4d} samples ({100*delta_y_counts[dy]/len(delta_y_values):.1f}%)\")\n",
    "        \n",
    "        # ä¿å­˜è¯¦ç»†ç»“æœ\n",
    "        step_error_path = os.path.join(RESULTS_DIR, 'step_by_step_errors_rrm_aware.json')\n",
    "        with open(step_error_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'statistics': {key: {\n",
    "                    'mean': float(np.mean(step_errors[key])),\n",
    "                    'std': float(np.std(step_errors[key])),\n",
    "                    'min': float(np.min(step_errors[key])),\n",
    "                    'max': float(np.max(step_errors[key]))\n",
    "                } for key, _ in step_names},\n",
    "                'rrm_sampling': {\n",
    "                    'delta_y_distribution': delta_y_counts,\n",
    "                },\n",
    "                'detailed': step_error_details\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\nâœ… Step-by-step errors (RRM-aware) saved to: {step_error_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227328c8",
   "metadata": {},
   "source": [
    "## Section 8: ç»“æœä¿å­˜å’Œå¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73930128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 9: ä¿å­˜è¯„ä¼°ç»“æœ =====\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Saving Evaluation Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# å‡†å¤‡ä¿å­˜çš„æ•°æ®\n",
    "results_summary = {\n",
    "    'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'total_samples': len(inference_results),\n",
    "    'model_config': {\n",
    "        'lambdas': lambdas,\n",
    "        'Ng': int(Ng),\n",
    "        'Npos': int(Npos),\n",
    "        'Np': int(Np),\n",
    "        'Ns': int(Ns),\n",
    "        'max_effects': max_effects,\n",
    "        'thresh': float(thresh)\n",
    "    },\n",
    "    'gemma_centroid': {\n",
    "        'accuracy': float(correct_gemma_arr.mean()),\n",
    "        'correct_count': int(correct_gemma_arr.sum()),\n",
    "        'total_count': len(correct_gemma_arr)\n",
    "    },\n",
    "    'vh_reconstructed': {\n",
    "        'accuracy': float(correct_recon_arr.mean()),\n",
    "        'correct_count': int(correct_recon_arr.sum()),\n",
    "        'total_count': len(correct_recon_arr)\n",
    "    },\n",
    "    'reconstruction_analysis': {\n",
    "        'mean_reconstruction_similarity': float(recon_sim_arr.mean()),\n",
    "        'std_reconstruction_similarity': float(recon_sim_arr.std()),\n",
    "        'min_reconstruction_similarity': float(recon_sim_arr.min()),\n",
    "        'max_reconstruction_similarity': float(recon_sim_arr.max()),\n",
    "        'mean_gemma_confidence': float(gemma_conf_arr.mean()),\n",
    "        'mean_similarity_delta': float(sim_delta_arr.mean()),\n",
    "        'positive_delta_samples': int((sim_delta_arr > 0).sum()),\n",
    "        'negative_delta_samples': int((sim_delta_arr < 0).sum())\n",
    "    },\n",
    "    'answer_change_analysis': {\n",
    "        'total_samples': len(reconstruction_analysis),\n",
    "        'both_correct': answer_change_stats['both_correct'],\n",
    "        'both_wrong': answer_change_stats['both_wrong'],\n",
    "        'originally_correct_now_wrong': answer_change_stats['originally_correct_now_wrong'],\n",
    "        'originally_wrong_now_correct': answer_change_stats['originally_wrong_now_correct'],\n",
    "        'total_answer_changed': answer_change_stats['originally_correct_now_wrong'] + answer_change_stats['originally_wrong_now_correct'],\n",
    "        'net_negative_impact': answer_change_stats['originally_correct_now_wrong'] - answer_change_stats['originally_wrong_now_correct']\n",
    "    }\n",
    "}\n",
    "\n",
    "# æŒ‰ askfor åˆ†ç»„ç»Ÿè®¡\n",
    "for askfor_type in ['cause', 'effect']:\n",
    "    subset = [r for r in inference_results if r['askfor'] == askfor_type]\n",
    "    if subset:\n",
    "        results_summary[f'{askfor_type}_analysis'] = {\n",
    "            'count': len(subset),\n",
    "            'gemma_accuracy': float(np.mean([r['correct_gemma'] for r in subset])),\n",
    "            'vh_accuracy': float(np.mean([r['correct_recon'] for r in subset]))\n",
    "        }\n",
    "\n",
    "# ä¿å­˜æ‘˜è¦\n",
    "summary_path = os.path.join(RESULTS_DIR, 'inference_evaluation.json')\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_summary, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… Summary saved to: {summary_path}\")\n",
    "\n",
    "# ä¿å­˜è¯¦ç»†ç»“æœ\n",
    "detailed_path = os.path.join(RESULTS_DIR, 'inference_detailed.json')\n",
    "with open(detailed_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(inference_results, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… Detailed results saved to: {detailed_path}\")\n",
    "\n",
    "# ä¿å­˜é‡å»ºåˆ†æç»“æœï¼ˆåŒ…å«ç­”æ¡ˆå˜åŒ–è¯¦æƒ…ï¼‰\n",
    "reconstruction_path = os.path.join(RESULTS_DIR, 'reconstruction_analysis.json')\n",
    "with open(reconstruction_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        'timestamp': results_summary['timestamp'],\n",
    "        'statistics': results_summary['reconstruction_analysis'],\n",
    "        'answer_change_statistics': results_summary['answer_change_analysis'],\n",
    "        'answer_change_details': answer_change_details,\n",
    "        'detailed': reconstruction_analysis\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… Reconstruction analysis saved to: {reconstruction_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"Total samples: {results_summary['total_samples']}\n",
    "\n",
    "Model Configuration:\n",
    "  lambdas: {results_summary['model_config']['lambdas']}\n",
    "  Ng: {results_summary['model_config']['Ng']}\n",
    "  Npos: {results_summary['model_config']['Npos']}\n",
    "  Np: {results_summary['model_config']['Np']}\n",
    "\n",
    "Inference Accuracy:\n",
    "  Gemma Centroid:           {results_summary['gemma_centroid']['accuracy']:.2%} ({results_summary['gemma_centroid']['correct_count']}/{results_summary['gemma_centroid']['total_count']})\n",
    "  V-H Reconstructed:       {results_summary['vh_reconstructed']['accuracy']:.2%} ({results_summary['vh_reconstructed']['correct_count']}/{results_summary['vh_reconstructed']['total_count']})\n",
    "  Performance delta:       {results_summary['vh_reconstructed']['accuracy'] - results_summary['gemma_centroid']['accuracy']:+.2%}\n",
    "\n",
    "Reconstruction Quality:\n",
    "  Mean V-Hâ†’Gemma similarity:      {results_summary['reconstruction_analysis']['mean_reconstruction_similarity']:.4f}\n",
    "  Std deviation:                  {results_summary['reconstruction_analysis']['std_reconstruction_similarity']:.4f}\n",
    "  Mean Gemma confidence (â†’H1/H2): {results_summary['reconstruction_analysis']['mean_gemma_confidence']:.4f}\n",
    "  Mean similarity delta (V-H-Gemma): {results_summary['reconstruction_analysis']['mean_similarity_delta']:+.4f}\n",
    "\n",
    "Answer Change Impact (é‡å»ºä¸å®Œç¾çš„å½±å“):\n",
    "  åŸæœ¬å¯¹â†’é‡å»ºåé”™: {results_summary['answer_change_analysis']['originally_correct_now_wrong']} ({results_summary['answer_change_analysis']['originally_correct_now_wrong']/results_summary['total_samples']:.2%})\n",
    "  åŸæœ¬é”™â†’é‡å»ºåå¯¹: {results_summary['answer_change_analysis']['originally_wrong_now_correct']} ({results_summary['answer_change_analysis']['originally_wrong_now_correct']/results_summary['total_samples']:.2%})\n",
    "  å‡€è´Ÿé¢å½±å“:      {results_summary['answer_change_analysis']['net_negative_impact']}\n",
    "\n",
    "Random Baseline: 50.00%\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635fcaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 10: Visualization =====\n",
    "\"\"\"\n",
    "Visualization of inference results and step-by-step reconstruction errors\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Set global font to Times New Roman\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['mathtext.fontset'] = 'custom'\n",
    "plt.rcParams['mathtext.rm'] = 'Times New Roman'\n",
    "plt.rcParams['mathtext.it'] = 'Times New Roman:italic'\n",
    "plt.rcParams['mathtext.bf'] = 'Times New Roman:bold'\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 11\n",
    "plt.rcParams['ytick.labelsize'] = 11\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "# Define color palette (sophisticated colors)\n",
    "colors_palette = {\n",
    "    'gemma_cause': '#2E86AB',      # Steel Blue\n",
    "    'gemma_effect': '#A23B72',     # Dark Pink\n",
    "    'vh_cause': '#F18F01',         # Orange\n",
    "    'vh_effect': '#C73E1D',        # Red\n",
    "    'correct_to_wrong': '#E63946', # Bright Red\n",
    "    'wrong_to_correct': '#2A9D8F', # Teal\n",
    "    'random': '#6C757D',           # Gray\n",
    "}\n",
    "\n",
    "# ===== Figure 1: Inference Accuracy & Answer Change Impact =====\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Vector-HASH Inference Evaluation', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "# --- Left subplot: Inference Accuracy by askfor type ---\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Calculate accuracy by askfor\n",
    "cause_results = [r for r in inference_results if r['askfor'] == 'cause']\n",
    "effect_results = [r for r in inference_results if r['askfor'] == 'effect']\n",
    "\n",
    "gemma_cause_acc = np.mean([r['correct_gemma'] for r in cause_results]) * 100\n",
    "gemma_effect_acc = np.mean([r['correct_gemma'] for r in effect_results]) * 100\n",
    "vh_cause_acc = np.mean([r['correct_recon'] for r in cause_results]) * 100\n",
    "vh_effect_acc = np.mean([r['correct_recon'] for r in effect_results]) * 100\n",
    "\n",
    "# Bar positions\n",
    "x = np.arange(2)  # Cause, Effect\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, [gemma_cause_acc, gemma_effect_acc], width, \n",
    "                label='Gemma Centroid', color=[colors_palette['gemma_cause'], colors_palette['gemma_effect']], \n",
    "                edgecolor='white', linewidth=1.5, alpha=0.9)\n",
    "bars2 = ax1.bar(x + width/2, [vh_cause_acc, vh_effect_acc], width, \n",
    "                label='V-H Reconstructed', color=[colors_palette['vh_cause'], colors_palette['vh_effect']], \n",
    "                edgecolor='white', linewidth=1.5, alpha=0.9)\n",
    "\n",
    "# Add value labels on bars\n",
    "def add_value_labels(ax, bars, fmt='{:.1f}%'):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(fmt.format(height),\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "add_value_labels(ax1, bars1)\n",
    "add_value_labels(ax1, bars2)\n",
    "\n",
    "# Add random baseline\n",
    "ax1.axhline(y=50, color=colors_palette['random'], linestyle='--', linewidth=2, alpha=0.7, label='Random Baseline')\n",
    "\n",
    "ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax1.set_xlabel('ASKFOR Type', fontsize=12)\n",
    "ax1.set_title('Inference Accuracy by ASKFOR Type', fontsize=13, fontweight='bold', pad=10)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(['Cause', 'Effect'])\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.legend(loc='upper right', framealpha=0.9)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# --- Right subplot: Answer Change Impact ---\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Data for answer change\n",
    "categories = ['Correctâ†’Wrong\\n(Negative)', 'Wrongâ†’Correct\\n(Positive)']\n",
    "values = [\n",
    "    answer_change_stats['originally_correct_now_wrong'],\n",
    "    answer_change_stats['originally_wrong_now_correct']\n",
    "]\n",
    "percentages = [v / len(reconstruction_analysis) * 100 for v in values]\n",
    "\n",
    "bar_colors = [colors_palette['correct_to_wrong'], colors_palette['wrong_to_correct']]\n",
    "bars3 = ax2.bar(categories, values, color=bar_colors, edgecolor='white', linewidth=1.5, alpha=0.9)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val, pct in zip(bars3, values, percentages):\n",
    "    height = bar.get_height()\n",
    "    ax2.annotate(f'{val}\\n({pct:.1f}%)',\n",
    "                 xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                 xytext=(0, 3),\n",
    "                 textcoords=\"offset points\",\n",
    "                 ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Add net impact annotation\n",
    "net_impact = values[0] - values[1]\n",
    "impact_text = f'Net Negative Impact: {net_impact}'\n",
    "ax2.annotate(impact_text, xy=(0.5, 0.95), xycoords='axes fraction',\n",
    "             ha='center', va='top', fontsize=12, fontweight='bold',\n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow', edgecolor='gray', alpha=0.8))\n",
    "\n",
    "ax2.set_ylabel('Number of Samples', fontsize=12)\n",
    "ax2.set_title('Answer Change Impact\\n(Due to Imperfect Reconstruction)', fontsize=13, fontweight='bold', pad=10)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Figure 1 saved to {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6d9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 11: Step-by-Step Reconstruction Error Visualization =====\n",
    "\"\"\"\n",
    "Visualization of step-by-step reconstruction errors with LaTeX labels\n",
    "\"\"\"\n",
    "\n",
    "# Set up the figure with sophisticated style\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Define step labels with LaTeX formatting\n",
    "step_labels = [\n",
    "    r'$s_i \\rightarrow p_i$',\n",
    "    r'$p_i \\rightarrow g_i^{\\mathrm{noisy}}$',\n",
    "    r'$g_i^{\\mathrm{noisy}} \\rightarrow g_i^{\\mathrm{clean}}$',\n",
    "    r'$g_i^{\\mathrm{clean}} \\rightarrow g_{i+1}^{\\mathrm{clean}}$',\n",
    "    r'$g_{i+1}^{\\mathrm{clean}} \\rightarrow p_{i+1}$',\n",
    "    r'$p_{i+1} \\rightarrow s_{i+1}$'\n",
    "]\n",
    "\n",
    "# Mean errors for each step\n",
    "mean_errors = [\n",
    "    np.mean(step_errors['err_p_i']),\n",
    "    np.mean(step_errors['err_g_i_noisy']),\n",
    "    np.mean(step_errors['err_g_i_clean']),\n",
    "    np.mean(step_errors['err_g_next']),\n",
    "    np.mean(step_errors['err_p_next']),\n",
    "    np.mean(step_errors['err_s_next'])\n",
    "]\n",
    "\n",
    "# Color gradient from cool to warm\n",
    "colors_gradient = ['#264653', '#2A9D8F', '#E9C46A', '#F4A261', '#E76F51', '#9B2226']\n",
    "\n",
    "# Create bar plot\n",
    "x_pos = np.arange(len(step_labels))\n",
    "bars = ax.bar(x_pos, mean_errors, color=colors_gradient, edgecolor='white', linewidth=2, alpha=0.9)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for bar, val in zip(bars, mean_errors):\n",
    "    height = bar.get_height()\n",
    "    if height > 0.001:  # Only show label if error > 0.001\n",
    "        ax.annotate(f'{val:.4f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 5),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    else:\n",
    "        ax.annotate(f'{val:.4f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, 0.002),\n",
    "                    xytext=(0, 5),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10, color='gray')\n",
    "\n",
    "# Styling\n",
    "ax.set_xlabel('Inference Step', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Mean Reconstruction Error', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Step-by-Step Reconstruction Error in Vector-HASH Inference', fontsize=15, fontweight='bold', pad=15)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(step_labels, fontsize=12)\n",
    "ax.set_ylim(0, max(mean_errors) * 1.25)\n",
    "\n",
    "# Add grid\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--', zorder=0)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Remove top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add a subtle annotation about error definition\n",
    "ax.annotate(r'Error: $\\frac{\\sum |predicted - true|}{2N}$', \n",
    "            xy=(0.98, 0.95), xycoords='axes fraction',\n",
    "            ha='right', va='top', fontsize=11,\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='whitesmoke', edgecolor='gray', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Figure 2 saved to {RESULTS_DIR}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Step-by-Step Error Summary\")\n",
    "print(\"=\"*60)\n",
    "for label, err in zip(step_labels, mean_errors):\n",
    "    print(f\"  {label}: {err:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b9a5ec",
   "metadata": {},
   "source": [
    "## Section 9: è‡ªé‡å»ºæµç¨‹ç»Ÿè®¡æµ‹è¯•\n",
    "\n",
    "æœ¬èŠ‚è¿›è¡Œè‡ªé‡å»ºæµ‹è¯•ï¼Œåˆ†æè®­ç»ƒæ¬¡æ•°ä¸é‡å»ºç‡çš„å…³ç³»ï¼š\n",
    "$$s_i \\rightarrow p_i \\rightarrow g_i^{noisy} \\rightarrow g_i^{clean} \\rightarrow p_{i} \\rightarrow s_{i}$$\n",
    "\n",
    "**åŠ¨æœº**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œpremiseè®­ç»ƒäº†100æ¬¡ï¼Œä½†æ¯ä¸ªå›ç­”ï¼ˆClusterçš„Centroidå‘é‡ï¼‰è®­ç»ƒçš„æ¬¡æ•°å–å†³äºè¯¥clusterçš„æ ·æœ¬æ•°é‡ï¼ˆå°‘äº100æ¬¡ï¼‰ã€‚æˆ‘ä»¬æƒ³éªŒè¯é‡å»ºç‡æ˜¯å¦ä¸è®­ç»ƒæ¬¡æ•°ç›¸å…³ã€‚\n",
    "\n",
    "**åˆ†æå†…å®¹**ï¼š\n",
    "1. Premise å„æ­¥é‡å»ºç‡ï¼ˆè®­ç»ƒ100æ¬¡ï¼‰\n",
    "2. æœ€å¤§ Cluster çš„ Centroid å„æ­¥é‡å»ºç‡ï¼ˆè®­ç»ƒæ¬¡æ•°=clusteræ ·æœ¬æ•°ï¼‰\n",
    "3. é‡å»ºç‡ä¸è®­ç»ƒæ¬¡æ•°çš„å…³ç³»æ›²çº¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c97902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 9.1: è‡ªé‡å»ºæµç¨‹åˆ†æ =====\n",
    "\"\"\"\n",
    "è‡ªé‡å»ºæµç¨‹ï¼šs_i -> p_i -> g_i^noisy -> g_i^clean -> p_i -> s_i\n",
    "ä¸è¿›è¡Œè·¯å¾„ç§¯åˆ†ï¼Œæµ‹è¯•ä»è¾“å…¥é‡å»ºå›è¾“å…¥çš„èƒ½åŠ›\n",
    "\n",
    "åˆ†æå†…å®¹ï¼š\n",
    "1. Premise çš„è‡ªé‡å»ºï¼ˆæ¯ä¸ªpremiseè®­ç»ƒ100æ¬¡ï¼‰\n",
    "2. æœ€å¤§ Cluster Centroid çš„è‡ªé‡å»ºï¼ˆè®­ç»ƒæ¬¡æ•°=clusteræ ·æœ¬æ•°ï¼‰\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Self-Reconstruction Analysis (æ— è·¯å¾„ç§¯åˆ†)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def self_reconstruction(s_input, Wps, Wsp, Wgp, Wpg, module_gbooks, module_sizes, thresh=2.0):\n",
    "    \"\"\"\n",
    "    è‡ªé‡å»ºæµç¨‹: s_i -> p_i -> g_i^noisy -> g_i^clean -> p_i' -> s_i'\n",
    "    ä¸è¿›è¡Œè·¯å¾„ç§¯åˆ†\n",
    "    \n",
    "    Returns:\n",
    "        s_output: é‡å»ºåçš„sensoryå‘é‡\n",
    "        stats_dict: åŒ…å«ä¸­é—´æ­¥éª¤å‘é‡çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # å½’ä¸€åŒ–è¾“å…¥\n",
    "    s = normalize_vec(s_input)\n",
    "    \n",
    "    # Step 1: Sensory -> Place cell\n",
    "    p_i = sensory_to_place(s, Wps)\n",
    "    \n",
    "    # Step 2: Place cell -> Grid cell (noisy)\n",
    "    g_i_noisy = place_to_grid_noisy(p_i, Wgp)\n",
    "    \n",
    "    # Step 3: Grid cell æ¸…æ´— (ä½¿ç”¨ module_wise_NN_2d)\n",
    "    gin = g_i_noisy.reshape(1, -1, 1)  # (1, Ng, 1)\n",
    "    g_i_clean = module_wise_NN_2d(gin, module_gbooks, module_sizes)\n",
    "    g_i_clean = g_i_clean[0, :, 0]  # (Ng,)\n",
    "    \n",
    "    # Step 4: Grid cell -> Place cell (ç›´æ¥ä»g_i_cleanï¼Œä¸è¿›è¡Œè·¯å¾„ç§¯åˆ†)\n",
    "    p_i_reconstructed = grid_to_place_with_nonlin(g_i_clean, Wpg, thresh=thresh)\n",
    "    \n",
    "    # Step 5: Place cell -> Sensory\n",
    "    s_output = place_to_sensory(p_i_reconstructed, Wsp)\n",
    "    \n",
    "    stats_dict = {\n",
    "        's_input': s,\n",
    "        'p_i': p_i,\n",
    "        'g_i_noisy': g_i_noisy,\n",
    "        'g_i_clean': g_i_clean,\n",
    "        'p_i_reconstructed': p_i_reconstructed,\n",
    "        's_output': s_output,\n",
    "    }\n",
    "    \n",
    "    return s_output, stats_dict\n",
    "\n",
    "\n",
    "# ===== å‡†å¤‡æ•°æ®ï¼šæ„å»º premise å’Œ centroid çš„å‘é‡æ˜ å°„ =====\n",
    "print(\"\\n1. å‡†å¤‡ Premise å’Œ Centroid å‘é‡...\")\n",
    "\n",
    "# åŠ è½½ä½ç½®ä¿¡æ¯\n",
    "positions_path = os.path.join(RESULTS_DIR, 'positions.json')\n",
    "with open(positions_path, 'r', encoding='utf-8') as f:\n",
    "    positions_data = json.load(f)\n",
    "premise_locs = positions_data.get('premise_locs', {})\n",
    "print(f\"   Loaded {len(premise_locs)} premise locations\")\n",
    "\n",
    "# å‡†å¤‡æ•°æ®ç»“æ„\n",
    "premise_self_recon_data = []  # Premiseè‡ªé‡å»ºæ•°æ®\n",
    "centroid_self_recon_data = []  # Centroidè‡ªé‡å»ºæ•°æ®\n",
    "\n",
    "# éå†æ‰€æœ‰æ ·æœ¬\n",
    "for idx, sample_idx in tqdm(index_to_largest_sample.items(), desc=\"Preparing data\"):\n",
    "    meta = causal_metadata[sample_idx]\n",
    "    askfor = meta['ASKFOR']\n",
    "    \n",
    "    # æ„é€ ä½ç½®key\n",
    "    loc_key = f\"{idx}_{askfor}\"\n",
    "    if loc_key not in premise_locs:\n",
    "        continue\n",
    "    \n",
    "    pos_i = premise_locs[loc_key]\n",
    "    x_i, y_i = pos_i[0], pos_i[1]\n",
    "    \n",
    "    # æ£€æŸ¥ä½ç½®æ˜¯å¦åœ¨åˆæ³•èŒƒå›´å†…\n",
    "    if not (0 <= x_i < Npos and 0 <= y_i < Npos):\n",
    "        continue\n",
    "    \n",
    "    # è·å– premise å‘é‡ (è®­ç»ƒ100æ¬¡)\n",
    "    # askfor='cause' â†’ premiseæ˜¯Effect (causal_B)\n",
    "    # askfor='effect' â†’ premiseæ˜¯Cause (causal_A)\n",
    "    if askfor == 'cause':\n",
    "        premise_vec = causal_B[sample_idx]\n",
    "        centroid_vec = causal_A[sample_idx]  # Gemmaæ¨æ–­çš„Cause centroid\n",
    "    else:\n",
    "        premise_vec = causal_A[sample_idx]\n",
    "        centroid_vec = causal_B[sample_idx]  # Gemmaæ¨æ–­çš„Effect centroid\n",
    "    \n",
    "    # è·å–çœŸå®çš„pbookå’Œgbookå‘é‡ï¼ˆç”¨äºè¯¯å·®è®¡ç®—ï¼‰\n",
    "    true_p = pbook[:, x_i, y_i]\n",
    "    true_g = gbook[:, x_i, y_i]\n",
    "    \n",
    "    # è·å– cluster å¤§å°ï¼ˆè®­ç»ƒæ¬¡æ•°ï¼‰\n",
    "    if idx in index_to_largest_cluster:\n",
    "        cluster_size = index_to_largest_cluster[idx]['largest_count']\n",
    "    else:\n",
    "        cluster_size = 1  # é»˜è®¤\n",
    "    \n",
    "    # æ·»åŠ åˆ°æ•°æ®é›†\n",
    "    premise_self_recon_data.append({\n",
    "        'idx': idx,\n",
    "        'sample_idx': sample_idx,\n",
    "        'askfor': askfor,\n",
    "        'pos': (x_i, y_i),\n",
    "        's_input': premise_vec,\n",
    "        'true_p': true_p,\n",
    "        'true_g': true_g,\n",
    "        'training_count': 100,  # Premiseè®­ç»ƒ100æ¬¡\n",
    "    })\n",
    "    \n",
    "    centroid_self_recon_data.append({\n",
    "        'idx': idx,\n",
    "        'sample_idx': sample_idx,\n",
    "        'askfor': askfor,\n",
    "        'pos': (x_i, y_i),\n",
    "        's_input': centroid_vec,\n",
    "        'true_p': true_p,  # æ³¨æ„ï¼šcentroidä½¿ç”¨ç›¸åŒçš„ä½ç½®\n",
    "        'true_g': true_g,\n",
    "        'training_count': cluster_size,  # Centroidè®­ç»ƒæ¬¡æ•°=clusterå¤§å°\n",
    "    })\n",
    "\n",
    "print(f\"   Prepared {len(premise_self_recon_data)} premise samples\")\n",
    "print(f\"   Prepared {len(centroid_self_recon_data)} centroid samples\")\n",
    "\n",
    "\n",
    "# ===== å¯¹ Premise è¿›è¡Œè‡ªé‡å»ºåˆ†æ =====\n",
    "print(\"\\n2. å¯¹ Premise è¿›è¡Œè‡ªé‡å»ºåˆ†æï¼ˆè®­ç»ƒ100æ¬¡ï¼‰...\")\n",
    "\n",
    "premise_errors = {\n",
    "    'err_p_i': [],\n",
    "    'err_g_i_noisy': [],\n",
    "    'err_g_i_clean': [],\n",
    "    'err_p_reconstructed': [],\n",
    "    'err_s_output': [],\n",
    "}\n",
    "\n",
    "for data in tqdm(premise_self_recon_data, desc=\"Premise self-reconstruction\"):\n",
    "    s_input = data['s_input']\n",
    "    true_p = data['true_p']\n",
    "    true_g = data['true_g']\n",
    "    \n",
    "    try:\n",
    "        s_output, stats = self_reconstruction(\n",
    "            s_input, Wps, Wsp, Wgp, Wpg, \n",
    "            module_gbooks, module_sizes, thresh=thresh\n",
    "        )\n",
    "        \n",
    "        # è®¡ç®—å„æ­¥è¯¯å·®\n",
    "        err_p_i = np.sum(np.abs(stats['p_i'] - true_p)) / (2 * Np)\n",
    "        err_g_i_noisy = np.sum(np.abs(stats['g_i_noisy'] - true_g)) / (2 * Ng)\n",
    "        err_g_i_clean = np.sum(np.abs(stats['g_i_clean'] - true_g)) / (2 * Ng)\n",
    "        err_p_reconstructed = np.sum(np.abs(stats['p_i_reconstructed'] - true_p)) / (2 * Np)\n",
    "        err_s_output = np.sum(np.abs(stats['s_output'] - s_input)) / (2 * Ns)  # é‡å»ºå›åŸè¾“å…¥\n",
    "        \n",
    "        premise_errors['err_p_i'].append(err_p_i)\n",
    "        premise_errors['err_g_i_noisy'].append(err_g_i_noisy)\n",
    "        premise_errors['err_g_i_clean'].append(err_g_i_clean)\n",
    "        premise_errors['err_p_reconstructed'].append(err_p_reconstructed)\n",
    "        premise_errors['err_s_output'].append(err_s_output)\n",
    "        \n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f\"   Successfully processed {len(premise_errors['err_p_i'])} premise samples\")\n",
    "\n",
    "\n",
    "# ===== å¯¹ Centroid è¿›è¡Œè‡ªé‡å»ºåˆ†æ =====\n",
    "# test_mode: \"gt\" = ä½¿ç”¨ ground truth g_i^cleanï¼Œ\"org\" = ä½¿ç”¨ module_wise_NN_2d æ¸…æ´—\n",
    "test_mode = \"org\"  # å¯é€‰: \"gt\" æˆ– \"org\"\n",
    "\n",
    "if test_mode == \"gt\":\n",
    "    print(\"\\n3. å¯¹ Centroid è¿›è¡Œè‡ªé‡å»ºåˆ†æï¼ˆè®­ç»ƒæ¬¡æ•°=clusterå¤§å°ï¼Œg_i^clean ä½¿ç”¨ ground truthï¼‰...\")\n",
    "else:\n",
    "    print(\"\\n3. å¯¹ Centroid è¿›è¡Œè‡ªé‡å»ºåˆ†æï¼ˆè®­ç»ƒæ¬¡æ•°=clusterå¤§å°ï¼Œg_i^clean ä½¿ç”¨ module_wise_NN_2dï¼‰...\")\n",
    "\n",
    "centroid_errors = {\n",
    "    'err_p_i': [],\n",
    "    'err_g_i_noisy': [],\n",
    "    'err_g_i_clean': [],\n",
    "    'err_p_reconstructed': [],\n",
    "    'err_s_output': [],\n",
    "    'training_count': [],  # è®°å½•è®­ç»ƒæ¬¡æ•°\n",
    "}\n",
    "\n",
    "for data in tqdm(centroid_self_recon_data, desc=\"Centroid self-reconstruction\"):\n",
    "    s_input = data['s_input']\n",
    "    true_p = data['true_p']\n",
    "    true_g = data['true_g']  # ground truth g_i\n",
    "    training_count = data['training_count']\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Sensory -> Place cell\n",
    "        s = normalize_vec(s_input)\n",
    "        p_i = sensory_to_place(s, Wps)\n",
    "        \n",
    "        # Step 2: Place cell -> Grid cell (noisy)\n",
    "        g_i_noisy = place_to_grid_noisy(p_i, Wgp)\n",
    "        \n",
    "        # Step 3: Grid cell æ¸…æ´—\n",
    "        if test_mode == \"gt\":\n",
    "            # ä½¿ç”¨ ground truthï¼Œé¿å… module_wise_NN_2d æ‹‰å›é”™è¯¯å¸å¼•å­\n",
    "            g_i_clean = true_g\n",
    "            err_g_i_clean = 0.0\n",
    "        else:\n",
    "            # ä½¿ç”¨åŸæœ¬çš„ module_wise_NN_2d æ¸…æ´—\n",
    "            gin = g_i_noisy.reshape(1, -1, 1)\n",
    "            g_i_clean = module_wise_NN_2d(gin, module_gbooks, module_sizes)\n",
    "            g_i_clean = g_i_clean[0, :, 0]\n",
    "            err_g_i_clean = np.sum(np.abs(g_i_clean - true_g)) / (2 * Ng)\n",
    "        \n",
    "        # Step 4: Grid cell -> Place cell\n",
    "        p_i_reconstructed = grid_to_place_with_nonlin(g_i_clean, Wpg, thresh=thresh)\n",
    "        \n",
    "        # Step 5: Place cell -> Sensory\n",
    "        s_output = place_to_sensory(p_i_reconstructed, Wsp)\n",
    "        \n",
    "        # è®¡ç®—å„æ­¥è¯¯å·®\n",
    "        err_p_i = np.sum(np.abs(p_i - true_p)) / (2 * Np)\n",
    "        err_g_i_noisy = np.sum(np.abs(g_i_noisy - true_g)) / (2 * Ng)\n",
    "        err_p_reconstructed = np.sum(np.abs(p_i_reconstructed - true_p)) / (2 * Np)\n",
    "        err_s_output = np.sum(np.abs(s_output - s_input)) / (2 * Ns)  # é‡å»ºå›åŸè¾“å…¥\n",
    "        \n",
    "        centroid_errors['err_p_i'].append(err_p_i)\n",
    "        centroid_errors['err_g_i_noisy'].append(err_g_i_noisy)\n",
    "        centroid_errors['err_g_i_clean'].append(err_g_i_clean)\n",
    "        centroid_errors['err_p_reconstructed'].append(err_p_reconstructed)\n",
    "        centroid_errors['err_s_output'].append(err_s_output)\n",
    "        centroid_errors['training_count'].append(training_count)\n",
    "        \n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f\"   Successfully processed {len(centroid_errors['err_p_i'])} centroid samples\")\n",
    "print(f\"   Test mode: {test_mode}\")\n",
    "\n",
    "\n",
    "# ===== æ‰“å°ç»Ÿè®¡ç»“æœ =====\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Self-Reconstruction Error Statistics (test_mode={test_mode})\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "step_names = [\n",
    "    ('err_p_i',           r's_i â†’ p_i'),\n",
    "    ('err_g_i_noisy',     r'p_i â†’ g_i^noisy'),\n",
    "    ('err_g_i_clean',     r'g_i^noisy â†’ g_i^clean'),\n",
    "    ('err_p_reconstructed', r'g_i^clean â†’ p_i'),\n",
    "    ('err_s_output',      r'p_i â†’ s_i'),\n",
    "]\n",
    "\n",
    "print(\"\\n--- Premise (è®­ç»ƒ100æ¬¡) ---\")\n",
    "print(f\"{'Step':<40} {'Mean Err':>12} {'Std':>12} {'Min':>12} {'Max':>12}\")\n",
    "print(\"-\"*90)\n",
    "for key, name in step_names:\n",
    "    errs = np.array(premise_errors[key])\n",
    "    print(f\"{name:<40} {errs.mean():>12.4f} {errs.std():>12.4f} {errs.min():>12.4f} {errs.max():>12.4f}\")\n",
    "\n",
    "print(f\"\\n--- Centroid (è®­ç»ƒæ¬¡æ•°=clusterå¤§å°, mode={test_mode}) ---\")\n",
    "print(f\"{'Step':<40} {'Mean Err':>12} {'Std':>12} {'Min':>12} {'Max':>12}\")\n",
    "print(\"-\"*90)\n",
    "for key, name in step_names:\n",
    "    errs = np.array(centroid_errors[key])\n",
    "    print(f\"{name:<40} {errs.mean():>12.4f} {errs.std():>12.4f} {errs.min():>12.4f} {errs.max():>12.4f}\")\n",
    "\n",
    "# è®­ç»ƒæ¬¡æ•°åˆ†å¸ƒ\n",
    "training_counts = np.array(centroid_errors['training_count'])\n",
    "print(f\"\\n--- Centroid è®­ç»ƒæ¬¡æ•°åˆ†å¸ƒ ---\")\n",
    "print(f\"  Mean: {training_counts.mean():.2f}\")\n",
    "print(f\"  Std:  {training_counts.std():.2f}\")\n",
    "print(f\"  Min:  {training_counts.min()}\")\n",
    "print(f\"  Max:  {training_counts.max()}\")\n",
    "print(f\"  Median: {np.median(training_counts):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0fc7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 9.2: å¯è§†åŒ– - Premise å’Œ Centroid å„æ­¥é‡å»ºç‡ =====\n",
    "\"\"\"\n",
    "ç»˜åˆ¶ä¸¤å¼ å›¾ï¼š\n",
    "1. Premise å„æ­¥é‡å»ºç‡ï¼ˆè®­ç»ƒ100æ¬¡ï¼‰\n",
    "2. Centroid å„æ­¥é‡å»ºç‡ï¼ˆè®­ç»ƒæ¬¡æ•°=clusterå¤§å°ï¼‰\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Set global font to Times New Roman\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['mathtext.fontset'] = 'custom'\n",
    "plt.rcParams['mathtext.rm'] = 'Times New Roman'\n",
    "plt.rcParams['mathtext.it'] = 'Times New Roman:italic'\n",
    "plt.rcParams['mathtext.bf'] = 'Times New Roman:bold'\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 11\n",
    "plt.rcParams['ytick.labelsize'] = 11\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "# ===== Figure 1: Premise Self-Reconstruction =====\n",
    "fig1, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Define step labels with LaTeX formatting for self-reconstruction\n",
    "step_labels_self = [\n",
    "    r'$s_i \\rightarrow p_i$',\n",
    "    r'$p_i \\rightarrow g_i^{\\mathrm{noisy}}$',\n",
    "    r'$g_i^{\\mathrm{noisy}} \\rightarrow g_i^{\\mathrm{clean}}$',\n",
    "    r'$g_i^{\\mathrm{clean}} \\rightarrow p_i$',\n",
    "    r'$p_i \\rightarrow s_i$'\n",
    "]\n",
    "\n",
    "# Mean errors for each step (Premise)\n",
    "premise_mean_errors = [\n",
    "    np.mean(premise_errors['err_p_i']),\n",
    "    np.mean(premise_errors['err_g_i_noisy']),\n",
    "    np.mean(premise_errors['err_g_i_clean']),\n",
    "    np.mean(premise_errors['err_p_reconstructed']),\n",
    "    np.mean(premise_errors['err_s_output'])\n",
    "]\n",
    "\n",
    "# Color gradient (blue theme for Premise)\n",
    "colors_premise = ['#1a237e', '#303f9f', '#3f51b5', '#7986cb', '#c5cae9']\n",
    "\n",
    "# Create bar plot\n",
    "x_pos = np.arange(len(step_labels_self))\n",
    "bars = ax1.bar(x_pos, premise_mean_errors, color=colors_premise, edgecolor='white', linewidth=2, alpha=0.9)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for bar, val in zip(bars, premise_mean_errors):\n",
    "    height = bar.get_height()\n",
    "    if height > 0.001:\n",
    "        ax1.annotate(f'{val:.4f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 5),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    else:\n",
    "        ax1.annotate(f'{val:.6f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, 0.002),\n",
    "                    xytext=(0, 5),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10, color='gray')\n",
    "\n",
    "# Styling\n",
    "ax1.set_xlabel('Self-Reconstruction Step', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('Mean Reconstruction Error', fontsize=13, fontweight='bold')\n",
    "ax1.set_title('Premise Self-Reconstruction Error (Training: 100 times)', fontsize=15, fontweight='bold', pad=15)\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(step_labels_self, fontsize=12)\n",
    "ax1.set_ylim(0, max(premise_mean_errors) * 1.3)\n",
    "\n",
    "# Add grid\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--', zorder=0)\n",
    "ax1.set_axisbelow(True)\n",
    "\n",
    "# Remove top and right spines\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "# Add annotation about error definition\n",
    "ax1.annotate(r'Error: $\\frac{\\sum |predicted - true|}{2N}$', \n",
    "            xy=(0.98, 0.95), xycoords='axes fraction',\n",
    "            ha='right', va='top', fontsize=11,\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='whitesmoke', edgecolor='gray', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'premise_self_reconstruction.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Figure 1: Premise Self-Reconstruction saved\")\n",
    "\n",
    "\n",
    "# ===== Figure 2: Centroid Self-Reconstruction =====\n",
    "fig2, ax2 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Mean errors for each step (Centroid)\n",
    "centroid_mean_errors = [\n",
    "    np.mean(centroid_errors['err_p_i']),\n",
    "    np.mean(centroid_errors['err_g_i_noisy']),\n",
    "    np.mean(centroid_errors['err_g_i_clean']),\n",
    "    np.mean(centroid_errors['err_p_reconstructed']),\n",
    "    np.mean(centroid_errors['err_s_output'])\n",
    "]\n",
    "\n",
    "# Color gradient (orange theme for Centroid)\n",
    "colors_centroid = ['#e65100', '#f57c00', '#ff9800', '#ffb74d', '#ffe0b2']\n",
    "\n",
    "# Create bar plot\n",
    "bars = ax2.bar(x_pos, centroid_mean_errors, color=colors_centroid, edgecolor='white', linewidth=2, alpha=0.9)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for bar, val in zip(bars, centroid_mean_errors):\n",
    "    height = bar.get_height()\n",
    "    if height > 0.001:\n",
    "        ax2.annotate(f'{val:.4f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 5),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    else:\n",
    "        ax2.annotate(f'{val:.6f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, 0.002),\n",
    "                    xytext=(0, 5),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10, color='gray')\n",
    "\n",
    "# Calculate mean training count\n",
    "mean_training_count = np.mean(centroid_errors['training_count'])\n",
    "\n",
    "# Styling\n",
    "ax2.set_xlabel('Self-Reconstruction Step', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('Mean Reconstruction Error', fontsize=13, fontweight='bold')\n",
    "ax2.set_title(f'Centroid Self-Reconstruction Error (Mean Training: {mean_training_count:.1f} times)', \n",
    "              fontsize=15, fontweight='bold', pad=15)\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(step_labels_self, fontsize=12)\n",
    "ax2.set_ylim(0, max(centroid_mean_errors) * 1.3)\n",
    "\n",
    "# Add grid\n",
    "ax2.grid(axis='y', alpha=0.3, linestyle='--', zorder=0)\n",
    "ax2.set_axisbelow(True)\n",
    "\n",
    "# Remove top and right spines\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "# Add annotation about error definition\n",
    "ax2.annotate(r'Error: $\\frac{\\sum |predicted - true|}{2N}$', \n",
    "            xy=(0.98, 0.95), xycoords='axes fraction',\n",
    "            ha='right', va='top', fontsize=11,\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='whitesmoke', edgecolor='gray', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'centroid_self_reconstruction.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Figure 2: Centroid Self-Reconstruction saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823378ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 9.3: å¯è§†åŒ– - é‡å»ºç‡ä¸è®­ç»ƒæ¬¡æ•°çš„å…³ç³» =====\n",
    "\"\"\"\n",
    "ç»˜åˆ¶ Centroid å„æ­¥é‡å»ºç‡éšè®­ç»ƒæ¬¡æ•°ï¼ˆclusterå¤§å°ï¼‰çš„å˜åŒ–æ›²çº¿\n",
    "åªå…³æ³¨ s-p, p-g, p-s ä¸‰æ­¥\n",
    "\n",
    "åˆ†æé‡å»ºç‡æ˜¯å¦ä¸è®­ç»ƒæ¬¡æ•°ç›¸å…³\n",
    "\"\"\"\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# ===== å‡†å¤‡æ•°æ®ï¼šæŒ‰è®­ç»ƒæ¬¡æ•°åˆ†ç»„ =====\n",
    "training_counts = np.array(centroid_errors['training_count'])\n",
    "unique_counts = np.unique(training_counts)\n",
    "\n",
    "# è®¡ç®—æ¯ä¸ªè®­ç»ƒæ¬¡æ•°å¯¹åº”çš„å¹³å‡è¯¯å·®ï¼ˆåªå…³æ³¨3ä¸ªæ­¥éª¤ï¼‰\n",
    "count_to_errors = {step: {} for step in ['err_p_i', 'err_g_i_noisy', 'err_s_output']}\n",
    "\n",
    "for step_key in count_to_errors.keys():\n",
    "    step_errs = np.array(centroid_errors[step_key])\n",
    "    for count in unique_counts:\n",
    "        mask = training_counts == count\n",
    "        count_to_errors[step_key][count] = step_errs[mask].mean()\n",
    "\n",
    "# ===== Figure 3: é‡å»ºç‡éšè®­ç»ƒæ¬¡æ•°çš„å˜åŒ– (åªå…³æ³¨ s-p, p-g, p-s) =====\n",
    "fig3, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "step_keys = ['err_p_i', 'err_g_i_noisy', 'err_s_output']\n",
    "step_titles = [\n",
    "    r'$s_i \\rightarrow p_i$',\n",
    "    r'$p_i \\rightarrow g_i^{\\mathrm{noisy}}$',\n",
    "    r'$p_i \\rightarrow s_i$'\n",
    "]\n",
    "\n",
    "colors_scatter = ['#1976d2', '#388e3c', '#7b1fa2']\n",
    "\n",
    "for i, (step_key, step_title) in enumerate(zip(step_keys, step_titles)):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # è·å–æ•°æ®\n",
    "    step_errs = np.array(centroid_errors[step_key])\n",
    "    \n",
    "    # æ•£ç‚¹å›¾\n",
    "    ax.scatter(training_counts, step_errs, alpha=0.4, s=30, c=colors_scatter[i], edgecolors='white', linewidth=0.5)\n",
    "    \n",
    "    # æŒ‰è®­ç»ƒæ¬¡æ•°åˆ†ç»„è®¡ç®—å‡å€¼\n",
    "    sorted_counts = sorted(unique_counts)\n",
    "    mean_errs = [count_to_errors[step_key][c] for c in sorted_counts]\n",
    "    \n",
    "    # ç»˜åˆ¶è¶‹åŠ¿çº¿ï¼ˆå‡å€¼æŠ˜çº¿ï¼‰\n",
    "    ax.plot(sorted_counts, mean_errs, 'o-', color='black', linewidth=2, markersize=6, \n",
    "            label='Mean per count', zorder=5)\n",
    "    \n",
    "    # è®¡ç®—ç›¸å…³ç³»æ•°\n",
    "    correlation, p_value = stats.pearsonr(training_counts, step_errs)\n",
    "    \n",
    "    # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜\n",
    "    ax.set_xlabel('Training Count (Cluster Size)', fontsize=11)\n",
    "    ax.set_ylabel('Reconstruction Error', fontsize=11)\n",
    "    ax.set_title(step_title, fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # æ·»åŠ ç›¸å…³ç³»æ•°æ ‡æ³¨\n",
    "    ax.annotate(f'Pearson r = {correlation:.4f}\\np-value = {p_value:.2e}', \n",
    "                xy=(0.95, 0.95), xycoords='axes fraction',\n",
    "                ha='right', va='top', fontsize=10,\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow', edgecolor='gray', alpha=0.8))\n",
    "    \n",
    "    # Grid\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# æ•´ä½“æ ‡é¢˜\n",
    "fig3.suptitle('Centroid Self-Reconstruction Error vs Training Count (Cluster Size)', \n",
    "              fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'reconstruction_vs_training_count.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Figure 3: Reconstruction Error vs Training Count saved\")\n",
    "\n",
    "\n",
    "# ===== æ±‡æ€»ç›¸å…³æ€§åˆ†æ =====\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Correlation Analysis: Reconstruction Error vs Training Count\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Step':<45} {'Pearson r':>12} {'p-value':>15} {'Significant':>15}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "for step_key, step_title in zip(step_keys, step_titles):\n",
    "    step_errs = np.array(centroid_errors[step_key])\n",
    "    corr, pval = stats.pearsonr(training_counts, step_errs)\n",
    "    significant = \"Yes (p<0.05)\" if pval < 0.05 else \"No\"\n",
    "    print(f\"{step_title:<45} {corr:>12.4f} {pval:>15.2e} {significant:>15}\")\n",
    "\n",
    "# æœ€ç»ˆé‡å»ºè¯¯å·® (s_output) çš„è¯¦ç»†åˆ†æ\n",
    "final_errs = np.array(centroid_errors['err_s_output'])\n",
    "print(f\"\\n--- æœ€ç»ˆé‡å»ºè¯¯å·® (s_i â†’ s_i) ä¸è®­ç»ƒæ¬¡æ•°çš„å…³ç³» ---\")\n",
    "print(f\"  Pearson correlation: {stats.pearsonr(training_counts, final_errs)[0]:.4f}\")\n",
    "print(f\"  Spearman correlation: {stats.spearmanr(training_counts, final_errs)[0]:.4f}\")\n",
    "\n",
    "# æŒ‰è®­ç»ƒæ¬¡æ•°åˆ†ç»„çš„ç»Ÿè®¡\n",
    "print(f\"\\n--- æŒ‰è®­ç»ƒæ¬¡æ•°åˆ†ç»„çš„æœ€ç»ˆé‡å»ºè¯¯å·® ---\")\n",
    "bins = [1, 5, 10, 20, 50, 100, np.inf]\n",
    "bin_labels = ['1-4', '5-9', '10-19', '20-49', '50-99', '100+']\n",
    "for i in range(len(bins)-1):\n",
    "    mask = (training_counts >= bins[i]) & (training_counts < bins[i+1])\n",
    "    if mask.sum() > 0:\n",
    "        mean_err = final_errs[mask].mean()\n",
    "        std_err = final_errs[mask].std()\n",
    "        count = mask.sum()\n",
    "        print(f\"  Count {bin_labels[i]:>6}: Mean={mean_err:.4f}, Std={std_err:.4f}, N={count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feaab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 9.4: å¯¹æ¯”å›¾ - Premise vs Centroid é‡å»ºç‡ =====\n",
    "\"\"\"\n",
    "ç»˜åˆ¶å¯¹æ¯”å›¾ï¼šPremise vs Centroid å„æ­¥é‡å»ºç‡\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "import scipy.stats as scipy_stats\n",
    "\n",
    "# ===== Figure 4: Premise vs Centroid Comparison =====\n",
    "fig4, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Define step labels\n",
    "step_labels_self = [\n",
    "    r'$s_i \\rightarrow p_i$',\n",
    "    r'$p_i \\rightarrow g_i^{\\mathrm{noisy}}$',\n",
    "    r'$g_i^{\\mathrm{noisy}} \\rightarrow g_i^{\\mathrm{clean}}$',\n",
    "    r'$g_i^{\\mathrm{clean}} \\rightarrow p_i$',\n",
    "    r'$p_i \\rightarrow s_i$'\n",
    "]\n",
    "\n",
    "# Mean errors\n",
    "premise_mean_errors = [\n",
    "    np.mean(premise_errors['err_p_i']),\n",
    "    np.mean(premise_errors['err_g_i_noisy']),\n",
    "    np.mean(premise_errors['err_g_i_clean']),\n",
    "    np.mean(premise_errors['err_p_reconstructed']),\n",
    "    np.mean(premise_errors['err_s_output'])\n",
    "]\n",
    "\n",
    "centroid_mean_errors = [\n",
    "    np.mean(centroid_errors['err_p_i']),\n",
    "    np.mean(centroid_errors['err_g_i_noisy']),\n",
    "    np.mean(centroid_errors['err_g_i_clean']),\n",
    "    np.mean(centroid_errors['err_p_reconstructed']),\n",
    "    np.mean(centroid_errors['err_s_output'])\n",
    "]\n",
    "\n",
    "# Bar positions\n",
    "x_pos = np.arange(len(step_labels_self))\n",
    "width = 0.35\n",
    "\n",
    "# Create grouped bar chart\n",
    "bars1 = ax.bar(x_pos - width/2, premise_mean_errors, width, label='Premise (100 training)', \n",
    "               color='#1976d2', edgecolor='white', linewidth=1.5, alpha=0.9)\n",
    "bars2 = ax.bar(x_pos + width/2, centroid_mean_errors, width, label=f'Centroid (mean {mean_training_count:.1f} training)', \n",
    "               color='#ff7043', edgecolor='white', linewidth=1.5, alpha=0.9)\n",
    "\n",
    "# Add value labels\n",
    "def add_labels(bars, vals, offset_y=5):\n",
    "    for bar, val in zip(bars, vals):\n",
    "        height = bar.get_height()\n",
    "        if height > 0.001:\n",
    "            ax.annotate(f'{val:.4f}',\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                        xytext=(0, offset_y),\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "add_labels(bars1, premise_mean_errors)\n",
    "add_labels(bars2, centroid_mean_errors)\n",
    "\n",
    "# Styling\n",
    "ax.set_xlabel('Self-Reconstruction Step', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Mean Reconstruction Error', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Premise vs Centroid: Self-Reconstruction Error Comparison', fontsize=15, fontweight='bold', pad=15)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(step_labels_self, fontsize=11)\n",
    "ax.set_ylim(0, max(max(premise_mean_errors), max(centroid_mean_errors)) * 1.35)\n",
    "\n",
    "ax.legend(loc='upper right', fontsize=11, framealpha=0.9)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--', zorder=0)\n",
    "ax.set_axisbelow(True)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add annotation\n",
    "ax.annotate(r'Error: $\\frac{\\sum |predicted - true|}{2N}$', \n",
    "            xy=(0.02, 0.95), xycoords='axes fraction',\n",
    "            ha='left', va='top', fontsize=11,\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='whitesmoke', edgecolor='gray', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'premise_vs_centroid_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Figure 4: Premise vs Centroid Comparison saved\")\n",
    "\n",
    "\n",
    "# ===== ä¿å­˜åˆ†æç»“æœ =====\n",
    "self_recon_results = {\n",
    "    'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'premise_analysis': {\n",
    "        'training_count': 100,\n",
    "        'sample_count': len(premise_errors['err_p_i']),\n",
    "        'step_errors': {\n",
    "            's_i â†’ p_i': float(np.mean(premise_errors['err_p_i'])),\n",
    "            'p_i â†’ g_i^noisy': float(np.mean(premise_errors['err_g_i_noisy'])),\n",
    "            'g_i^noisy â†’ g_i^clean': float(np.mean(premise_errors['err_g_i_clean'])),\n",
    "            'g_i^clean â†’ p_i': float(np.mean(premise_errors['err_p_reconstructed'])),\n",
    "            'p_i â†’ s_i': float(np.mean(premise_errors['err_s_output'])),\n",
    "        }\n",
    "    },\n",
    "    'centroid_analysis': {\n",
    "        'mean_training_count': float(np.mean(centroid_errors['training_count'])),\n",
    "        'sample_count': len(centroid_errors['err_p_i']),\n",
    "        'step_errors': {\n",
    "            's_i â†’ p_i': float(np.mean(centroid_errors['err_p_i'])),\n",
    "            'p_i â†’ g_i^noisy': float(np.mean(centroid_errors['err_g_i_noisy'])),\n",
    "            'g_i^noisy â†’ g_i^clean': float(np.mean(centroid_errors['err_g_i_clean'])),\n",
    "            'g_i^clean â†’ p_i': float(np.mean(centroid_errors['err_p_reconstructed'])),\n",
    "            'p_i â†’ s_i': float(np.mean(centroid_errors['err_s_output'])),\n",
    "        },\n",
    "        'correlation_with_training_count': {\n",
    "            step_title: {\n",
    "                'pearson_r': float(scipy_stats.pearsonr(training_counts, np.array(centroid_errors[step_key]))[0]),\n",
    "                'p_value': float(scipy_stats.pearsonr(training_counts, np.array(centroid_errors[step_key]))[1])\n",
    "            }\n",
    "            for step_key, step_title in zip(step_keys, step_titles)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "self_recon_path = os.path.join(RESULTS_DIR, 'self_reconstruction_analysis.json')\n",
    "with open(self_recon_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(self_recon_results, f, ensure_ascii=False, indent=2)\n",
    "print(f\"\\nâœ… Self-reconstruction analysis saved to: {self_recon_path}\")\n",
    "\n",
    "\n",
    "# ===== æœ€ç»ˆæ€»ç»“ =====\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SELF-RECONSTRUCTION ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "æµç¨‹: s_i â†’ p_i â†’ g_i^noisy â†’ g_i^clean â†’ p_i â†’ s_i (æ— è·¯å¾„ç§¯åˆ†)\n",
    "\n",
    "1. Premise (è®­ç»ƒ100æ¬¡):\n",
    "   - æ ·æœ¬æ•°: {len(premise_errors['err_p_i'])}\n",
    "   - æœ€ç»ˆé‡å»ºè¯¯å·® (p_i â†’ s_i): {np.mean(premise_errors['err_s_output']):.4f}\n",
    "\n",
    "2. Centroid (å¹³å‡è®­ç»ƒ{np.mean(centroid_errors['training_count']):.1f}æ¬¡):\n",
    "   - æ ·æœ¬æ•°: {len(centroid_errors['err_p_i'])}\n",
    "   - æœ€ç»ˆé‡å»ºè¯¯å·® (p_i â†’ s_i): {np.mean(centroid_errors['err_s_output']):.4f}\n",
    "\n",
    "3. è¯¯å·®å·®å¼‚:\n",
    "   - Premise æœ€ç»ˆè¯¯å·®: {np.mean(premise_errors['err_s_output']):.4f}\n",
    "   - Centroid æœ€ç»ˆè¯¯å·®: {np.mean(centroid_errors['err_s_output']):.4f}\n",
    "   - å·®å€¼ (Centroid - Premise): {np.mean(centroid_errors['err_s_output']) - np.mean(premise_errors['err_s_output']):.4f}\n",
    "\n",
    "4. è®­ç»ƒæ¬¡æ•°ä¸é‡å»ºè¯¯å·®çš„ç›¸å…³æ€§:\n",
    "   - æœ€ç»ˆè¯¯å·® (s_i â†’ s_i) çš„ Pearson r: {scipy_stats.pearsonr(training_counts, np.array(centroid_errors['err_s_output']))[0]:.4f}\n",
    "\n",
    "   - p-value: {scipy_stats.pearsonr(training_counts, np.array(centroid_errors['err_s_output']))[1]:.2e}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec79f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvembed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
