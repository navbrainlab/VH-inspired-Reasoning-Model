{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ff02ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\VH_analogy\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277950c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型: google/gemma-2-9b-it ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:44<00:00, 11.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 模型已以 BF16 加载 \n",
      "正在加载数据集...\n",
      "总数据量: 1320 条\n",
      "跳过已处理: 0 条\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma 96GB High-Res Extract: 100%|██████████| 1320/1320 [1:21:30<00:00,  3.71s/it]\n"
     ]
    }
   ],
   "source": [
    "# ================= 配置区域 =================\n",
    "MODEL_ID = \"google/gemma-2-9b-it\"\n",
    "OUTPUT_FILE = \"ekar_gemma_extraction.jsonl\"\n",
    "\n",
    "# ================= 模型初始化 (针对 96GB 显存优化) =================\n",
    "print(f\"正在加载模型: {MODEL_ID} ...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"cuda\",  # 如果是单张大卡用 \"cuda\"，如果是多卡且想自动分配用 \"auto\"\n",
    "    torch_dtype=torch.bfloat16, \n",
    ")\n",
    "print(\">>> 模型已以 BF16 加载 \")\n",
    "\n",
    "\n",
    "# ================= Prompt 构造函数 (保持不变) =================\n",
    "def construct_prompt(item):\n",
    "    question = item['question']\n",
    "    choices = item['choices'] \n",
    "    answer_key = item['answerKey']\n",
    "    \n",
    "    # 处理解析\n",
    "    raw_explanation = item.get('explanation', [])\n",
    "    if isinstance(raw_explanation, list):\n",
    "        explanation_str = \"\\n\".join(raw_explanation)\n",
    "    else:\n",
    "        explanation_str = str(raw_explanation)\n",
    "\n",
    "    # 提取选项\n",
    "    choice_texts = choices['text']\n",
    "    correct_idx = ord(answer_key) - ord('A')\n",
    "    correct_choice_text = choice_texts[correct_idx] if 0 <= correct_idx < 4 else \"未知\"\n",
    "\n",
    "    # Prompt 内容\n",
    "    prompt_content = f\"\"\"以下是一个处理范例：\n",
    "输入：\n",
    "题目词对：生病:医药\n",
    "选项：A.空虚：信仰 B.糊涂：明白 C.雷雨：大风 D.难过：高兴\n",
    "正确答案：A\n",
    "参考解析：\n",
    "[\n",
    "\"改善“生病”的状况可以用“医药”。\",\n",
    "\"改善“空虚”的状况可以借助“信仰”。\",\n",
    "\"“糊涂”和“明白”的意思相反，“明白”不能改善“糊涂”的状况。\",\n",
    "\"“雷雨”和“大风”都属于自然现象，“雷雨”的状况不可以由“大风”改善。\",\n",
    "\"“难过”和“高兴”的意思相反，“难过”的状况不一定可以由“高兴”改善。\"\n",
    "]\n",
    "输出：\n",
    "{{\n",
    "\"question_logic\": \"医药是用来治疗生病状态的事物\",\n",
    "\"answer_logic\": \"信仰是用来填补空虚状态的事物\",\n",
    "\"abstract_relation\": \"<词2>是用来改善或解决<词1>所代表的负面状态的手段/事物\"\n",
    "}}\n",
    "\n",
    "请分析以下数据并提取关系：\n",
    "\n",
    "**题目词对**: {question}\n",
    "**选项**:\n",
    "A: {choice_texts[0]}\n",
    "B: {choice_texts[1]}\n",
    "C: {choice_texts[2]}\n",
    "D: {choice_texts[3]}\n",
    "\n",
    "**正确答案**: {answer_key} (即选项 {correct_choice_text})\n",
    "**参考解析**: \n",
    "{explanation_str}\n",
    "\n",
    "### 思考步骤\n",
    "\n",
    "1. 分析【题目词对】的逻辑关系。\n",
    "2. 分析【正确选项】的逻辑关系，寻找它与题目词对的共同点（这就是最佳颗粒度）。\n",
    "3. 检查这个共同关系是否适用于错误选项。如果适用，说明颗粒度太粗，需要增加限定条件；如果不适用，则保留。\n",
    "\n",
    "### 输出格式\n",
    "\n",
    "请直接输出一个 JSON 对象，不要包含其他废话，格式如下：\n",
    "{{\n",
    "\"question_logic\": \"简述题目词对的具体关系\",\n",
    "\"answer_logic\": \"简述正确答案的具体关系\",\n",
    "\"abstract_relation\": \"使用<词1>和<词2>生成的标准化关系模板\"\n",
    "}}\"\"\"\n",
    "    return prompt_content\n",
    "\n",
    "# ================= 推理函数 =================\n",
    "def generate_response(prompt_text):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        return_tensors=\"pt\", \n",
    "        add_generation_prompt=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
    "    ]\n",
    "\n",
    "    # Gemma-2 在 BF16 下性能极佳\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False, # 贪婪解码，保证逻辑提取的一致性\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def process_dataset():\n",
    "    print(\"正在加载数据集...\")\n",
    "    try:\n",
    "        dataset = load_dataset(\"jiangjiechen/ekar_chinese\")\n",
    "    except Exception as e:\n",
    "        print(f\"数据集加载失败: {e}\")\n",
    "        return\n",
    "\n",
    "    all_data = []\n",
    "    for split in ['train', 'validation']:\n",
    "        for item in dataset[split]:\n",
    "            all_data.append(item)\n",
    "    \n",
    "    print(f\"总数据量: {len(all_data)} 条\")\n",
    "\n",
    "    # 断点续传\n",
    "    processed_ids = set()\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    if 'id' in record: \n",
    "                        processed_ids.add(record['id'])\n",
    "                except: \n",
    "                    pass\n",
    "    print(f\"跳过已处理: {len(processed_ids)} 条\")\n",
    "\n",
    "    # 主循环\n",
    "    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f_out:\n",
    "        for item in tqdm(all_data, desc=\"Gemma 96GB High-Res Extract\"):\n",
    "            item_id = item['id']\n",
    "            if item_id in processed_ids:\n",
    "                continue\n",
    "\n",
    "            prompt = construct_prompt(item)\n",
    "            \n",
    "            try:\n",
    "                response_text = generate_response(prompt)\n",
    "                \n",
    "                # 清洗 JSON\n",
    "                clean_json_str = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "                try:\n",
    "                    model_output = json.loads(clean_json_str)\n",
    "                except json.JSONDecodeError:\n",
    "                    model_output = {\"error\": \"JSON_DECODE_FAIL\", \"raw_content\": response_text}\n",
    "\n",
    "                final_record = {\n",
    "                    \"id\": item_id,\n",
    "                    \"original_question\": item['question'],\n",
    "                    \"model_extraction\": model_output\n",
    "                }\n",
    "                \n",
    "                f_out.write(json.dumps(final_record, ensure_ascii=False) + \"\\n\")\n",
    "                f_out.flush()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\n[Fail] ID {item_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cde8f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已成功导出 1319 条关系描述到 relations_for_analysis.txt\n",
      "现在你可以把这个文件直接传给 Google AI Studio 了。\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILE = \"ekar_gemma_extraction.jsonl\"\n",
    "OUTPUT_FILE = \"relations_for_analysis.txt\"\n",
    "\n",
    "def export_relations():\n",
    "    count = 0\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f_out:\n",
    "        with open(INPUT_FILE, 'r', encoding='utf-8') as f_in:\n",
    "            for line in f_in:\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    # 获取模型提取的抽象关系\n",
    "                    # 注意：要兼容一下你之前代码里可能的字段结构\n",
    "                    if 'model_extraction' in record and 'abstract_relation' in record['model_extraction']:\n",
    "                        relation = record['model_extraction']['abstract_relation']\n",
    "                        question = record.get('original_question', '未知题目')\n",
    "                        \n",
    "                        # 格式：[行号] (题目) 关系描述\n",
    "                        f_out.write(f\"[{count+1}] ({question}) {relation}\\n\")\n",
    "                        count += 1\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "    \n",
    "    print(f\"✅ 已成功导出 {count} 条关系描述到 {OUTPUT_FILE}\")\n",
    "    print(\"现在你可以把这个文件直接传给 Google AI Studio 了。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export_relations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888b0063",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_relation_tags = [\n",
    "    # --- 基础语义逻辑 ---\n",
    "    \"近义/同一\",      # Synonyms/Identity (马铃薯:土豆)\n",
    "    \"反义/对立\",      # Antonyms (高:矮) - 注意：一般的反义词放这里\n",
    "    \"包含/种属\",      # Hyponymy (苹果:水果)\n",
    "    \"组成/整体\",      # Meronymy (轮胎:汽车)\n",
    "    \"并列/同类\",      # Coordinate (长江:黄河)\n",
    "    \"属性/特征\",      # Attribute (糖:甜)\n",
    "    \"象征/比喻\",      # Symbolism (鸽子:和平)\n",
    "\n",
    "    # --- 实体交互逻辑 (物理/功能) ---\n",
    "    \"主体-动作\",      # Agent-Action (鸟:飞)\n",
    "    \"主体-产物\",      # Agent-Product (作家:书)\n",
    "    \"材料-成品\",      # Material-Product (木头:桌子)\n",
    "    \"工具-功能\",      # Tool-Function (刀:切)\n",
    "    \"位置/空间\",      # Location (轮船:海)\n",
    "    \"顺序/过程\",      # Sequence (报名:考试)\n",
    "    \"因果/依赖\",      # Causality (缺水:干旱)\n",
    "\n",
    "    # --- 社会/人物逻辑 (从\"社会关系\"拆分) ---\n",
    "    \"亲属关系\",       # Kinship (舅舅:外甥) - 只有血缘/婚姻\n",
    "    \"师生传承\",       # Mentorship (孔子:颜回) - 强调教导\n",
    "    \"职业-对象\",      # Professional-Client (医生:病人) - 强调服务\n",
    "    \"等级/排序\",      # Hierarchy (经理:职员) - 强调地位差\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5659dd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型: google/gemma-2-9b-it ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 模型已以 BF16 加载完成\n",
      "成功读取 1319 条数据。\n",
      "开始推理... (Batch Size: 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/42 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Processing Batches: 100%|██████████| 42/42 [00:46<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "任务完成！结果已保存至: labeled_relations.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ================= 配置区域 =================\n",
    "MODEL_ID = \"google/gemma-2-9b-it\"\n",
    "INPUT_FILE = 'relations_for_analysis.txt'\n",
    "OUTPUT_FILE = 'labeled_relations.csv'\n",
    "# --- 新增：批处理大小 ---\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ================= 模型初始化 =================\n",
    "print(f\"正在加载模型: {MODEL_ID} ...\")\n",
    "\n",
    "# --- 修改点：为 Tokenizer 添加 padding_side 以支持批处理 ---\n",
    "# 对于解码器-only 的模型 (如 Gemma), padding 需要在左侧\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side='left')\n",
    "# 如果分词器没有默认的 pad token, 手动设置为 eos_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "print(\">>> 模型已以 BF16 加载完成\")\n",
    "\n",
    "# ================= 功能函数 =================\n",
    "\n",
    "def parse_line(line):\n",
    "    \"\"\"\n",
    "    解析原始文本行。\n",
    "    格式: [1] (军队:命令) <词2>是用来控制或引导<词1>的行为\n",
    "    \"\"\"\n",
    "    pattern = r\"^\\[(\\d+)\\] \\((.*?)\\) (.*)$\"\n",
    "    match = re.match(pattern, line.strip())\n",
    "\n",
    "    if match:\n",
    "        return {\n",
    "            \"id\": match.group(1),\n",
    "            \"terms\": match.group(2),\n",
    "            \"description\": match.group(3)\n",
    "        }\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- 重大修改：函数重构为批量处理 ---\n",
    "def get_labels_from_gemma_batch(batch_items):\n",
    "    \"\"\"\n",
    "    构造 Prompt 并获取模型对一个批次的分类结果 (中文 Prompt 版本)\n",
    "    \"\"\"\n",
    "    tags_str = \"\\n\".join([f\"- {tag}\" for tag in final_relation_tags])\n",
    "    all_messages = []\n",
    "\n",
    "    # 为批次中的每个项目创建 Prompt\n",
    "    for item in batch_items:\n",
    "        user_content = f\"\"\"你是一位严格的知识图谱数据分类专家。\n",
    "你的任务是根据提供的描述，将词项之间的语义关系归类到预定义的候选列表中。\n",
    "\n",
    "### 候选分类列表：\n",
    "{tags_str}\n",
    "\n",
    "### 待分类数据：\n",
    "- 词项：({item['terms']})\n",
    "- 逻辑描述：{item['description']}\n",
    "\n",
    "### 指令：\n",
    "请分析“逻辑描述”所表达的关系。从“候选分类列表”中选择最恰当的一个类别。\n",
    "请严格仅输出列表中的类别名称。不要输出任何其他文本、标点或解释。\n",
    "\"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "        all_messages.append(messages)\n",
    "\n",
    "    # 使用 Gemma 的标准对话模板，并进行 padding\n",
    "    # tokenizer 可以一次性处理一个批次的对话列表\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        all_messages,\n",
    "        return_tensors=\"pt\",\n",
    "        add_generation_prompt=True,\n",
    "        padding=True, # 启用填充\n",
    "        truncation=True # 启用截断\n",
    "    ).to(model.device)\n",
    "\n",
    "    # 生成配置: 贪婪解码(do_sample=False)以获得最稳定的分类\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=32,  # 标签很短，不需要生成很长\n",
    "        do_sample=False,    # 关闭随机采样\n",
    "        temperature=None,   # 贪婪模式不需要 temp\n",
    "        top_p=None\n",
    "    )\n",
    "\n",
    "    # 解码 (只取生成的新 token)\n",
    "    # --- 修改点：使用 batch_decode ---\n",
    "    input_ids_len = input_ids.shape[1]\n",
    "    generated_tokens = outputs[:, input_ids_len:]\n",
    "    responses = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # 返回一个包含所有原始输出的列表\n",
    "    return [res.strip() for res in responses]\n",
    "\n",
    "\n",
    "def clean_label(raw_output):\n",
    "    \"\"\"\n",
    "    清洗模型输出，去除可能的标点或多余空格\n",
    "    \"\"\"\n",
    "    # 移除可能出现的 \"类别：\" 或 \"Category:\" 前缀\n",
    "    clean_text = raw_output.replace(\"Category:\", \"\").replace(\"类别：\", \"\").strip()\n",
    "\n",
    "    # 移除可能的末尾句号\n",
    "    if clean_text.endswith(\"。\") or clean_text.endswith(\".\"):\n",
    "        clean_text = clean_text[:-1]\n",
    "\n",
    "    # 验证是否在列表中\n",
    "    if clean_text in final_relation_tags:\n",
    "        return clean_text\n",
    "\n",
    "    # 模糊匹配修复\n",
    "    for tag in final_relation_tags:\n",
    "        if tag in clean_text:\n",
    "            return tag\n",
    "\n",
    "    return \"Uncategorized\"\n",
    "\n",
    "# ================= 主程序 =================\n",
    "\n",
    "def main():\n",
    "    # 1. 读取数据\n",
    "    data_items = []\n",
    "    try:\n",
    "        with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if line.strip():\n",
    "                    parsed = parse_line(line)\n",
    "                    if parsed:\n",
    "                        data_items.append(parsed)\n",
    "        print(f\"成功读取 {len(data_items)} 条数据。\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 找不到文件 {INPUT_FILE}\")\n",
    "        return\n",
    "\n",
    "    # 2. 处理并写入\n",
    "    print(f\"开始推理... (Batch Size: {BATCH_SIZE})\")\n",
    "\n",
    "    # 使用 'w' 模式写入 CSV\n",
    "    with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "        fieldnames = ['id', 'terms', 'description', 'label', 'raw_output']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # --- 修改点：按批次循环处理 ---\n",
    "        # 使用 tqdm 显示批次的进度\n",
    "        for i in tqdm(range(0, len(data_items), BATCH_SIZE), desc=\"Processing Batches\"):\n",
    "            # 获取当前批次的数据\n",
    "            batch = data_items[i:i + BATCH_SIZE]\n",
    "            \n",
    "            try:\n",
    "                # 批量获取模型的原始输出\n",
    "                raw_outputs = get_labels_from_gemma_batch(batch)\n",
    "\n",
    "                # 遍历当前批次的结果并写入文件\n",
    "                for item, raw_output in zip(batch, raw_outputs):\n",
    "                    # 清洗标签\n",
    "                    label = clean_label(raw_output)\n",
    "\n",
    "                    # 写入\n",
    "                    writer.writerow({\n",
    "                        'id': item['id'],\n",
    "                        'terms': item['terms'],\n",
    "                        'description': item['description'],\n",
    "                        'label': label,\n",
    "                        'raw_output': raw_output\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"处理批次 {i//BATCH_SIZE + 1} 时出错 (ID 从 {batch[0]['id']} 开始): {e}\")\n",
    "                # 你可以选择在这里为批次中的所有项目写入错误信息，或者跳过\n",
    "                for item in batch:\n",
    "                     writer.writerow({\n",
    "                        'id': item['id'],\n",
    "                        'terms': item['terms'],\n",
    "                        'description': item['description'],\n",
    "                        'label': 'Error',\n",
    "                        'raw_output': str(e)\n",
    "                    })\n",
    "                continue\n",
    "\n",
    "    print(f\"\\n任务完成！结果已保存至: {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c7f0ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'jiangjiechen/ekar_chinese' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已检测到 HF_TOKEN，正在使用身份验证...\n",
      "正在从 Hugging Face 下载 jiangjiechen/ekar_chinese 的 test 集...\n",
      "下载成功！共获取 335 条数据。\n",
      "正在转换并保存到 ekar_test.json ...\n",
      "完成！文件已保存，可以运行提取脚本了。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "\n",
    "def main():\n",
    "    # 1. 加载环境变量\n",
    "    load_dotenv()\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    \n",
    "    if not hf_token:\n",
    "        print(\"警告: 未在 .env 文件或环境变量中找到 HF_TOKEN。\")\n",
    "        print(\"尝试以匿名模式下载（如果数据集是公开的通常没问题）...\")\n",
    "    else:\n",
    "        print(\"已检测到 HF_TOKEN，正在使用身份验证...\")\n",
    "\n",
    "    # 数据集 ID\n",
    "    DATASET_ID = \"jiangjiechen/ekar_chinese\"\n",
    "    OUTPUT_FILE = \"ekar_test.json\"\n",
    "\n",
    "    print(f\"正在从 Hugging Face 下载 {DATASET_ID} 的 test 集...\")\n",
    "\n",
    "    try:\n",
    "        # 2. 加载数据集\n",
    "        # split=\"test\" 指定只下载测试集\n",
    "        # token=hf_token 用于身份验证\n",
    "        dataset = load_dataset(\n",
    "            DATASET_ID, \n",
    "            split=\"test\", \n",
    "            token=hf_token,\n",
    "            trust_remote_code=True # 允许执行数据集仓库中的加载脚本\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"下载失败: {e}\")\n",
    "        print(\"请检查你的 HF_TOKEN 是否正确，以及是否有权限访问该数据集。\")\n",
    "        return\n",
    "\n",
    "    print(f\"下载成功！共获取 {len(dataset)} 条数据。\")\n",
    "\n",
    "    # 3. 转换为 Python 列表并保存为 JSON\n",
    "    # 这步是为了配合你之前的脚本，将其保存为标准的 JSON 数组格式\n",
    "    print(f\"正在转换并保存到 {OUTPUT_FILE} ...\")\n",
    "    \n",
    "    data_list = []\n",
    "    # 遍历 dataset 对象将其转为普通 dict\n",
    "    for item in dataset:\n",
    "        data_list.append(item)\n",
    "\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data_list, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"完成！文件已保存，可以运行提取脚本了。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b333a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载标签映射...\n",
      "正在加载 EKAR 原始数据...\n",
      "已加载 335 条原始题目数据。\n",
      "正在加载分析结果...\n",
      "正在匹配数据...\n",
      "正在写入 1005 条数据到 output_word_pairs.txt ...\n",
      "处理完成！\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# ================= 配置区域 =================\n",
    "RESULT_FILE = 'analysis_consistency_result.json'  # 分析结果文件\n",
    "LABEL_MAP_FILE = 'label_map.json'                 # 标签映射文件\n",
    "EKAR_FILE = 'ekar_test.json'                      # 原始数据文件（包含选项文本）\n",
    "OUTPUT_FILE = 'output_word_pairs.txt'             # 输出文件\n",
    "# ===========================================\n",
    "\n",
    "def load_json(filename):\n",
    "    \"\"\"读取JSON文件，带错误处理\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"错误: 找不到文件 {filename}\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"错误: 读取 {filename} 失败。详情: {e}\")\n",
    "        return None\n",
    "\n",
    "def split_pair(text):\n",
    "    \"\"\"\n",
    "    将 '词1:词2' 格式的字符串分割为元组。\n",
    "    支持中文冒号和英文冒号。\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return (\"Unknown\", \"Unknown\")\n",
    "    \n",
    "    # 统一替换中文冒号\n",
    "    text = text.replace(\"：\", \":\")\n",
    "    \n",
    "    parts = text.split(\":\")\n",
    "    if len(parts) >= 2:\n",
    "        return (parts[0].strip(), parts[1].strip())\n",
    "    else:\n",
    "        # 如果无法分割，保留原文本作为词1，词2为空\n",
    "        return (text.strip(), \"\")\n",
    "\n",
    "def build_ekar_lookup(ekar_data):\n",
    "    \"\"\"\n",
    "    将ekar列表转换为以id为key的字典，方便查询\n",
    "    返回结构:\n",
    "    {\n",
    "        \"id\": {\n",
    "            \"q_words\": (\"生病\", \"医药\"),\n",
    "            \"opt_words_list\": [(\"词A\", \"词B\"), (\"词C\", \"词D\")...]\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    lookup = {}\n",
    "    # 兼容 ekar_data 可能是 list 或者 dict (lines) 的情况\n",
    "    if isinstance(ekar_data, dict):\n",
    "        # 有些数据集是一行一个json对象的lines格式，这里假设是标准list\n",
    "        pass \n",
    "    \n",
    "    data_list = ekar_data if isinstance(ekar_data, list) else []\n",
    "    \n",
    "    for item in data_list:\n",
    "        data_id = item.get(\"id\")\n",
    "        if not data_id:\n",
    "            continue\n",
    "            \n",
    "        # 1. 提取题干词对\n",
    "        # 假设字段可能是 'question' 或 'question_text'\n",
    "        q_text = item.get(\"question\", item.get(\"question_text\", \"\"))\n",
    "        q_pair = split_pair(q_text)\n",
    "        \n",
    "        # 2. 提取选项词对\n",
    "        # 假设字段是 'choices' 或 'options'\n",
    "        choices = item.get(\"choices\", item.get(\"options\", []))\n",
    "        # 如果是字典形式 {\"A\": \"...\", \"B\": \"...\"} 转为列表排序\n",
    "        if isinstance(choices, dict):\n",
    "            sorted_keys = sorted(choices.keys())\n",
    "            choices_list = [choices[k] for k in sorted_keys]\n",
    "        else:\n",
    "            choices_list = choices\n",
    "            \n",
    "        opt_pairs = [split_pair(c) for c in choices_list]\n",
    "        \n",
    "        lookup[data_id] = {\n",
    "            \"q_words\": q_pair,\n",
    "            \"opt_words_list\": opt_pairs\n",
    "        }\n",
    "    return lookup\n",
    "\n",
    "def main():\n",
    "    # 1. 加载 Label Map\n",
    "    print(\"正在加载标签映射...\")\n",
    "    label_map_raw = load_json(LABEL_MAP_FILE)\n",
    "    if not label_map_raw: return\n",
    "    # 建立 反向映射: \"主体-动作\" -> 2\n",
    "    name_to_id = {v: int(k) for k, v in label_map_raw.items()}\n",
    "\n",
    "    # 2. 加载 EKAR 原始数据 (为了获取词语)\n",
    "    print(\"正在加载 EKAR 原始数据...\")\n",
    "    ekar_data = load_json(EKAR_FILE)\n",
    "    if not ekar_data: \n",
    "        print(\"请确认 ekar_test.json 文件是否存在且格式正确。\")\n",
    "        return\n",
    "    \n",
    "    # 构建查询字典\n",
    "    ekar_lookup = build_ekar_lookup(ekar_data)\n",
    "    print(f\"已加载 {len(ekar_lookup)} 条原始题目数据。\")\n",
    "\n",
    "    # 3. 加载分析结果 (为了获取预测标签)\n",
    "    print(\"正在加载分析结果...\")\n",
    "    analysis_data = load_json(RESULT_FILE)\n",
    "    if not analysis_data: return\n",
    "    \n",
    "    if isinstance(analysis_data, dict):\n",
    "        analysis_data = [analysis_data]\n",
    "\n",
    "    results_to_write = []\n",
    "    \n",
    "    # 4. 核心处理循环\n",
    "    print(\"正在匹配数据...\")\n",
    "    for entry in analysis_data:\n",
    "        uid = entry.get(\"id\")\n",
    "        \n",
    "        # 查找对应的词语数据\n",
    "        if uid not in ekar_lookup:\n",
    "            print(f\"跳过: ID {uid} 在 {EKAR_FILE} 中未找到。\")\n",
    "            continue\n",
    "            \n",
    "        word_data = ekar_lookup[uid]\n",
    "        classifier_info = entry.get(\"classifier_analysis\", {})\n",
    "        \n",
    "        # --- 处理题干 (Question) ---\n",
    "        q_label_name = classifier_info.get(\"q_label\")\n",
    "        q_grid_index = name_to_id.get(q_label_name, 0) # 找不到默认为0 (Uncategorized)\n",
    "        \n",
    "        q_word_a, q_word_b = word_data[\"q_words\"]\n",
    "        \n",
    "        # 添加题干数据: (词A, 词B, 标签ID)\n",
    "        results_to_write.append((q_word_a, q_word_b, q_grid_index))\n",
    "        \n",
    "        # --- 处理选项 (Options) ---\n",
    "        opt_label_names = classifier_info.get(\"opt_labels\", [])\n",
    "        opt_word_pairs = word_data[\"opt_words_list\"]\n",
    "        \n",
    "        # 确保选项数量对齐（通常是4个）\n",
    "        for idx, label_name in enumerate(opt_label_names):\n",
    "            if idx < len(opt_word_pairs):\n",
    "                # 获取该选项对应的词对\n",
    "                o_word_a, o_word_b = opt_word_pairs[idx]\n",
    "                \n",
    "                # 获取该选项对应的标签ID\n",
    "                o_grid_index = name_to_id.get(label_name, 0)\n",
    "                \n",
    "                # 添加选项数据\n",
    "                results_to_write.append((o_word_a, o_word_b, o_grid_index))\n",
    "\n",
    "    # 5. 写入文件\n",
    "    print(f\"正在写入 {len(results_to_write)} 条数据到 {OUTPUT_FILE} ...\")\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "        for item in results_to_write:\n",
    "            # item 已经是 (Obj_A, Obj_B, grid_index) 格式\n",
    "            f.write(f\"{str(item)}\\n\")\n",
    "            \n",
    "    print(\"处理完成！\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d59147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis_consistency_result 题目总数: 335\n",
      "成功写出的 (Obj_A, Obj_B, grid_index) 行数: 1675\n",
      "在 ekar_test.json 中找不到对应 id 的题目数: 0\n",
      "题干 label 在 label_map 中找不到的数量: 0\n",
      "选项 label 在 label_map 中找不到的数量: 0\n",
      "题干文本无法抽出两词对 (Obj_A, Obj_B) 的数量: 0\n",
      "选项文本无法抽出两词对 (Obj_A, Obj_B) 的数量: 0\n",
      "写出文件: d:\\VH_analogy\\20260201_analogyreason\\obj_grid_index.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# 根目录（按你的路径）\n",
    "BASE_DIR = r\"d:\\VH_analogy\\20260201_analogyreason\"\n",
    "\n",
    "analysis_path = os.path.join(BASE_DIR, \"analysis_consistency_result.json\")\n",
    "label_map_path = os.path.join(BASE_DIR, \"label_map.json\")\n",
    "ekar_path = os.path.join(BASE_DIR, \"ekar_test.json\")\n",
    "output_path = os.path.join(BASE_DIR, \"obj_grid_index.csv\")\n",
    "\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def build_label_index(label_map):\n",
    "    \"\"\"\n",
    "    label_map: 形如 {\"0\": \"Uncategorized\", \"1\": \"主体-产物\", ...}\n",
    "    返回: {\"主体-产物\": 1, ...}，自动 strip() 掉前后空格\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for k, v in label_map.items():\n",
    "        if not isinstance(v, str):\n",
    "            continue\n",
    "        mapping[v.strip()] = int(k)\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def extract_pair(text):\n",
    "    \"\"\"\n",
    "    从形如 \"校长:老师\" 或 \"青年:村官:基层锻炼\" 的字符串中\n",
    "    提取前两个词，作为 (Obj_A, Obj_B)。\n",
    "\n",
    "    如果不能提取到两个词，返回 (None, None)。\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return None, None\n",
    "    parts = [p.strip() for p in text.split(\":\") if p.strip()]\n",
    "    if len(parts) < 2:\n",
    "        return None, None\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 读取三个 JSON\n",
    "    analysis_data = load_json(analysis_path)\n",
    "    label_map = load_json(label_map_path)\n",
    "    ekar_data = load_json(ekar_path)\n",
    "\n",
    "    # 构建 label -> index 映射\n",
    "    label_to_index = build_label_index(label_map)\n",
    "\n",
    "    # 把 ekar_test 按 id 建索引\n",
    "    ekar_by_id = {item.get(\"id\"): item for item in ekar_data if item.get(\"id\")}\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # 统计信息\n",
    "    total_items = len(analysis_data)\n",
    "    missing_ekar_count = 0\n",
    "    missing_q_label_index = 0\n",
    "    missing_opt_label_index = 0\n",
    "    bad_question_text_count = 0\n",
    "    bad_option_text_count = 0\n",
    "\n",
    "    for item in analysis_data:\n",
    "        qid = item.get(\"id\")\n",
    "        if not qid:\n",
    "            continue\n",
    "\n",
    "        ekar_item = ekar_by_id.get(qid)\n",
    "        if ekar_item is None:\n",
    "            missing_ekar_count += 1\n",
    "\n",
    "        classifier = item.get(\"classifier_analysis\", {}) or {}\n",
    "        q_label_raw = classifier.get(\"q_label\")\n",
    "        opt_labels_raw = classifier.get(\"opt_labels\", []) or []\n",
    "\n",
    "        # 规范化 label\n",
    "        q_label = q_label_raw.strip() if isinstance(q_label_raw, str) else None\n",
    "        opt_labels = [\n",
    "            (l.strip() if isinstance(l, str) else None) for l in opt_labels_raw\n",
    "        ]\n",
    "\n",
    "        # 1）题干：优先用 ekar_test.json 的 question；如果 ekar 里没有，就用 analysis 的 question_text\n",
    "        if ekar_item is not None:\n",
    "            q_text = ekar_item.get(\"question\")\n",
    "        else:\n",
    "            q_text = item.get(\"question_text\")\n",
    "\n",
    "        obj_a, obj_b = extract_pair(q_text)\n",
    "\n",
    "        if obj_a and obj_b and q_label in label_to_index:\n",
    "            grid_index = label_to_index[q_label]\n",
    "            rows.append((obj_a, obj_b, grid_index))\n",
    "        else:\n",
    "            # 统计问题：label 找不到 or 文本不是合法词对\n",
    "            if not (obj_a and obj_b):\n",
    "                bad_question_text_count += 1\n",
    "            if q_label and q_label not in label_to_index:\n",
    "                missing_q_label_index += 1\n",
    "\n",
    "        # 2）选项：只能在 ekar_test 中找选项文本；如果 ekar 里没这个 id，选项就没法生成\n",
    "        if ekar_item is None:\n",
    "            continue\n",
    "\n",
    "        choices = ekar_item.get(\"choices\", {}) or {}\n",
    "        choice_texts = choices.get(\"text\", []) or []\n",
    "\n",
    "        for i, opt_label in enumerate(opt_labels):\n",
    "            if i >= len(choice_texts):\n",
    "                # 防止 opt_labels 和 choices.text 长度不一致\n",
    "                continue\n",
    "\n",
    "            text = choice_texts[i]\n",
    "            obj_a, obj_b = extract_pair(text)\n",
    "            if not (obj_a and obj_b):\n",
    "                bad_option_text_count += 1\n",
    "                continue\n",
    "\n",
    "            if not opt_label or opt_label not in label_to_index:\n",
    "                missing_opt_label_index += 1\n",
    "                continue\n",
    "\n",
    "            grid_index = label_to_index[opt_label]\n",
    "            rows.append((obj_a, obj_b, grid_index))\n",
    "\n",
    "    # 写出 CSV\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Obj_A\", \"Obj_B\", \"grid_index\"])\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    # 打印统计\n",
    "    print(f\"analysis_consistency_result 题目总数: {total_items}\")\n",
    "    print(f\"成功写出的 (Obj_A, Obj_B, grid_index) 行数: {len(rows)}\")\n",
    "    print(f\"在 ekar_test.json 中找不到对应 id 的题目数: {missing_ekar_count}\")\n",
    "    print(f\"题干 label 在 label_map 中找不到的数量: {missing_q_label_index}\")\n",
    "    print(f\"选项 label 在 label_map 中找不到的数量: {missing_opt_label_index}\")\n",
    "    print(f\"题干文本无法抽出两词对 (Obj_A, Obj_B) 的数量: {bad_question_text_count}\")\n",
    "    print(f\"选项文本无法抽出两词对 (Obj_A, Obj_B) 的数量: {bad_option_text_count}\")\n",
    "    print(f\"写出文件: {output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311edda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3cec49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型: google/gemma-2-9b-it ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 模型已以 BF16 加载 \n",
      "正在加载本地数据集...\n",
      "✓ 加载训练集: 750 条数据\n",
      "✓ 加载验证集: 113 条数据\n",
      "总数据量: 863 条\n",
      "跳过已处理: 15 条\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma 本地数据处理:   0%|          | 0/14 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Gemma 本地数据处理: 100%|██████████| 14/14 [04:17<00:00, 18.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# ================= 配置区域 =================\n",
    "MODEL_ID = \"google/gemma-2-9b-it\"\n",
    "OUTPUT_FILE = \"RelationBook_train_valid.jsonl\"\n",
    "\n",
    "# 本地数据文件路径\n",
    "TRAIN_FILE = r\"D:\\VH_analogy\\ekar_train_cleaned.json\"\n",
    "VALID_FILE = r\"D:\\VH_analogy\\ekar_valid_cleaned.json\"\n",
    "\n",
    "# --- 新增：批处理大小 ---\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "os.environ['HF_HUB_ENDPOINT'] = os.getenv(\"HF_ENDPOINT\")\n",
    "# ================= 模型初始化 (针对 96GB 显存优化) =================\n",
    "print(f\"正在加载模型: {MODEL_ID} ...\")\n",
    "\n",
    "# --- 修改点：为 Tokenizer 添加 padding_side 以支持批处理 ---\n",
    "# 对于解码器-only 的模型 (如 Gemma), padding 需要在左侧\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side='left')\n",
    "# 如果分词器没有默认的 pad token, 手动设置为 eos_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"cuda\",  # 如果是单张大卡用 \"cuda\"，如果是多卡且想自动分配用 \"auto\"\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    token=HF_TOKEN\n",
    ")\n",
    "print(\">>> 模型已以 BF16 加载 \")\n",
    "\n",
    "\n",
    "# ================= Prompt 构造函数 (保持不变) =================\n",
    "def construct_prompt(item):\n",
    "    question = item['question']\n",
    "    choices = item['choices'] \n",
    "    answer_key = item['answerKey']\n",
    "    \n",
    "    # 处理解析\n",
    "    raw_explanation = item.get('explanation', [])\n",
    "    if isinstance(raw_explanation, list):\n",
    "        explanation_str = \"\\n\".join(raw_explanation)\n",
    "    else:\n",
    "        explanation_str = str(raw_explanation)\n",
    "\n",
    "    # 提取选项\n",
    "    choice_texts = choices['text']\n",
    "    correct_idx = ord(answer_key) - ord('A')\n",
    "    correct_choice_text = choice_texts[correct_idx] if 0 <= correct_idx < 4 else \"未知\"\n",
    "\n",
    "    # Prompt 内容\n",
    "    prompt_content = f\"\"\"以下是一个处理范例：\n",
    "输入：\n",
    "题目词对：生病:医药\n",
    "选项：A.空虚：信仰 B.糊涂：明白 C.雷雨：大风 D.难过：高兴\n",
    "正确答案：A\n",
    "参考解析：\n",
    "[\n",
    "\"改善\"生病\"的状况可以用\"医药\"。\",\n",
    "\"改善\"空虚\"的状况可以借助\"信仰\"。\",\n",
    "\"\"糊涂\"和\"明白\"的意思相反，\"明白\"不能改善\"糊涂\"的状况。\",\n",
    "\"\"雷雨\"和\"大风\"都属于自然现象，\"雷雨\"的状况不可以由\"大风\"改善。\",\n",
    "\"\"难过\"和\"高兴\"的意思相反，\"难过\"的状况不一定可以由\"高兴\"改善。\"\n",
    "]\n",
    "输出：注意这里的关系描述都是<词1>相对<词2>的关系，请严格按照这个格式输出。\n",
    "{{\n",
    "\"question_relation\": \"可以被治疗或改善\",\n",
    "\"relation_A\": \"可以被治疗或改善\",\n",
    "\"relation_B\": \"相反的意思\",\n",
    "\"relation_C\": \"并列伴随关系\",\n",
    "\"relation_D\": \"相反的意思\"\n",
    "}}\n",
    "\n",
    "请分析以下数据并提取关系：注意提取的关系描述都是<词1>相对<词2>的关系\n",
    "\n",
    "**题目词对**: {question}\n",
    "**选项**:\n",
    "A: {choice_texts[0]}\n",
    "B: {choice_texts[1]}\n",
    "C: {choice_texts[2]}\n",
    "D: {choice_texts[3]}\n",
    "\n",
    "**正确答案**: {answer_key} (即选项 {correct_choice_text})\n",
    "**参考解析**: \n",
    "{explanation_str}\n",
    "\n",
    "### 思考步骤\n",
    "思考的步骤如下：\n",
    "1. 分析【题目词对】的逻辑关系。\n",
    "2. 分析【正确选项】的逻辑关系，寻找它与题目词对的共同点（这就是最佳颗粒度）。\n",
    "3. 检查这个共同关系是否适用于错误选项。如果适用，说明颗粒度太粗，需要增加限定条件；如果不适用，则保留。\n",
    "4. 对每个错误选项，分析其逻辑关系\n",
    "5. 确保提取的关系是从<词1>相对<词2>的视角出发的。\n",
    "### 输出格式\n",
    "\n",
    "请直接输出一个 JSON 对象，不要包含其他废话，格式如下：\n",
    "{{\n",
    "\"question_relation\": \"题目的词对之间的关系\",\n",
    "\"relation_A\": \"选项A的词对之间的关系\",\n",
    "\"relation_B\": \"选项B的词对之间的关系\",\n",
    "\"relation_C\": \"选项C的词对之间的关系\",\n",
    "\"relation_D\": \"选项D的词对之间的关系\"\n",
    "}}\"\"\"\n",
    "    return prompt_content\n",
    "\n",
    "# ================= 推理函数 =================\n",
    "def get_extractions_from_gemma_batch(batch_items):\n",
    "    \"\"\"\n",
    "    构造 Prompt 并获取模型对一个批次的提取结果\n",
    "    \"\"\"\n",
    "    all_messages = []\n",
    "\n",
    "    # 为批次中的每个项目创建 Prompt\n",
    "    for item in batch_items:\n",
    "        prompt_text = construct_prompt(item)\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "        all_messages.append(messages)\n",
    "\n",
    "    # 使用 Gemma 的标准对话模板，并进行 padding\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        all_messages,\n",
    "        return_tensors=\"pt\",\n",
    "        add_generation_prompt=True,\n",
    "        padding=True,  # 启用填充\n",
    "        truncation=True  # 启用截断\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
    "    ]\n",
    "\n",
    "    # 生成配置: 贪婪解码(do_sample=False)以获得最稳定的提取\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False,  # 贪婪解码，保证逻辑提取的一致性\n",
    "    )\n",
    "\n",
    "    # 解码 (只取生成的新 token)\n",
    "    input_ids_len = input_ids.shape[1]\n",
    "    generated_tokens = outputs[:, input_ids_len:]\n",
    "    responses = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # 返回一个包含所有原始输出的列表\n",
    "    return [res.strip() for res in responses]\n",
    "\n",
    "def process_dataset():\n",
    "    \"\"\"从本地JSON文件加载数据\"\"\"\n",
    "    print(\"正在加载本地数据集...\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    # 加载训练集\n",
    "    try:\n",
    "        with open(TRAIN_FILE, 'r', encoding='utf-8') as f:\n",
    "            train_data = json.load(f)\n",
    "            all_data.extend(train_data)\n",
    "        print(f\"✓ 加载训练集: {len(train_data)} 条数据\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"✗ 找不到文件: {TRAIN_FILE}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"✗ 加载训练集失败: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 加载验证集\n",
    "    try:\n",
    "        with open(VALID_FILE, 'r', encoding='utf-8') as f:\n",
    "            valid_data = json.load(f)\n",
    "            all_data.extend(valid_data)\n",
    "        print(f\"✓ 加载验证集: {len(valid_data)} 条数据\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"✗ 找不到文件: {VALID_FILE}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"✗ 加载验证集失败: {e}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"总数据量: {len(all_data)} 条\")\n",
    "\n",
    "    # 断点续传\n",
    "    processed_ids = set()\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    if 'id' in record: \n",
    "                        processed_ids.add(record['id'])\n",
    "                except: \n",
    "                    pass\n",
    "    print(f\"跳过已处理: {len(processed_ids)} 条\")\n",
    "\n",
    "    # 主循环 - 按批次处理\n",
    "    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f_out:\n",
    "        for i in tqdm(range(0, len(all_data), BATCH_SIZE), desc=\"Gemma 本地数据处理\"):\n",
    "            # 获取当前批次的数据\n",
    "            batch = all_data[i:i + BATCH_SIZE]\n",
    "            \n",
    "            # 过滤已处理的\n",
    "            batch_to_process = [item for item in batch if item['id'] not in processed_ids]\n",
    "            \n",
    "            if not batch_to_process:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # 批量获取模型的原始输出\n",
    "                responses = get_extractions_from_gemma_batch(batch_to_process)\n",
    "\n",
    "                # 遍历当前批次的结果并写入文件\n",
    "                for item, response_text in zip(batch_to_process, responses):\n",
    "                    item_id = item['id']\n",
    "                    \n",
    "                    # 清洗 JSON\n",
    "                    clean_json_str = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "                    try:\n",
    "                        model_output = json.loads(clean_json_str)\n",
    "                    except json.JSONDecodeError:\n",
    "                        model_output = {\"error\": \"JSON_DECODE_FAIL\", \"raw_content\": response_text}\n",
    "\n",
    "                    final_record = {\n",
    "                        \"id\": item_id,\n",
    "                        \"original_question\": item['question'],\n",
    "                        \"model_extraction\": model_output\n",
    "                    }\n",
    "                    \n",
    "                    f_out.write(json.dumps(final_record, ensure_ascii=False) + \"\\n\")\n",
    "                    f_out.flush()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"处理批次 {i//BATCH_SIZE + 1} 时出错 (ID 从 {batch[0]['id']} 开始): {e}\")\n",
    "                # 你可以选择在这里为批次中的所有项目写入错误信息，或者跳过\n",
    "                for item in batch_to_process:\n",
    "                    item_id = item['id']\n",
    "                    final_record = {\n",
    "                        \"id\": item_id,\n",
    "                        \"original_question\": item['question'],\n",
    "                        \"model_extraction\": {\"error\": str(e)}\n",
    "                    }\n",
    "                    f_out.write(json.dumps(final_record, ensure_ascii=False) + \"\\n\")\n",
    "                    f_out.flush()\n",
    "                continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "901faa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已成功提取关系到 relations_output.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# ================= 配置区域 =================\n",
    "INPUT_FILE = \"RelationBook_train_valid.jsonl\"\n",
    "OUTPUT_FILE = \"relations_output.txt\"\n",
    "\n",
    "def extract_relations():\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f_out:\n",
    "        with open(INPUT_FILE, 'r', encoding='utf-8') as f_in:\n",
    "            for line in f_in:\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    model_extraction = record.get('model_extraction', {})\n",
    "                    question_relation = model_extraction.get('question_relation', '')\n",
    "                    relation_A = model_extraction.get('relation_A', '')\n",
    "                    relation_B = model_extraction.get('relation_B', '')\n",
    "                    relation_C = model_extraction.get('relation_C', '')\n",
    "                    relation_D = model_extraction.get('relation_D', '')\n",
    "                    \n",
    "                    # 写入五个关系，每行一个（纵列排列）\n",
    "                    f_out.write(f\"{question_relation}\\n{relation_A}\\n{relation_B}\\n{relation_C}\\n{relation_D}\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing line: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    print(f\"✅ 已成功提取关系到 {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_relations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eb5153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Relation_Labels = [\n",
    "    \"同义关系\",\n",
    "    \"近义关系\",\n",
    "    \"反义关系\",\n",
    "    \"包含关系（整体-部分）\",\n",
    "    \"部分-整体关系（组成部分）\",\n",
    "    \"种属关系（词1是词2的一种）\",\n",
    "    \"类别关系（词1和词2是同类）\",\n",
    "    \"并列关系\",\n",
    "    \"对立矛盾关系\",\n",
    "    \"互补关系\",\n",
    "    \"可能因果关系\",\n",
    "    \"必然因果关系\",\n",
    "    \"必要条件关系\",\n",
    "    \"充分条件关系\",\n",
    "    \"目的关系\",\n",
    "    \"结果关系\",\n",
    "    \"递进关系\",\n",
    "    \"工具-用途关系\",\n",
    "    \"工具-使用者关系\",\n",
    "    \"工具-对象关系\",\n",
    "    \"场所-活动关系\",\n",
    "    \"场所-人员关系\",\n",
    "    \"场所-物品关系\",\n",
    "    \"相邻位置关系\",\n",
    "    \"位于关系\",\n",
    "    \"时间顺序关系\",\n",
    "    \"时间同时关系\",\n",
    "    \"时间间隔关系\",\n",
    "    \"时期朝代关系\",\n",
    "    \"节气关系\",\n",
    "    \"省份关系\",\n",
    "    \"城市关系\",\n",
    "    \"国家关系\",\n",
    "    \"亲属关系\",\n",
    "    \"师生关系\",\n",
    "    \"上下级关系\",\n",
    "    \"同事关系\",\n",
    "    \"服务关系\",\n",
    "    \"敌对关系\",\n",
    "    \"职业-工作内容关系\",\n",
    "    \"职业-工具关系\",\n",
    "    \"作者-作品关系\",\n",
    "    \"作品-人物关系\",\n",
    "    \"作品-思想关系\",\n",
    "    \"象征关系\",\n",
    "    \"比喻关系\",\n",
    "    \"标志关系\",\n",
    "    \"产地关系\",\n",
    "    \"材料-成品关系\",\n",
    "    \"功能关系\",\n",
    "    \"属性特征关系\",\n",
    "    \"数量关系\",\n",
    "    \"单位-物理量关系\",\n",
    "    \"语法关系-主谓\",\n",
    "    \"语法关系-动宾\",\n",
    "    \"语法关系-偏正\",\n",
    "    \"成语结构关系\",\n",
    "    \"音译关系\",\n",
    "    \"简称关系\",\n",
    "    \"别名关系\",\n",
    "    \"演变关系（古今、阶段）\",\n",
    "    \"替代关系\",\n",
    "    \"配套使用关系\",\n",
    "    \"使用关系\",\n",
    "    \"依赖关系\",\n",
    "    \"控制关系\",\n",
    "    \"领导关系\",\n",
    "    \"合作关系\",\n",
    "    \"对抗关系\",\n",
    "    \"法律关系\",\n",
    "    \"经济关系\",\n",
    "    \"文化习俗关系\",\n",
    "    \"自然现象关系\",\n",
    "    \"物理关系\",\n",
    "    \"化学关系\",\n",
    "    \"数学关系\",\n",
    "    \"抽象概念关系\",\n",
    "    \"具体事物关系\",\n",
    "    \"可能关系\",\n",
    "    \"无逻辑关系\",\n",
    "    \"不确定关系\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c0623a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已加载完成\n",
      "正在从 JSONL 提取关系...\n",
      "共提取 4315 条关系记录。\n",
      "使用用户定义的标签列表，共 81 个标签。\n",
      "开始批处理推理 (Batch Size: 128)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/34 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Processing Batches: 100%|██████████| 34/34 [04:09<00:00,  7.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "处理完成！结果已保存至: mapped_relations.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILE = \"RelationBook_train_valid.jsonl\"\n",
    "OUTPUT_FILE = \"mapped_relations.csv\"\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side='left')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "print(\"模型已加载完成\")\n",
    "\n",
    "# ================= 数据提取函数 =================\n",
    "def extract_relations_from_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    从 JSONL 文件中提取所有关系字段（question_relation 和 relation_A~D）\n",
    "    返回列表，每个元素为字典：{'id': 带后缀的唯一标识, 'relation': 关系字符串}\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"第 {line_num} 行 JSON 解析错误: {e}\")\n",
    "                continue\n",
    "\n",
    "            base_id = data.get('id', f'line_{line_num}')\n",
    "            model_ext = data.get('model_extraction', {})\n",
    "\n",
    "            # 提取 question_relation\n",
    "            q_rel = model_ext.get('question_relation')\n",
    "            if q_rel:\n",
    "                items.append({\n",
    "                    'id': f\"{base_id}_question\",\n",
    "                    'relation': q_rel\n",
    "                })\n",
    "\n",
    "            # 提取 relation_A 到 relation_D\n",
    "            for suffix in ['A', 'B', 'C', 'D']:\n",
    "                rel = model_ext.get(f'relation_{suffix}')\n",
    "                if rel:\n",
    "                    items.append({\n",
    "                        'id': f\"{base_id}_{suffix}\",\n",
    "                        'relation': rel\n",
    "                    })\n",
    "\n",
    "    return items\n",
    "\n",
    "# ================= 批处理推理函数 =================\n",
    "def get_labels_from_gemma_batch(batch_items, relation_labels):\n",
    "    \"\"\"\n",
    "    对一批关系文本进行分类，返回模型原始输出列表\n",
    "    \"\"\"\n",
    "    # 构建候选标签字符串\n",
    "    tags_str = \"\\n\".join([f\"- {tag}\" for tag in relation_labels])\n",
    "\n",
    "    all_messages = []\n",
    "    for item in batch_items:\n",
    "        user_content = f\"\"\"你是一位关系分类专家。你的任务是将给定的关系短语归类到预定义的标准关系类型列表中。\n",
    "\n",
    "### 标准关系类型列表：\n",
    "{tags_str}\n",
    "\n",
    "### 待分类的关系短语：\n",
    "{item['relation']}\n",
    "\n",
    "### 指令：\n",
    "请从“标准关系类型列表”中选择最匹配的一个类型。严格仅输出列表中的类型名称，不要输出任何其他文本。\n",
    "\"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "        all_messages.append(messages)\n",
    "\n",
    "    # 批量 tokenize\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        all_messages,\n",
    "        return_tensors=\"pt\",\n",
    "        add_generation_prompt=True,\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    # 生成\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=32,\n",
    "        do_sample=False,\n",
    "        temperature=None,\n",
    "        top_p=None\n",
    "    )\n",
    "\n",
    "    # 提取新生成的 token\n",
    "    input_len = input_ids.shape[1]\n",
    "    generated = outputs[:, input_len:]\n",
    "    responses = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "\n",
    "    return [res.strip() for res in responses]\n",
    "\n",
    "def clean_label(raw_output, relation_labels):\n",
    "    \"\"\"\n",
    "    清洗模型输出，映射到 relation_labels 中的某个标签\n",
    "    \"\"\"\n",
    "    # 去除可能的 \"类别：\" 等前缀和标点\n",
    "    cleaned = raw_output.replace(\"Category:\", \"\").replace(\"类别：\", \"\").strip()\n",
    "    cleaned = cleaned.rstrip(\"。.\")\n",
    "\n",
    "    # 完全匹配\n",
    "    if cleaned in relation_labels:\n",
    "        return cleaned\n",
    "\n",
    "    # 模糊匹配：检查 cleaned 是否包含某个标签，或标签包含 cleaned\n",
    "    for tag in relation_labels:\n",
    "        if tag in cleaned or cleaned in tag:\n",
    "            return tag\n",
    "\n",
    "    return \"Uncategorized\"\n",
    "\n",
    "# ================= 主程序 =================\n",
    "def main():\n",
    "    # 1. 读取数据\n",
    "    print(\"正在从 JSONL 提取关系...\")\n",
    "    all_items = extract_relations_from_jsonl(INPUT_FILE)\n",
    "    print(f\"共提取 {len(all_items)} 条关系记录。\")\n",
    "\n",
    "    if not all_items:\n",
    "        print(\"未提取到任何关系，程序退出。\")\n",
    "        return\n",
    "\n",
    "    # 2. 确定关系标签列表\n",
    "    relation_labels = Relation_Labels\n",
    "    if not relation_labels:\n",
    "        # 自动收集唯一关系值作为默认标签\n",
    "        unique_relations = sorted(set(item['relation'] for item in all_items))\n",
    "        relation_labels = unique_relations\n",
    "        print(f\"未定义 Relation_Labels，已自动从数据中收集 {len(relation_labels)} 个唯一关系作为标签。\")\n",
    "    else:\n",
    "        print(f\"使用用户定义的标签列表，共 {len(relation_labels)} 个标签。\")\n",
    "\n",
    "    # 3. 批处理推理\n",
    "    print(f\"开始批处理推理 (Batch Size: {BATCH_SIZE})...\")\n",
    "    with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "        fieldnames = ['id', 'relation', 'label', 'raw_output']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for i in tqdm(range(0, len(all_items), BATCH_SIZE), desc=\"Processing Batches\"):\n",
    "            batch = all_items[i:i+BATCH_SIZE]\n",
    "            try:\n",
    "                raw_outputs = get_labels_from_gemma_batch(batch, relation_labels)\n",
    "\n",
    "                for item, raw_out in zip(batch, raw_outputs):\n",
    "                    label = clean_label(raw_out, relation_labels)\n",
    "                    writer.writerow({\n",
    "                        'id': item['id'],\n",
    "                        'relation': item['relation'],\n",
    "                        'label': label,\n",
    "                        'raw_output': raw_out\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"处理批次 {i//BATCH_SIZE + 1} 时出错: {e}\")\n",
    "                # 将错误写入 CSV\n",
    "                for item in batch:\n",
    "                    writer.writerow({\n",
    "                        'id': item['id'],\n",
    "                        'relation': item['relation'],\n",
    "                        'label': 'Error',\n",
    "                        'raw_output': str(e)\n",
    "                    })\n",
    "\n",
    "    print(f\"\\n处理完成！结果已保存至: {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adcd5df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "307ffd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型: google/gemma-2-9b-it ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 模型已以 BF16 加载完成\n",
      "正在读取 D:\\VH_analogy\\ekar_test_cleaned.json ...\n",
      "共 205 条数据\n",
      "跳过已处理: 0 条\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma 关系提取:   0%|          | 0/4 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Gemma 关系提取: 100%|██████████| 4/4 [00:43<00:00, 10.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，结果保存到 ekar_test_relations.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# ================= 配置区域 =================\n",
    "MODEL_ID = \"google/gemma-2-9b-it\"\n",
    "INPUT_FILE = r\"D:\\VH_analogy\\ekar_test_cleaned.json\"\n",
    "OUTPUT_FILE = \"ekar_test_relations.jsonl\"\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# ================= 模型初始化 =================\n",
    "print(f\"正在加载模型: {MODEL_ID} ...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side='left', token=HF_TOKEN)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "print(\">>> 模型已以 BF16 加载完成\")\n",
    "\n",
    "# ================= Prompt 构造函数 =================\n",
    "def construct_prompt(item):\n",
    "    q_pair = item['question']\n",
    "    choices = item['choices']['text']\n",
    "    answer_key = item['answerKey']\n",
    "\n",
    "    prompt = f\"\"\"作为一个语言逻辑专家，你的任务是分析类比推理题。\n",
    "输入：\n",
    "题目: {q_pair}\n",
    "选项:\n",
    "A: {choices[0]}\n",
    "B: {choices[1]}\n",
    "C: {choices[2]}\n",
    "D: {choices[3]}\n",
    "\n",
    "任务：\n",
    "1. 分析【题目词对】的核心逻辑关系。\n",
    "2. 独立分析每个【选项词对】的逻辑关系。\n",
    "3. 不要输出答案。\n",
    "\n",
    "格式要求：\n",
    "- 必须确保提取的关系是从<词1>相对<词2>的视角出发的\n",
    "- 仅输出 JSON。\n",
    "\n",
    "输出示例：\n",
    "{{\n",
    "    \"q_relation\": \"词1是词2的组成部分\",\n",
    "    \"a_relation\": \"位置是高度的决定因素\",\n",
    "    \"b_relation\": \"...\",\n",
    "    \"c_relation\": \"...\",\n",
    "    \"d_relation\": \"...\"\n",
    "}}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# ================= 批处理推理函数 =================\n",
    "def get_relations_from_gemma_batch(batch_items):\n",
    "    \"\"\"\n",
    "    批量获取 Gemma 的关系提取结果\n",
    "    \"\"\"\n",
    "    all_messages = []\n",
    "    for item in batch_items:\n",
    "        prompt_text = construct_prompt(item)\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "        all_messages.append(messages)\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        all_messages,\n",
    "        return_tensors=\"pt\",\n",
    "        add_generation_prompt=True,\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    input_ids_len = input_ids.shape[1]\n",
    "    generated_tokens = outputs[:, input_ids_len:]\n",
    "    responses = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return [res.strip() for res in responses]\n",
    "\n",
    "# ================= 主程序 =================\n",
    "def main():\n",
    "    # 读取数据\n",
    "    print(f\"正在读取 {INPUT_FILE} ...\")\n",
    "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(f\"共 {len(data)} 条数据\")\n",
    "\n",
    "    # 断点续传\n",
    "    processed_ids = set()\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    if 'id' in record:\n",
    "                        processed_ids.add(record['id'])\n",
    "                except:\n",
    "                    pass\n",
    "    print(f\"跳过已处理: {len(processed_ids)} 条\")\n",
    "\n",
    "    # 主循环\n",
    "    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f_out:\n",
    "        for i in tqdm(range(0, len(data), BATCH_SIZE), desc=\"Gemma 关系提取\"):\n",
    "            batch = data[i:i + BATCH_SIZE]\n",
    "            batch_to_process = [item for item in batch if item['id'] not in processed_ids]\n",
    "\n",
    "            if not batch_to_process:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                responses = get_relations_from_gemma_batch(batch_to_process)\n",
    "\n",
    "                for item, response_text in zip(batch_to_process, responses):\n",
    "                    item_id = item['id']\n",
    "\n",
    "                    # 清洗 JSON\n",
    "                    clean_json_str = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "                    try:\n",
    "                        relations = json.loads(clean_json_str)\n",
    "                    except json.JSONDecodeError:\n",
    "                        relations = {\"error\": \"JSON_DECODE_FAIL\", \"raw_content\": response_text}\n",
    "\n",
    "                    final_record = {\n",
    "                        \"id\": item_id,\n",
    "                        \"relations\": relations\n",
    "                    }\n",
    "\n",
    "                    f_out.write(json.dumps(final_record, ensure_ascii=False) + \"\\n\")\n",
    "                    f_out.flush()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"处理批次 {i//BATCH_SIZE + 1} 时出错: {e}\")\n",
    "                for item in batch_to_process:\n",
    "                    final_record = {\n",
    "                        \"id\": item['id'],\n",
    "                        \"relations\": {\"error\": str(e)}\n",
    "                    }\n",
    "                    f_out.write(json.dumps(final_record, ensure_ascii=False) + \"\\n\")\n",
    "                    f_out.flush()\n",
    "\n",
    "    print(f\"处理完成，结果保存到 {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4f4843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VH_analogy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
