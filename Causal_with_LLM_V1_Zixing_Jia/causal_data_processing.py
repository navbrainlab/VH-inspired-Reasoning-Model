# ===== å› æœæ•°æ®å¤„ç†ä¸ç¼–ç å­¦ä¹  =====
# åŠŸèƒ½ï¼š
# 1. åŠ è½½Gemmaå›ç­”æ•°æ®å¹¶ä½¿ç”¨NV-Embedå‘é‡åŒ–
# 2. ä½¿ç”¨HDBSCANèšç±»
# 3. ä½¿ç”¨centroidæ–¹æ³•æ‰¾åˆ°ç±»ä»£è¡¨å‘é‡
# 4. æ„é€ è®­ç»ƒæ•°æ®é›† {A, B, ASKFOR, INDEX}
# 5. æ¨¡ä»¿Causal_V2è¿›è¡Œå› æœäº‹ä»¶ç¼–ç å­¦ä¹ 

import os
import json
import numpy as np
import torch
import torch.nn.functional as F
from collections import defaultdict
from tqdm import tqdm
import hdbscan
from sklearn.preprocessing import normalize
from sklearn.metrics.pairwise import cosine_similarity

# ================== é…ç½®å‚æ•° ==================
# Linux è·¯å¾„
BASE_DIR = "/home/amax/Zixing_Jia/Vector-HASH-tinkering-main/Gemma_anwer_causal"
DATA_PATH = os.path.join(BASE_DIR, "T_1.25_Gemma_Answer/gemma_causal_answers_100.jsonl")
MODEL_PATH = "/home/amax/Gemma_and_NVembed/NV/snapshots/main"
OUTPUT_DIR = os.path.join(BASE_DIR, "Causal_Data")

# HDBSCANå‚æ•°
HDBSCAN_MIN_CLUSTER_SIZE = 5
HDBSCAN_MIN_SAMPLES = 3

# ================== åˆ›å»ºè¾“å‡ºç›®å½• ==================
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ================== åŠ è½½NV-Embedæ¨¡å‹ ==================
print("=" * 60)
print("Step 1: åŠ è½½NV-Embed-v2æ¨¡å‹")
print("=" * 60)

from transformers import AutoModel, AutoTokenizer

print(f"æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

print("\næ­£åœ¨åŠ è½½ tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    local_files_only=True
)

print("â³ æ­£åœ¨åŠ è½½æ¨¡å‹...")
model = AutoModel.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    local_files_only=True,
    torch_dtype=torch.float16
)

device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)
model.eval()

print(f"\næ¨¡å‹åŠ è½½æˆåŠŸï¼è¿è¡Œè®¾å¤‡: {device}")

# ================== å®šä¹‰ç¼–ç å‡½æ•° ==================
def encode_texts(texts, max_length=4096, batch_size=32):
    """
    å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡
    
    Args:
        texts: å•ä¸ªå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
        max_length: æœ€å¤§åºåˆ—é•¿åº¦
        batch_size: æ‰¹å¤„ç†å¤§å°
    
    Returns:
        numpy array of shape (n_texts, 4096)
    """
    if isinstance(texts, str):
        texts = [texts]
    
    eos_token = tokenizer.eos_token if tokenizer.eos_token else "</s>"
    texts_with_eos = [text + eos_token for text in texts]
    
    all_embeddings = []
    
    for i in range(0, len(texts_with_eos), batch_size):
        batch_texts = texts_with_eos[i:i+batch_size]
        
        batch_dict = tokenizer(
            batch_texts,
            padding=True,
            truncation=True,
            max_length=max_length,
            return_tensors="pt"
        )
        batch_dict = {k: v.to(device) for k, v in batch_dict.items()}
        
        attention_mask = batch_dict["attention_mask"]
        seq_lengths = attention_mask.sum(dim=1)
        pool_mask = torch.zeros_like(attention_mask)
        for j, length in enumerate(seq_lengths):
            pool_mask[j, length - 1] = 1
        
        with torch.no_grad():
            outputs = model(
                input_ids=batch_dict["input_ids"],
                attention_mask=attention_mask,
                pool_mask=pool_mask
            )
            
            if isinstance(outputs, dict):
                if "sentence_embeddings" in outputs:
                    embeddings = outputs["sentence_embeddings"]
                elif "last_hidden_state" in outputs:
                    embeddings = outputs["last_hidden_state"][:, -1, :]
                else:
                    raise ValueError(f"æ— æ³•ä»è¾“å‡ºä¸­æå–åµŒå…¥å‘é‡: {outputs.keys()}")
            elif hasattr(outputs, "sentence_embeddings"):
                embeddings = outputs.sentence_embeddings
            elif hasattr(outputs, "last_hidden_state"):
                embeddings = outputs.last_hidden_state[:, -1, :]
            else:
                embeddings = outputs
        
        embeddings = F.normalize(embeddings, p=2, dim=1)
        embeddings_cpu = embeddings.float().cpu()
        all_embeddings.append(np.array(embeddings_cpu.tolist()))
    
    return np.vstack(all_embeddings)

# ================== åŠ è½½æ•°æ® ==================
print("\n" + "=" * 60)
print("Step 2: åŠ è½½Gemmaå›ç­”æ•°æ®")
print("=" * 60)

data_by_premise = defaultdict(list)  # premise -> list of records
all_records = []

print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {DATA_PATH}")

with open(DATA_PATH, 'r', encoding='utf-8') as f:
    for line in f:
        record = json.loads(line.strip())
        all_records.append(record)
        premise = record['premise']
        data_by_premise[premise].append(record)

print(f"âœ… åŠ è½½äº† {len(all_records)} æ¡è®°å½•")
print(f"âœ… å…±æœ‰ {len(data_by_premise)} ä¸ªä¸åŒçš„ Premise")

# ç»Ÿè®¡ask-foråˆ†å¸ƒ
ask_for_counts = defaultdict(int)
for r in all_records:
    ask_for_counts[r['ask-for']] += 1
print(f"ğŸ“Š Ask-for åˆ†å¸ƒ: {dict(ask_for_counts)}")

# ================== å‘é‡åŒ– Premise ==================
print("\n" + "=" * 60)
print("Step 3: å‘é‡åŒ–æ‰€æœ‰å”¯ä¸€çš„ Premise")
print("=" * 60)

unique_premises = list(data_by_premise.keys())
print(f"ğŸ”„ æ­£åœ¨å‘é‡åŒ– {len(unique_premises)} ä¸ªå”¯ä¸€ Premise...")

premise_embeddings = {}
batch_size = 32
for i in tqdm(range(0, len(unique_premises), batch_size)):
    batch = unique_premises[i:i+batch_size]
    batch_emb = encode_texts(batch)
    for j, premise in enumerate(batch):
        premise_embeddings[premise] = batch_emb[j]

print(f"âœ… Premise å‘é‡åŒ–å®Œæˆï¼ç»´åº¦: {next(iter(premise_embeddings.values())).shape}")

# ================== å‘é‡åŒ– Gemma_answer å¹¶èšç±» ==================
print("\n" + "=" * 60)
print("Step 4: å¯¹æ¯ä¸ª Premise çš„ Gemma_answer è¿›è¡Œå‘é‡åŒ–å’Œèšç±»")
print("=" * 60)

# å­˜å‚¨èšç±»ç»“æœ
# Structure: {premise: {
#   'premise_vec': np.array,
#   'ask_for': str,
#   'index': str,
#   'clusters': {cluster_id: {'centroid': np.array, 'count': int, 'answers': list}}
# }}
clustering_results = {}

for premise, records in tqdm(data_by_premise.items(), desc="å¤„ç† Premise"):
    # è·å–è¯¥premiseçš„æ‰€æœ‰å›ç­”
    answers = [r['Gemma_answer'] for r in records]
    ask_for = records[0]['ask-for']
    index = records[0]['index']
    
    # å‘é‡åŒ–æ‰€æœ‰å›ç­”
    answer_embeddings = encode_texts(answers)
    answer_embeddings_norm = normalize(answer_embeddings, norm='l2')
    
    # HDBSCANèšç±»
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=HDBSCAN_MIN_CLUSTER_SIZE,
        min_samples=HDBSCAN_MIN_SAMPLES,
        metric='euclidean',
        cluster_selection_method='eom'
    )
    cluster_labels = clusterer.fit_predict(answer_embeddings_norm)
    
    # è®¡ç®—æ¯ä¸ªèšç±»çš„centroidå’Œæ ·æœ¬æ•°
    unique_clusters = set(cluster_labels)
    clusters_info = {}
    
    for cluster_id in unique_clusters:
        if cluster_id == -1:  # è·³è¿‡å™ªå£°ç‚¹
            continue
        
        # è·å–è¯¥èšç±»çš„æ‰€æœ‰å‘é‡
        mask = cluster_labels == cluster_id
        cluster_vectors = answer_embeddings_norm[mask]
        cluster_answers = [answers[i] for i, m in enumerate(mask) if m]
        
        # è®¡ç®—centroid
        centroid = np.mean(cluster_vectors, axis=0)
        centroid = centroid / (np.linalg.norm(centroid) + 1e-12)  # å½’ä¸€åŒ–
        
        clusters_info[cluster_id] = {
            'centroid': centroid,
            'count': int(np.sum(mask)),
            'answers': cluster_answers[:5]  # åªä¿å­˜å‰5ä¸ªä½œä¸ºç¤ºä¾‹
        }
    
    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆèšç±»ï¼Œå°†æ‰€æœ‰ç‚¹ä½œä¸ºä¸€ä¸ªèšç±»
    if len(clusters_info) == 0:
        centroid = np.mean(answer_embeddings_norm, axis=0)
        centroid = centroid / (np.linalg.norm(centroid) + 1e-12)
        clusters_info[0] = {
            'centroid': centroid,
            'count': len(answers),
            'answers': answers[:5]
        }
    
    clustering_results[premise] = {
        'premise_vec': premise_embeddings[premise],
        'ask_for': ask_for,
        'index': index,
        'clusters': clusters_info,
        'total_answers': len(answers),
        'n_clusters': len(clusters_info)
    }

print(f"\nâœ… èšç±»å®Œæˆï¼")
print(f"ğŸ“Š èšç±»ç»Ÿè®¡:")
total_clusters = sum(r['n_clusters'] for r in clustering_results.values())
print(f"   - æ€»èšç±»æ•°: {total_clusters}")
print(f"   - å¹³å‡æ¯ä¸ªPremiseçš„èšç±»æ•°: {total_clusters / len(clustering_results):.2f}")

# ================== æ„é€ è®­ç»ƒæ•°æ®é›† ==================
print("\n" + "=" * 60)
print("Step 5: æ„é€ è®­ç»ƒæ•°æ®é›† {A, B, ASKFOR, INDEX}")
print("=" * 60)

# æ•°æ®é›†ç»“æ„ï¼š
# - A: Premiseå‘é‡ (æˆ– Gemma_answerå‘é‡ï¼Œå–å†³äºASKFOR)
# - B: Gemma_answerå‘é‡çš„centroid (æˆ– Premiseå‘é‡)
# - ASKFOR: 'cause' æˆ– 'effect'
# - INDEX: åŸå§‹æ•°æ®é›†ä¸­çš„ç´¢å¼•
# 
# å¦‚æœ ASKFOR='cause': Premiseæ˜¯æœï¼ŒGemma_answeræ˜¯å› 
# å¦‚æœ ASKFOR='effect': Premiseæ˜¯å› ï¼ŒGemma_answeræ˜¯æœ

causal_data = []

for premise, result in clustering_results.items():
    ask_for = result['ask_for']
    index = result['index']
    premise_vec = result['premise_vec']
    
    for cluster_id, cluster_info in result['clusters'].items():
        centroid = cluster_info['centroid']
        count = cluster_info['count']
        
        # æ ¹æ®ask_forç¡®å®šAå’ŒB
        # ask_for='cause' è¡¨ç¤ºé—®çš„æ˜¯åŸå› ï¼Œæ‰€ä»¥Premiseæ˜¯ç»“æœ(Effect)ï¼ŒGemma_answeræ˜¯åŸå› (Cause)
        # ask_for='effect' è¡¨ç¤ºé—®çš„æ˜¯ç»“æœï¼Œæ‰€ä»¥Premiseæ˜¯åŸå› (Cause)ï¼ŒGemma_answeræ˜¯ç»“æœ(Effect)
        
        if ask_for == 'cause':
            # Premiseæ˜¯Effectï¼ŒGemma_answer(centroid)æ˜¯Cause
            A = centroid  # Cause
            B = premise_vec  # Effect
            direction = 'cause'  # è¡¨ç¤º A->B æ˜¯ å› ->æœ
        else:  # effect
            # Premiseæ˜¯Causeï¼ŒGemma_answer(centroid)æ˜¯Effect
            A = premise_vec  # Cause
            B = centroid  # Effect
            direction = 'effect'  # è¡¨ç¤º A->B æ˜¯ å› ->æœ
        
        # æ ¹æ®countå¤åˆ¶æ ·æœ¬
        for _ in range(count):
            causal_data.append({
                'A': A,
                'B': B,
                'ASKFOR': ask_for,
                'INDEX': index,
                'cluster_id': cluster_id,
                'premise': premise
            })

print(f"âœ… æ•°æ®é›†æ„é€ å®Œæˆï¼")
print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
print(f"   - æ€»æ ·æœ¬æ•°: {len(causal_data)}")
print(f"   - ask-for='cause' æ ·æœ¬æ•°: {sum(1 for d in causal_data if d['ASKFOR'] == 'cause')}")
print(f"   - ask-for='effect' æ ·æœ¬æ•°: {sum(1 for d in causal_data if d['ASKFOR'] == 'effect')}")

# ================== ä¿å­˜æ•°æ® ==================
print("\n" + "=" * 60)
print("Step 6: ä¿å­˜æ•°æ®")
print("=" * 60)

# ä¿å­˜å‘é‡åŒ–æ•°æ®ï¼ˆnumpyæ ¼å¼ï¼‰
np.save(os.path.join(OUTPUT_DIR, 'causal_A_vectors.npy'), 
        np.array([d['A'] for d in causal_data]))
np.save(os.path.join(OUTPUT_DIR, 'causal_B_vectors.npy'), 
        np.array([d['B'] for d in causal_data]))

# ä¿å­˜å…ƒæ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰
metadata = [{
    'ASKFOR': d['ASKFOR'],
    'INDEX': d['INDEX'],
    'cluster_id': int(d['cluster_id']),
    'premise': d['premise']
} for d in causal_data]

with open(os.path.join(OUTPUT_DIR, 'causal_metadata.json'), 'w', encoding='utf-8') as f:
    json.dump(metadata, f, ensure_ascii=False, indent=2)

# ä¿å­˜èšç±»ç»“æœæ‘˜è¦
clustering_summary = {}
for premise, result in clustering_results.items():
    clustering_summary[premise] = {
        'ask_for': result['ask_for'],
        'index': result['index'],
        'total_answers': result['total_answers'],
        'n_clusters': result['n_clusters'],
        'clusters': {
            str(k): {
                'count': v['count'],
                'sample_answers': v['answers']
            } for k, v in result['clusters'].items()
        }
    }

with open(os.path.join(OUTPUT_DIR, 'clustering_summary.json'), 'w', encoding='utf-8') as f:
    json.dump(clustering_summary, f, ensure_ascii=False, indent=2)

# ä¿å­˜Premiseå‘é‡
premise_vecs = {premise: result['premise_vec'].tolist() 
                for premise, result in clustering_results.items()}
with open(os.path.join(OUTPUT_DIR, 'premise_vectors.json'), 'w', encoding='utf-8') as f:
    json.dump(premise_vecs, f, ensure_ascii=False)

print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
print(f"   - causal_A_vectors.npy: Aå‘é‡ï¼ˆå› æˆ–æœï¼‰")
print(f"   - causal_B_vectors.npy: Bå‘é‡ï¼ˆæœæˆ–å› ï¼‰")
print(f"   - causal_metadata.json: å…ƒæ•°æ®")
print(f"   - clustering_summary.json: èšç±»æ‘˜è¦")
print(f"   - premise_vectors.json: Premiseå‘é‡")

print("\n" + "=" * 60)
print("æ•°æ®å¤„ç†å®Œæˆï¼")
print("=" * 60)
