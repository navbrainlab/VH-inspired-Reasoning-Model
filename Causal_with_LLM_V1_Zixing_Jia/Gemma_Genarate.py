"""
å› æœæ¨ç†æµ‹è¯•è„šæœ¬
ä» train.jsonl ä¸­éšæœºæŠ½å–æ ·æœ¬ï¼Œä½¿ç”¨ Gemma-2-9B-IT æ¨¡å‹è¿›è¡Œå› æœæ¨ç†å›ç­”
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
import random
import os
from tqdm import tqdm
from datetime import datetime

# =================é…ç½®åŒºåŸŸ=================
# æ¨¡å‹è·¯å¾„é…ç½®
CACHE_DIR = "D:/huggingface_cache/hub2/hub"
MODEL_NAME = "google/gemma-2-9b-it"

# æ•°æ®è·¯å¾„é…ç½®
INPUT_FILE = "D:/Jiazixing/Vector-HASH-tinkering-main/Causal_Reasoning/train.jsonl"
OUTPUT_DIR = "D:/Jiazixing/Vector-HASH-tinkering-main/Gemma_anwer_causal"

# å·²å›ç­”æ–‡ä»¶è·¯å¾„ï¼ˆæ’é™¤è¿™äº› indexï¼Œæ–°å›ç­”è¿½åŠ åˆ°æ­¤æ–‡ä»¶ï¼‰
EXISTING_ANSWERS_FILE = "D:/Jiazixing/Vector-HASH-tinkering-main/Gemma_anwer_causal/T_1.25_Gemma_Answer/gemma_causal_answers_100.jsonl"

# é‡‡æ ·å’Œç”Ÿæˆé…ç½®
N_SAMPLES = 700          # éšæœºæŠ½å–çš„æ ·æœ¬æ•°é‡
K_ANSWERS = 100            # æ¯æ¡æ ·æœ¬å›ç­”çš„æ¬¡æ•°

# ç”Ÿæˆå‚æ•°ï¼ˆå‚è€ƒ generate_pairs_2.pyï¼‰
MAX_NEW_TOKENS = 512
TEMPERATURE = 1.25
TOP_P = 0.95
DO_SAMPLE = True
# ========================================


def setup_model():
    """åŠ è½½æœ¬åœ° Gemma æ¨¡å‹"""
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_NAME} ...")
    
    # æŸ¥æ‰¾æœ¬åœ°æ¨¡å‹è·¯å¾„
    model_base = os.path.join(CACHE_DIR, "models--google--gemma-2-9b-it", "snapshots")
    
    if not os.path.exists(model_base):
        raise FileNotFoundError(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_base}")
    
    snapshots = os.listdir(model_base)
    if not snapshots:
        raise FileNotFoundError("æ¨¡å‹å°šæœªä¸‹è½½ï¼Œè¯·å…ˆä¸‹è½½æ¨¡å‹")
    
    local_model_path = os.path.join(model_base, snapshots[0])
    print(f"ğŸ“ æœ¬åœ°æ¨¡å‹è·¯å¾„: {local_model_path}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    safetensor_files = [f for f in os.listdir(local_model_path) if f.endswith('.safetensors')]
    print(f"ğŸ“¦ æ‰¾åˆ° {len(safetensor_files)} ä¸ª safetensor æ–‡ä»¶")
    
    # åŠ è½½ tokenizer
    print("\nâ³ æ­£åœ¨åŠ è½½ tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        local_model_path,
        local_files_only=True
    )
    print("âœ… Tokenizer åŠ è½½æˆåŠŸï¼")
    
    # åŠ è½½æ¨¡å‹
    print("\nâ³ æ­£åœ¨åŠ è½½æ¨¡å‹ï¼ˆçº¦éœ€ 1-2 åˆ†é’Ÿï¼‰...")
    model = AutoModelForCausalLM.from_pretrained(
        local_model_path,
        local_files_only=True,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    print(f"ğŸ’» æ¨¡å‹è®¾å¤‡: {model.device}")
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {model.num_parameters() / 1e9:.2f}B")
    
    return tokenizer, model


def load_existing_indices(existing_file):
    """åŠ è½½å·²å›ç­”è¿‡çš„ index åˆ—è¡¨"""
    existing_indices = set()
    if os.path.exists(existing_file):
        print(f"\nğŸ“‚ åŠ è½½å·²å›ç­”æ–‡ä»¶: {existing_file}")
        with open(existing_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    record = json.loads(line)
                    existing_indices.add(record['index'])
        print(f"ğŸ“Š å·²å›ç­”çš„ unique index æ•°: {len(existing_indices)}")
    else:
        print(f"âš ï¸ å·²å›ç­”æ–‡ä»¶ä¸å­˜åœ¨: {existing_file}ï¼Œå°†ä¸æ’é™¤ä»»ä½•æ ·æœ¬")
    return existing_indices


def load_data(file_path, n_samples, exclude_indices=None):
    """ä» jsonl æ–‡ä»¶åŠ è½½æ•°æ®å¹¶éšæœºæŠ½æ ·ï¼ŒåŒæ—¶è¿”å›å®Œæ•´æ•°æ®é›†ç”¨äº few-shot"""
    print(f"\nğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®: {file_path}")
    
    all_data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                all_data.append(json.loads(line))
    
    print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(all_data)}")
    
    # æ’é™¤å·²å›ç­”çš„ index
    if exclude_indices:
        available_data = [item for item in all_data if item['index'] not in exclude_indices]
        print(f"ğŸš« æ’é™¤å·²å›ç­”çš„ {len(exclude_indices)} ä¸ª index")
        print(f"ğŸ“Š å¯ç”¨äºæŠ½æ ·çš„æ ·æœ¬æ•°: {len(available_data)}")
    else:
        available_data = all_data
    
    # éšæœºæŠ½æ ·
    if n_samples > len(available_data):
        print(f"âš ï¸ è¯·æ±‚æ ·æœ¬æ•° ({n_samples}) å¤§äºå¯ç”¨æ ·æœ¬æ•° ({len(available_data)})ï¼Œå°†ä½¿ç”¨å…¨éƒ¨å¯ç”¨æ•°æ®")
        n_samples = len(available_data)
    
    sampled_data = random.sample(available_data, n_samples)
    print(f"ğŸ² éšæœºæŠ½å–æ ·æœ¬æ•°: {len(sampled_data)}")
    
    return sampled_data, all_data


def get_few_shot_examples(all_data, current_index, ask_for, n_examples=3):
    """
    ä»æ•°æ®é›†ä¸­è·å– few-shot ç¤ºä¾‹
    
    Args:
        all_data: å®Œæ•´æ•°æ®é›†
        current_index: å½“å‰æ ·æœ¬çš„ indexï¼ˆé¿å…ä½¿ç”¨ï¼‰
        ask_for: "effect" æˆ– "cause"
        n_examples: ç¤ºä¾‹æ•°é‡
    
    Returns:
        ç¤ºä¾‹åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (premise, answer) å…ƒç»„
    """
    # ç­›é€‰ç›¸åŒç±»å‹ï¼ˆeffect/causeï¼‰ä¸”ä¸æ˜¯å½“å‰æ ·æœ¬çš„æ•°æ®
    candidates = [
        item for item in all_data 
        if item["ask-for"] == ask_for and item["index"] != current_index
    ]
    
    # éšæœºé€‰æ‹© n_examples ä¸ªç¤ºä¾‹
    if len(candidates) < n_examples:
        selected = candidates
    else:
        selected = random.sample(candidates, n_examples)
    
    examples = []
    for item in selected:
        premise = item["premise"]
        # æ ¹æ® label é€‰æ‹©æ­£ç¡®çš„ hypothesis ä½œä¸ºç­”æ¡ˆ
        if item["label"] == 0:
            answer = item["hypothesis1"]
        else:
            answer = item["hypothesis2"]
        examples.append((premise, answer))
    
    return examples

def construct_prompt(premise, ask_for, all_data=None, current_index=None, n_examples=3):
    """
    Construct a prompt based on premise and ask_for, including few-shot examples.

    Args:
        premise: the premise / event description (string)
        ask_for: either "effect" or "cause"
        all_data: the full dataset (used to sample few-shot examples)
        current_index: index of the current sample (to avoid using the same sample as an example)
        n_examples: number of few-shot examples to include

    Returns:
        A prompt string ready to feed into the model.
    """
    # Gather few-shot examples
    examples_text = ""
    if all_data is not None and current_index is not None:
        examples = get_few_shot_examples(all_data, current_index, ask_for, n_examples)
        
        if ask_for == "effect":
            examples_text = "Here are some examples:\n\n"
            for i, (ex_premise, ex_answer) in enumerate(examples, 1):
                examples_text += f"Example {i}:\nPremise: {ex_premise}\nResult: {ex_answer}\n\n"
        else:  # cause
            examples_text = "Here are some examples:\n\n"
            for i, (ex_premise, ex_answer) in enumerate(examples, 1):
                examples_text += f"Example {i}:\nResult: {ex_premise}\nCause: {ex_answer}\n\n"
    
    if ask_for == "effect":
        prompt = f"""You are a causal inference expert. Given a premise, infer a possible outcome or effect.

{examples_text}Now please answer:

Premise: {premise}

Please answer in one short sentence: what result might this premise lead to?

Requirements:
1. Provide only one most likely result.
2. Keep the answer concise, no more than 40 words.
3. Give the answer directly, do not explain the reasoning process.
4. All answers must be in English.

Result:"""
    else:  # ask_for == "cause"
        prompt = f"""You are a causal inference expert. Given an outcome, infer a possible cause.

{examples_text}Now please answer:

Result: {premise}

Please answer in one short sentence: what cause might have led to this result?

Requirements:
1. Provide only one most likely cause.
2. Keep the answer concise, no more than 30 words.
3. Give the answer directly, do not explain the reasoning process.

Cause:"""
    
    return prompt



def generate_answer(tokenizer, model, prompt):
    """
    ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”
    
    Args:
        tokenizer: åˆ†è¯å™¨
        model: è¯­è¨€æ¨¡å‹
        prompt: è¾“å…¥ prompt
    
    Returns:
        ç”Ÿæˆçš„å›ç­”æ–‡æœ¬
    """
    # ä½¿ç”¨ chat template
    chat = [{"role": "user", "content": prompt}]
    prompt_formatted = tokenizer.apply_chat_template(
        chat, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    inputs = tokenizer(prompt_formatted, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=DO_SAMPLE,
            temperature=TEMPERATURE,
            top_p=TOP_P,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # åªæå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
    generated_text = tokenizer.decode(
        outputs[0][inputs.input_ids.shape[1]:], 
        skip_special_tokens=True
    )
    
    # æ¸…ç†å›ç­”ï¼ˆå»é™¤å¤šä½™çš„ç©ºç™½å’Œæ¢è¡Œï¼‰
    answer = generated_text.strip()
    
    # å¦‚æœå›ç­”è¿‡é•¿ï¼Œæˆªå–ç¬¬ä¸€å¥
    if len(answer) > 200:
        # å°è¯•åœ¨å¥å·å¤„æˆªæ–­
        for sep in ['ã€‚', '.', 'ï¼', '!', 'ï¼Ÿ', '?', '\n']:
            if sep in answer:
                answer = answer.split(sep)[0] + sep
                break
    
    return answer


def run_inference(tokenizer, model, sampled_data, all_data, k_answers, output_file, n_examples=3):
    """
    å¯¹æŠ½æ ·æ•°æ®è¿›è¡Œæ¨ç†
    
    Args:
        tokenizer: åˆ†è¯å™¨
        model: è¯­è¨€æ¨¡å‹
        sampled_data: æŠ½æ ·çš„æ•°æ®åˆ—è¡¨
        all_data: å®Œæ•´æ•°æ®é›†ï¼ˆç”¨äº few-shot ç¤ºä¾‹ï¼‰
        k_answers: æ¯æ¡æ ·æœ¬å›ç­”çš„æ¬¡æ•°
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
        n_examples: few-shot ç¤ºä¾‹æ•°é‡
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    print(f"\nğŸš€ å¼€å§‹æ¨ç†...")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰: {output_file}")
    print(f"ğŸ“Š æ–°æ ·æœ¬æ•°: {len(sampled_data)}, æ¯æ¡å›ç­” {k_answers} æ¬¡")
    print(f"ğŸ“ˆ æœ¬æ¬¡ç”Ÿæˆæ¬¡æ•°: {len(sampled_data) * k_answers}")
    print(f"ğŸ“š æ¯ä¸ª prompt åŒ…å« {n_examples} ä¸ª few-shot ç¤ºä¾‹")
    
    results = []
    
    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
    with tqdm(total=len(sampled_data) * k_answers, desc="ç”Ÿæˆå›ç­”") as pbar:
        for item in sampled_data:
            index = item["index"]
            premise = item["premise"]
            ask_for = item["ask-for"]
            
            # æ„å»º promptï¼ˆåŒ…å« few-shot ç¤ºä¾‹ï¼‰
            prompt = construct_prompt(premise, ask_for, all_data, index, n_examples)
            
            # ç”Ÿæˆ k æ¬¡å›ç­”
            for k in range(k_answers):
                answer = generate_answer(tokenizer, model, prompt)
                
                # æ„å»ºç»“æœè®°å½•
                record = {
                    "index": index,
                    "premise": premise,
                    "ask-for": ask_for,
                    "answer_round": k + 1,
                    "Gemma_answer": answer
                }
                
                results.append(record)
                
                # å®æ—¶å†™å…¥æ–‡ä»¶
                with open(output_file, 'a', encoding='utf-8') as f:
                    f.write(json.dumps(record, ensure_ascii=False) + "\n")
                
                pbar.update(1)
    
    print(f"\nâœ… æ¨ç†å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ“Š å…±ç”Ÿæˆ {len(results)} æ¡å›ç­”")
    
    return results, output_file


def print_sample_results(results, n=5):
    """æ‰“å°éƒ¨åˆ†ç»“æœç¤ºä¾‹"""
    print(f"\nğŸ“‹ ç»“æœç¤ºä¾‹ï¼ˆå‰ {n} æ¡ï¼‰:")
    print("=" * 80)
    
    for i, record in enumerate(results[:n]):
        print(f"\n[{i+1}] Index: {record['index']}")
        print(f"    Premise: {record['premise']}")
        print(f"    Ask-for: {record['ask-for']}")
        print(f"    Round: {record['answer_round']}")
        print(f"    Gemma Answer: {record['Gemma_answer']}")
        print("-" * 80)


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ”¬ å› æœæ¨ç†æµ‹è¯• - Gemma-2-9B-IT")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­ï¼ˆå¯é€‰ï¼Œç”¨äºå¤ç°ï¼‰
    random.seed(42)
    
    # 1. åŠ è½½æ¨¡å‹
    tokenizer, model = setup_model()
    
    # 2. åŠ è½½å·²å›ç­”çš„ indexï¼ˆç”¨äºæ’é™¤ï¼‰
    existing_indices = load_existing_indices(EXISTING_ANSWERS_FILE)
    
    # 3. åŠ è½½å¹¶æŠ½æ ·æ•°æ®ï¼ˆæ’é™¤å·²å›ç­”çš„ï¼ŒåŒæ—¶è·å–å®Œæ•´æ•°æ®é›†ç”¨äº few-shotï¼‰
    sampled_data, all_data = load_data(INPUT_FILE, N_SAMPLES, exclude_indices=existing_indices)
    
    # 4. è¿›è¡Œæ¨ç†ï¼ˆè¿½åŠ åˆ°å·²æœ‰æ–‡ä»¶ï¼‰
    results, output_file = run_inference(
        tokenizer, 
        model, 
        sampled_data,
        all_data,
        K_ANSWERS, 
        EXISTING_ANSWERS_FILE,  # ç›´æ¥è¿½åŠ åˆ°å·²æœ‰æ–‡ä»¶
        n_examples=3  # few-shot ç¤ºä¾‹æ•°é‡
    )
    
    # 4. æ‰“å°ç¤ºä¾‹ç»“æœ
    print_sample_results(results)
    
    # 5. ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    effect_count = sum(1 for r in results if r["ask-for"] == "effect")
    cause_count = sum(1 for r in results if r["ask-for"] == "cause")
    print(f"   - Effect ç±»å‹å›ç­”: {effect_count}")
    print(f"   - Cause ç±»å‹å›ç­”: {cause_count}")
    
    print("\nğŸ‰ ä»»åŠ¡å®Œæˆï¼")


if __name__ == "__main__":
    main()
